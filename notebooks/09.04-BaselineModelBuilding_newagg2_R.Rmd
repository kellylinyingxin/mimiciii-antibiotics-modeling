---
title: "model_building_prolongedAB_project"
output:
  html_document: default
  pdf_document: default
---


###note: 04-15-19 this is model building worksheet for using median, range, and std of values, where the values are now normalized to a "healthy" population. x-median /IQR where meadian and IQR were calculated for the last 24 hours of ab_partial/culture-neg patients. 
### note: this is a slightly modified median imputation preproessing document where I'm working with the new data aggregations (max,min, median, and std) for the top most important variables found on the inital . data extracted from my python pipeline (newagg data from 07.30 ipynb) and preprocessed in 09.newagg2_preprocessing_median_impute.rmd.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE, comment="  ", prompt = TRUE)
```

```{r}
library(dplyr)
library(ggplot2)
library(caret)
library(doParallel)
library(doSNOW)
library(MLmetrics)
library(mlbench)
library(gbm)
library(randomForest)

#https://www.kaggle.com/captcalculator/imputing-missing-data-with-the-mice-package-in-r  #great reference

```

```{r}
data<- read.csv("/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling/models/imputation/04042019_newagg2_median_imputed_train.csv", stringsAsFactors=TRUE,check.names=TRUE)
# data_test<- read.csv("/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling/models/imputation/04042019_newagg_median_imputed_test.csv", stringsAsFactors=TRUE,check.names=TRUE)

cores<- 5 #number of parallel processing clusters,
```

```{r}
glimpse(data)

```


#restricting to two class classification
```{r}
# data<-subset(data, final_bin %in% c('C_neg/A_full','C_pos/A_full',"C_neg/A_partial"))
# data_test<-subset(data_test, final_bin %in% c('C_neg/A_full','C_pos/A_full',"C_neg/A_partial"))
# 
# data_2class<-subset(data, final_bin %in% c('C_pos/A_full',"C_neg/A_partial"))
# data_test_2class<-subset(data_test, final_bin %in% c('C_pos/A_full',"C_neg/A_partial"))
```
```{r}
log(8)
```



```{r}
factor_convert<- function(preprocessed_imputd, level_number=2, level1='C_neg/A_partial', level2='C_pos/A_full', label1= "neg", label2="pos"){
  if (level_number==2){
    preprocessed_imputd$final_bin= factor(preprocessed_imputd$final_bin, levels=c(level1,level2), labels=c(label1,label2))
  }
  logical_vec<-names(preprocessed_imputd[,sapply(preprocessed_imputd, is.logical)])
  preprocessed_imputd[,logical_vec]<-lapply(preprocessed_imputd[,logical_vec],as.factor)
  return(preprocessed_imputd)
}
```


### fixing logical -> factors
```{r}
# # norm_and_transform(data)
#full_imputed<- norm_and_transform(data)
full_imputed<-factor_convert(data, level_number=2)
table(full_imputed$final_bin)
# 
# full_imputed_test<- norm_and_transform(data_test)
# full_imputed_test<-factor_convert(full_imputed_test, level_number=4)
# 
# preprocessed_imputd<-norm_and_transform(data_2class)
# preprocessed_imputd<-factor_convert(preprocessed_imputd)
# 
# preprocessed_imputd_test<-norm_and_transform(data_test_2class)
# preprocessed_imputd_test<-factor_convert(preprocessed_imputd_test)
```

### dropping varibles

```{r preprocessing}
#converting weight and first_admit age to the median/iqr based zscore
weight_median=median(log(full_imputed[full_imputed$final_bin=='neg',"weight"]+1))
weight_iqr=IQR(log(full_imputed[full_imputed$final_bin=='neg',"weight"]+1))

age_median=median(log(full_imputed[full_imputed$final_bin=='neg',"first_admit_age"]+1))
age_iqr=IQR(log(full_imputed[full_imputed$final_bin=='neg',"first_admit_age"]+1))

full_imputed$weight=(log(full_imputed$weight+1) - weight_median)/weight_iqr
full_imputed$first_admit_age=(log(full_imputed$first_admit_age+1) - age_median)/age_iqr


####If a log transformation of the data is appropriate then you should typically do the transformation on the original data first, whatever you call that process.

# weight_median=median(log(full_imputed[full_imputed$final_bin=='neg',"weight"]))
# weight_iqr=IQR((full_imputed[full_imputed$final_bin=='neg',"weight"]))
# 
# age_median=median((full_imputed[full_imputed$final_bin=='neg',"first_admit_age"]))
# age_iqr=IQR(full_imputed[full_imputed$final_bin=='neg',"first_admit_age"])
# 
# full_imputed$weight=(log(full_imputed$weight +1) - log(weight_median+1))/log(weight_iqr+1)
# full_imputed$first_admit_age=(log(full_imputed$first_admit_age+1) - log(age_median+1))/log(age_iqr+1)
# 
# log(77)


# 
# weight_median=median((full_imputed[full_imputed$final_bin=='neg',"weight"]))
# weight_iqr=IQR((full_imputed[full_imputed$final_bin=='neg',"weight"]))
# 
# age_median=median((full_imputed[full_imputed$final_bin=='neg',"first_admit_age"]))
# age_iqr=IQR(full_imputed[full_imputed$final_bin=='neg',"first_admit_age"])
# 
# full_imputed$weight=((full_imputed$weight) - weight_median)/weight_iqr
# full_imputed$first_admit_age=((full_imputed$first_admit_age) - age_median)/age_iqr

#removing icustay id and keeping as a seperate vector
icustay_id= full_imputed$icustay_id
full_imputed<-full_imputed[,!names(full_imputed) %in% c("icustay_id")]

full_imputed[is.na(full_imputed)] <- 0
full_imputed$first_admit_age
```



# visualizing data

```{r}
# library(skimr)
# # skim(data)
# #skim(full_imputed.lapply(log(+1)))
# 
# 
# 
# full_imputed_numeric<-full_imputed[,names(full_imputed[,sapply(full_imputed, is.double)])]
# #full_imputed_numeric<-full_imputed_numeric[,names(full_imputed_numeric[,sapply(full_imputed, is.integer)==FALSE])]
# 
# 
# summary(full_imputed_numeric)
# 
# full_log_numeric<-log(full_imputed_numeric+1.1)
# skim(full_log_numeric)
# full_log_numeric[is.na(full_log_numeric)] =0
# 
# skim(full_log_numeric)
# skim(full_imputed_numeric)
# 
# log()
# 
# #ok weird, mean_creatinine max is greater than ptp_creatinine max. oh right, the range is just max-min. 
# 
# full_imputed_numeric[full_imputed_numeric['ptp_creatinine']==13.11111,c('ptp_creatinine',"mean_creatinine","std_creatinine")]
# 
# max(full_imputed_numeric[c('ptp_creatinine')]) #
# 
# full_imputed_numeric[full_imputed_numeric['ptp_creatinine']==max(full_imputed_numeric[c('ptp_creatinine')]),c('ptp_creatinine',"mean_creatinine","std_creatinine") ]
# full_imputed_numeric[full_imputed_numeric['mean_creatinine']==max(full_imputed_numeric[c('mean_creatinine')]),c('ptp_creatinine',"mean_creatinine","std_creatinine") ]
# 
# icustay_id[1833] 
# 
# ##need to explore:
# #  #icustay id:248526
# # ptp_creatinine: 1.666667
# # mean_creatinine: 15.40741
# # std_creatinine:0.9049475
# 
#  
#  #  #icustay id:233363
# # ptp_creatinine: 13.11111
# # mean_creatinine: 9.574074
# # std_creatinine:5.281889
# 
# 
# ##explore relationship between creatinine ptp and std
# 
# range_std=full_imputed_numeric['ptp_creatinine']/full_imputed_numeric['std_creatinine']
# range_std
# ggplot(range_std, aes(x=ptp_creatinine)) + geom_density() 
# 
# ggplot(full_imputed_numeric, aes(x=ptp_creatinine)) + geom_density() 
# ggplot(full_imputed_numeric, aes(x=std_creatinine)) + geom_density() 

```

```{r}
library(skimr)
skim(full_imputed)

exp(log(5))
full_imputed$median_meanartpress
exp(full_imputed$median_meanartpress)

mean(exp(full_imputed$median_meanartpress)-1)
mean(full_imputed$median_meanartpress)
```


preprocessing transform methods in caret: https://machinelearningmastery.com/pre-process-your-dataset-in-r/

```{r}
names(full_imputed)
for (element in names(full_imputed)){
  print(ggplot(full_imputed, aes(x=element)) + geom_density() )
}

ggplot(full_imputed, aes(x=ptp_wbc)) + geom_density() 
ggplot(full_imputed, aes(x=mean_bilirubin)) + geom_density() 

mean_bilirubin
```



-----------2 CLASS MODEL BUILDING------------------




###logistic regression

```{r}
library(caret)
set.seed(12345)
fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           classProbs = TRUE,
                           sampling= "smote",
                           summaryFunction= multiClassSummary,
                           repeats = 5,
                           savePredictions = T)

logit_1 <- train(
  final_bin~.,
  data=full_imputed,
  method="glm",
  family=binomial,
  metric='Accuracy',
  trControl = fitControl
  )


logit_1$results #%>% arrange(RMSE) %>% head(1)[c(1:2,3,5)] ->knn_output_10; knn_output_10

#cv-confusion matrix percentage
table(full_imputed$final_bin)/nrow(full_imputed)
table(full_imputed$final_bin)
confusionMatrix(logit_1, positive="pos")

##test confusion
#round(table(yhat=predict(logit_1,preprocessed_imputd_test),y=preprocessed_imputd_test$final_bin)/ nrow(preprocessed_imputd_test),3) *100
```
pretty good. having a higher negative predictive value probably better as a screening tool. 

```{r}
# library(pROC)
# # Select a parameter setting
# #logit_1$pred$mtry
# selectedIndices <- logit_1$pred$mtry == "pos"
# # Plot:
# 
# # plot.roc(logit_1$pred$obs[selectedIndices],
# #          logit_1$pred$neg[selectedIndices])

```



```{r}
# 
# table(preprocessed_imputd$final_bin)
# 
# library(gridExtra)
# set.seed(12345)
# w<- 1/table(preprocessed_imputd$final_bin)
# 
# # fitControl <- trainControl(## oob
# #                            method = "oob",
# #                            classProbs = TRUE,
# #                            summaryFunction= multiClassSummary)
# 
# 
# fitControl <- trainControl(## 10-fold CV
#                            method = "cv",
#                            number = 10,
#                            classProbs = TRUE,
#                            summaryFunction= multiClassSummary)
# 
# tuning<- expand.grid(mtry=c(2,3))
# 
# rf_1 <- train(
#   final_bin~.,
#   data=preprocessed_imputd,
#   method="rf",
#   metric='logLoss',
#   trControl = fitControl,
#   tuneGrid= tuning,
#   classwt=w
#   )
# 
# rf_1$results #%>% arrange(RMSE) %>% head(1)[c(1:2,3,5)] ->knn_output_10; knn_output_10
# 
# confusionMatrix(rf_1)
```
 Accuracy (average) cv : 0.8445
 
 

```{r} 
#  z
# # set.seed(12345)
# # w<- 1/table(preprocessed_imputd$final_bin)
# # 
# # fitControl <- trainControl(## 10-fold CV
# #                            method = "cv",
# #                            number = 3,
# #                            classProbs = TRUE,
# #                            summaryFunction= twoClassSummary)
# # 
# # tuning<- expand.grid(mtry=c(3))
# # 
# # rf_1 <- train(
# #   x= preprocessed_imputd[,-c(46)],
# #   y=preprocessed_imputd$final_bin,
# #   # final_bin~.,
# #   # data=preprocessed_imputd,
# #   method="rf",
# #   metric='Spec',
# #   trControl = fitControl,
# #   tuneGrid= tuning,
# #   classwt=w
# #   )
# # 
# # rf_1$results#%>% arrange(-Specificity) #%>% arrange(RMSE) %>% head(1)[c(1:2,3,5)] ->knn_output_10; knn_output_10
# # 
# # confusionMatrix(rf_1, positive="pos", mode="everything")
# # 
# # predict(rf_1,preprocessed_imputd_test)
# # rf_1$
# # 
# # rf_test<-confusionMatrix(rf_1,reference=preprocessed_imputd_test$final_bin,positive="pos", mode="everything"); rf_test
```

yeiks, didn't work that well on the positive class. 
 
 

 
```{r}
# rf_1
# 
# 
# confusionMatrix(rf_1)# * nrow(preprocessed_imputd)
# 
# #test confusion
# round(table(yhat=predict(rf_1,preprocessed_imputd_test),y=preprocessed_imputd_test$final_bin)/ nrow(preprocessed_imputd_test),3) *100
# 
# table(yhat=predict(rf_1,preprocessed_imputd_test),y=preprocessed_imputd_test$final_bin)
```
 
 
shows really poor specificity!

confirming mtry for training on accuracy.
```{r}
library(randomForest)
w<- 1/table(full_imputed$final_bin)

set.seed(12345)
full_imputed[,names(full_imputed) !="final_bin"]
rFtune<- tuneRF(full_imputed[,names(full_imputed) !="final_bin"],  y=full_imputed$final_bin,
                mtryStart=6,
                ntreeTry=50, #number of trees used at the tuning step
                stepFactor=2, #at each iteration, mtry is inflated (or deflated) by this value
                improve=0.05, #the (relative) improvement in OOB error must be by this much for the 
                trace=TRUE,
                plot=TRUE,
                doBest=FALSE)#,
                #classwt=w)

#18:OOB error = 21.1%  #mtry=18
```

mtry =3 appears to be the best by both caret cv and tuneRF.



training rf on a different classification parameter, logloss. Log Loss is like accuracy (where 0 is perfect and 1 is worst) except it heavily penalises classifiers that are confident about an incorrect classification.

```{r}
# library(gridExtra)
# set.seed(12345)
# w<- 1/table(preprocessed_imputd$final_bin)
# 
# fitControl <- trainControl(## 10-fold CV
#                            method = "repeatedcv",
#                            number = 5,
#                            classProbs = TRUE,
#                            #sampling= "smote",
#                            summaryFunction= multiClassSummary,
#                            repeats = 1)
# 
# tuning<- expand.grid(mtry=c(2,3,4,5,10,20))
# 
# rf_2 <- train(
#   final_bin~.,
#   data=preprocessed_imputd,
#   method="rf",
#   metric='logLoss',
#   trControl = fitControl,
#   tuneGrid= tuning,
#   classwt=w
#   )
# 
# rf_2$results%>% arrange(logLoss) #%>% head(1)[c(1:2,3,5)] ->knn_output_10; knn_output_10
# 
# confusionMatrix(rf_2)
```




######using randomForest vanilla package
```{r}
# 
library(randomForest)
set.seed(12345)
w <- 1/table(full_imputed$final_bin)
w<- w/sum(w)
rForest <- randomForest(x=full_imputed[,!names(full_imputed) %in%c("icustay_id","final_bin")],
                        y=full_imputed$final_bin,
                        replace=FALSE,
                        #localImp = weights,
                        #classwt=w,
                        # xtest=preprocessed_imputd_test[,-c(46)],
                        # ytest=preprocessed_imputd_test$final_bin,
                        mtry=6,
                        ntree = 200,
                        importance = TRUE)



plot(rForest)
rForest

rForest$importance
varImpPlot(rForest)
#varImpPlot(rForest)$

?randomForest

# table(yhat=predict(rForest,preprocessed_imputd_test),y=preprocessed_imputd_test$final_bin)
# table(yhat=predict(rForest,preprocessed_imputd_test),y=preprocessed_imputd_test$final_bin)#/nrow(preprocessed_imputd_test)

#table(yhat=rForest$test$predicted,y=preprocessed_imputd_test$final_bin)#/nrow(preprocessed_imputd_test) *100
#table(yhat=rForest$test$predicted,y=preprocessed_imputd_test$final_bin)
confusionMatrix(rForest$predicted,reference=full_imputed$final_bin, positive="pos")


#rf_test<-confusionMatrix(rForest$test$predicted,reference=preprocessed_imputd_test$final_bin, positive="pos" );rf_test

```
         Reference
Prediction  neg  pos
       neg 3805  965
       pos  227  435
       
in the above, the false negatives are > true positives.  minimum of diagonal entry should be (435) > than maximum of off diagonal entry(965).


```{r}
simple_roc <- function(labels, scores){
  labels <- labels[order(scores, decreasing=TRUE)]
  data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)
}

simple_roc(full_imputed$final_bin,rForest$predicted)

```


repeating same thing with class weight
```{r}
# wn
# wn = sum(preprocessed_imputd$final_bin=="neg")/length(preprocessed_imputd$final_bin)
# wy = 1
# classwt = c("neg"=wn, "pos"=wy)
# 
# library(randomForest)
# rForest2 <- randomForest(x=preprocessed_imputd[,-c(46)],
#                         y=preprocessed_imputd$final_bin,
#                         xtest=preprocessed_imputd_test[,-c(46)],
#                         ytest=preprocessed_imputd_test$final_bin,
#                         classwt = c("neg"=wn, "pos"=wy),
#                         mtry=3,
#                         ntree = 150,
#                         importance = TRUE)
# plot(rForest2)
# 
# confusionMatrix(rForest2$predicted,reference=preprocessed_imputd$final_bin, positive="pos")
# rf_test2<-confusionMatrix(rForest2$test$predicted,reference=preprocessed_imputd_test$final_bin, positive="pos" );rf_test2
```
the above is likely incorrectly tuned because class imballance.
The "randomForest" function in the "randomForest" R package supports the Balanced Random Forest. One need to specify the "strata" and the "sampsize" parameters to enable the balanced bootstrapping resampling.

strata
A (factor) variable that is used for stratified sampling.
sampsize
Size(s) of sample to draw. For classification, if sampsize is a vector of the length the number of strata, then sampling is stratified by strata, and the elements of sampsize indicate the numbers to be drawn from the strata.

```{r}
# w <- 1/table(preprocessed_imputd$final_bin)
# w<- w/sum(w);w
# weights <- rep(0, nrow(preprocessed_imputd))
# weights[preprocessed_imputd$final_bin== "neg"] <- w['neg']
# weights[preprocessed_imputd$final_bin== "pos"] <- w['pos']
# table(weights, preprocessed_imputd$final_bin)
# weights
```



#####trying with ranger
```{r}
library(ranger)
w <- 1/table(full_imputed$final_bin)
w<- w/sum(w)
weights <- rep(0, nrow(full_imputed))
weights[full_imputed$final_bin== "neg"] <- w['neg']
weights[full_imputed$final_bin== "pos"] <- w['pos']
table(weights, full_imputed$final_bin)
weights

?ranger
##Sampling without replacement is important here, as otherwise samples from the smaller classes will contain many more repetitions, and the class will still be underrepresented.
ranger1<- ranger(final_bin~.,full_imputed, case.weights=weights, replace=FALSE)
ranger1 #performed the best!
ranger2<- ranger(final_bin~.,full_imputed, case.weights=weights, replace=TRUE)
ranger2

ranger2$predictions

ranger3<- ranger(final_bin~.,full_imputed,  replace=TRUE)
ranger3



ranger1_training<-confusionMatrix(ranger1$predictions, reference= full_imputed$final_bin, positive="pos" );ranger1_training
ranger2_training<-confusionMatrix(ranger2$predictions, reference= full_imputed$final_bin, positive="pos" );ranger2_training
ranger3_training<-confusionMatrix(ranger3$predictions, reference= full_imputed$final_bin, positive="pos" );ranger3_training
```

#auc
```{r}
library(pROC)
# predictions_c <- predict(object=ranger1, full_imputed)#$predictions
# 
# predictions <- predict(object=ranger1_prob, full_imputed)
# predictions$predictions[,2]
# 
# ranger_roc=roc(ifelse(full_imputed[,"final_bin"] == "pos", 1, 0), predictions$predictions[,2])
# plot(ranger_roc)
# 
# predictions$predictions[,1]
# 
# #roc(response=ifelse(full_imputed[,"final_bin"] == "pos", 1, 0), predictor=predictions$predictions[,2], percent=TRUE)
# plot(roc(response=full_imputed$final_bin, predictor=round(predictions$predictions[,1],2), smooth=TRUE))
# 
# full_imputed$final_bin
# #pred <- prediction(ranger1$predictions, as.numeric(full_imputed$final_bin))
# plot(ifelse(full_imputed[,"final_bin"] == "pos", 1, 0),predictions$predictions[,2])
# 
# pred <- prediction(labels=ifelse(full_imputed[,"final_bin"] == "pos", 1, 0),predictions$predictions[,2])

library(ROCR)
ranger1_prob<- ranger(final_bin~.,full_imputed, case.weights=weights, replace=FALSE,probability = T)

##works but only get ridgid roc
pred<-prediction(as.numeric(ranger1$predictions),as.numeric(ifelse(full_imputed[,"final_bin"] == "pos", 2, 1)) )
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)


#jesus finally got this to work.
pred<-prediction(ranger1_prob$predictions[,2],(ifelse(full_imputed[,"final_bin"] == "pos", 2, 1)) )
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)

```





--------------

###boosted classification tree

```{r}

gbmdata<-full_imputed
gbmdata$final_bin<-as.numeric(gbmdata$final_bin)-1
set.seed(12345)
w<- 1/table(full_imputed$final_bin)
w
weights <- rep(0, nrow(full_imputed))
weights[full_imputed$final_bin== "neg"] <- w['neg']
weights[full_imputed$final_bin== "pos"] <- w['pos']
table(weights, full_imputed$final_bin)
weights

?gbm


Inc.gbm2 <- gbm(final_bin ~ .,
                data=gbmdata,
                distribution="bernoulli",
                n.trees=5000,
                weights = weights,
                shrinkage=0.05,
                interaction.depth=3,
                bag.fraction = .5,
                train.fraction = 1,
                n.minobsinnode = 10,
                cv.folds = 10,
                keep.data=TRUE,
                verbose=FALSE)


best.iter <- gbm.perf(Inc.gbm2,method="cv");best.iter
Inc.gbm2$cv.error[best.iter]
summary(Inc.gbm2,n.trees=best.iter)
Inc.gbm2$var.names

Inc.gbm2

Inc.gbm2$cv.error[best.iter]
#?gbm
```

```{r}
Inc.gbm1 <- gbm(final_bin ~ .,
                data=gbmdata,
                distribution="bernoulli",
                n.trees=2000,
                weights = weights,
                shrinkage=0.05,
                interaction.depth=3,
                bag.fraction = .5,
                train.fraction = 1,
                n.minobsinnode = 10,
                cv.folds = 10,
                keep.data=TRUE,
                verbose=FALSE)
```

```{r}
best.iter
best.iter1 <- gbm.perf(Inc.gbm1,method="cv");best.iter
Inc.gbm1$cv.error[best.iter]
Inc.gbm1$cv.error#[325]

Inc.gbm1$cv.error

Inc.gbm1$fit[best.iter]

summary(Inc.gbm1,n.trees=best.iter)
#Inc.gbm1$var.names

Inc.gbm1_pred=predict(Inc.gbm1,newdata=gbmdata,n.trees=best.iter1, type="response")#,single.tree=TRUE)
gbm_training1<-confusionMatrix(ifelse(Inc.gbm1_pred>=0.5,"pos","neg"),reference=full_imputed$final_bin);gbm_training1
```

```{r}
### gbm using weights with interaction depth 2
#           Reference
# Prediction  neg  pos
#        neg 3094  360
#        pos  938 1040
#                                           
#                Accuracy : 0.761           
#                  95% CI : (0.7495, 0.7723)
#     No Information Rate : 0.7423          
#     P-Value [Acc > NIR] : 0.0007572       
#                                           
#                   Kappa : 0.4496          
#  Mcnemar's Test P-Value : < 2.2e-16       
#                                           
#             Sensitivity : 0.7674          
#             Specificity : 0.7429          
#          Pos Pred Value : 0.8958          
#          Neg Pred Value : 0.5258          
#              Prevalence : 0.7423          
#          Detection Rate : 0.5696          
#    Detection Prevalence : 0.6359          
#       Balanced Accuracy : 0.7551          
#                                           
#        'Positive' Class : neg    
```


```{r}
set.seed(12345)
Inc.gbm1 <- gbm(final_bin ~ .,
                data=gbmdata,
                distribution="bernoulli",
                n.trees=2000,
                weights = weights,
                shrinkage=0.05,
                interaction.depth=3,
                bag.fraction = .5,
                train.fraction = 1,
                n.minobsinnode = 10,
                cv.folds = 10,
                keep.data=TRUE,
                verbose=FALSE)

Inc.gbm2$cv.error[best.iter] #92% best cv error
Inc.gbm2$train.error[best.iter]#95% best train error...

Inc.gbm2_pred=predict(Inc.gbm2,newdata=gbmdata,n.trees=best.iter,single.tree=TRUE, type="response")
Inc.gbm2_pred[1:5]
gbmdata$final_bin[1:5]

gbm_training<-confusionMatrix(ifelse(Inc.gbm2_pred>=0.5,"pos","neg"),reference=full_imputed$final_bin);gbm_training

```

```{r}
### gbm using weights with interaction depth 3. odd though that the cv and train error are better than what are presented here.
###really odd that the cv.error at best iteration or train error at best iteration don't match the predict function.t
#           Reference
# Prediction  neg  pos
#        neg 3187  313
#        pos  845 1087
#                                           
#                Accuracy : 0.7868          
#                  95% CI : (0.7757, 0.7976)
#     No Information Rate : 0.7423          
#     P-Value [Acc > NIR] : 1.054e-14       
#                                           
#                   Kappa : 0.5043          
#  Mcnemar's Test P-Value : < 2.2e-16       
#                                           
#             Sensitivity : 0.7904          
#             Specificity : 0.7764          
#          Pos Pred Value : 0.9106          
#          Neg Pred Value : 0.5626          
#              Prevalence : 0.7423          
#          Detection Rate : 0.5867          
#    Detection Prevalence : 0.6443          
#       Balanced Accuracy : 0.7834          
                                    
```


##trying lightgbm
```{r}
##can't install?>?
```



# svm
```{r}

library(e1071)
classifier = svm(formula = final_bin ~ ., 
                 data = gbmdata, 
                 type = 'C-classification', 
                 kernel = 'polynomial') 
?svm
```

```{r}
# Predicting the Test set results 
y_pred = predict(classifier, newdata = gbmdata[,-43]) 

# Making the Confusion Matrix 
cm = confusionMatrix(gbmdata[, 43], y_pred); cm
```



```{r}
# installing library ElemStatLearn 
library(ElemStatLearn) 
  
# Plotting the training data set results 
set = gbmdata 
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01) #so first two features max and min and constant sequence between them
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01) 


  
grid_set = expand.grid(X1, X2) 
colnames(grid_set) = c('amax_bun', 'amax_creatinine')
y_grid = predict(classifier, newdata = grid_set) 
  
plot(set[, -3], 
     main = 'SVM (Training set)', 
     xlab = 'Age', ylab = 'Estimated Salary', 
     xlim = range(X1), ylim = range(X2)) 
  
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) 
  
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine')) 
  
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3')) 
```




```{r}
# Inc.gbm2 <- gbm(final_bin ~ .,
#                 data=gbmdata,
#                 distribution="bernoulli",
#                 n.trees=2000,
#                 #weights = weights,
#                 shrinkage=0.01,
#                 interaction.depth=3,
#                 bag.fraction = .5,
#                 train.fraction = 1,
#                 n.minobsinnode = 10,
#                 cv.folds = 10,
#                 keep.data=TRUE,
#                 verbose=FALSE)
# 
# 
# best.iter <- gbm.perf(Inc.gbm2,method="cv");best.iter
# Inc.gbm2$cv.error[best.iter]
# summary(Inc.gbm2,n.trees=best.iter)
# Inc.gbm2$var.names
# 
# Inc.gbm2
# 
# Inc.gbm2$cv.error[best.iter]
```

interesting, the gbm model is working a little funky.


trying it with caret
```{r}
# preprocessed_imputd$final_bin= factor(preprocessed_imputd$final_bin, levels=c('0','1'), labels=c("neg","pos"))
# preprocessed_imputd_test$final_bin= factor(preprocessed_imputd_test$final_bin, levels=c('0','1'), labels=c("neg","pos"))
# 
# fitControl <- trainControl(## 10-fold CV
#                            method = "cv",
#                            number = 3,#nrow(FGL1),
#                            #returnResamp = 'none',
#                            classProbs = TRUE,
#                            summaryFunction= twoClassSummary)
#                            ## repeated ten times
# 
# gbm_tuning <-  expand.grid(n.trees = c(1000,2000), shrinkage=c(0.005,0.01, 0.02, .05, .1), interaction.depth=c(2,3,4),n.minobsinnode=c(10))
# 
# gbm_1 <- train(
#   x= preprocessed_imputd[,-c(46)],
#   y=preprocessed_imputd$final_bin,
#   method = "gbm",
#   bag.fraction = .5,
#   distribution="bernoulli",
#   verbose=FALSE,
#   metric="Spec",
#   trControl = fitControl,
#   tuneGrid= gbm_tuning)
# 
# preprocessed_imputd$final_bin
# 
# gbm_1$results %>% arrange(-Spec)
# 
# confusionMatrix(gbm_1,positive="pos",mode="everything")
# #confusionMatrix(predict(gbm_1,preprocessed_imputd),reference=preprocessed_imputd$final_bin, positive="pos" )
# gbm_test<-confusionMatrix(predict(gbm_1,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );gbm_test
```

```{r}
# 
# gbmdata<-preprocessed_imputd 
# gbmdata$final_bin<-as.numeric(gbmdata$final_bin)-1
# set.seed(12345)
# w<- 1/table(preprocessed_imputd$final_bin)
# w
# 
# weights<- rep(0,nrow(preprocessed_imputd))
# weights[preprocessed_imputd$final_bin == 0] <- w['0']
# weights[preprocessed_imputd$final_bin == 1] <- w['1']
# weights
# 
# Inc.gbm3 <- gbm(final_bin ~ .,
#                 data=gbmdata,
#                 distribution="bernoulli",
#                 n.trees=2000,
#                 #weights = weights,
#                 shrinkage=0.05,
#                 interaction.depth=2,
#                 bag.fraction = .5,
#                 train.fraction = 1,
#                 n.minobsinnode = 10,
#                 cv.folds = 10,
#                 keep.data=TRUE,
#                 class.stratify.cv = TRUE,
#                 verbose=FALSE)
# 
# 
# best.iter <- gbm.perf(Inc.gbm3,method="cv");best.iter
# Inc.gbm3$cv.error[best.iter]
# summary(Inc.gbm3,n.trees=best.iter)
# Inc.gbm3$var.names
# 
# ?gbm
# 
# 
# yhat.Inc.gbm3<-predict(Inc.gbm3,newdata=gbmdata,n.trees=best.iter, type="response");yhat.Inc.gbm3
# 
# predict(Inc.gbm3,gbmdata, type="")
# 
# confusionMatrix(predict(Inc.gbm3$,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin)
```

---
### neural network
---
```{r}
# control <- trainControl(method = "repeatedcv",
#                      number = 10,
#                      repeats = 5,
#                      classProbs = TRUE,
#                      verboseIter = FALSE,
#                      sampling= "down", #comparing up vs smote
#                      summaryFunction= twoClassSummary
#                      )
# 
# metric<-"Spec"
# #9 nnet
# set.seed(12345)
# #tuning <-  expand.grid(decay = c(0.01,0.05, 0.1, 0.5), size=c(5, 10, 15, 40))
# tuning <-  expand.grid(decay = c(0.01,0.5, 1, 5), size=c(5,15, 25, 35))
# cl <- makePSOCKcluster(cores)
# registerDoParallel(cl)
# m_nnet4 <- train(x= preprocessed_imputd[,-c(46)],
#                  y=preprocessed_imputd$final_bin,
#                  method="nnet",
#                  metric=metric,
#                  trControl=control,
#                  tuneGrid=tuning,
#                  trace=F )
# stopCluster(cl)
# 
# 
# m_nnet4$results %>% arrange(-Spec)
# predict(m_nnet4, preprocessed_imputd_test)
# 
# confusionMatrix(m_nnet4, positive="pos")
# 
# #doSNOW::registerDoSNOW()
# nnet_test<-confusionMatrix(predict(m_nnet4,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );nnet_test
```


trying with smote--- made no real difference
```{r}
# control <- trainControl(method = "repeatedcv",
#                      number = 10,
#                      repeats = 5,
#                      classProbs = TRUE,
#                      verboseIter = FALSE,
#                      sampling= "down", #comparing up vs smote
#                      summaryFunction= twoClassSummary
#                      )
# 
# set.seed(12345)
# metric<-"Sens"
# #tuning <-  expand.grid(decay = c(0.01,0.05, 0.1, 0.5), size=c(5, 10, 15, 40))
# tuning <-  expand.grid(decay = c(0.01,0.5, 1, 5), size=c(5,15, 25, 35))
# cl <- makePSOCKcluster(cores)
# registerDoParallel(cl)
# m_nnet_smote <- train(x= preprocessed_imputd[,-c(46)],
#                  y=preprocessed_imputd$final_bin,
#                  method="nnet",
#                  metric=metric,
#                  trControl=control,
#                  tuneGrid=tuning,
#                  trace=F )
# stopCluster(cl)
# 
# 
# m_nnet4$results %>% arrange(-Spec)
#  predict(m_nnet_smote, preprocessed_imputd_test)
# 
# confusionMatrix(m_nnet_smote, positive="pos")
# 
# 
# 
# #doSNOW::registerDoSNOW()
# # nnet_test2<-confusionMatrix(predict(m_nnet_smote,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );nnet_test2
```




notes:

lets go back to clinical problem. clinical problem: # of patients whose culture is negative but have a true infection not captured by regular culture. and a group of patient with neg culture which are indeed not infected (but are given prolonged antibiotics). 

way we are training this model: predict who will have a true infection. patients we are using as cases to fit model: patients who have positive infection. we will then use 
need high sensitivity and negative predictive value, so we are not calling a lot of false negatives. if has low specificity but high sensitivity, when we apply to group who will routinely recieving antibitiocis, we can stratify patients by likelyhood of having a true infection and thus tailor antibiotic course. 


try specifying class ratio in down sampling. Some of the variables are 


###change factor ordering. 






##summarizing 2classification results.

```{r}
# confusionMatrix(logit_1, positive="pos")
# rf_test<-confusionMatrix(rForest$test$predicted,reference=preprocessed_imputd_test$final_bin, positive="pos" );rf_test
# ranger1_test<-confusionMatrix(predict(ranger1,preprocessed_imputd_test)$predictions, reference= preprocessed_imputd_test$final_bin, positive="pos" );ranger1_test
# gbm_test<-confusionMatrix(predict(gbm_1,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );gbm_test
# nnet_test<-confusionMatrix(predict(m_nnet4,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );nnet_test
# nnet_test2<-confusionMatrix(predict(m_nnet_smote,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );nnet_test2
```



```{r}
# confusionMatrix(logit_1, positive="pos")
# rf_test<-confusionMatrix(rForest$test$predicted,reference=preprocessed_imputd_test$final_bin, positive="pos" );rf_test
# ranger1_test<-confusionMatrix(predict(ranger1,preprocessed_imputd_test)$predictions, reference= preprocessed_imputd_test$final_bin, positive="pos" );ranger1_test
# gbm_test<-confusionMatrix(predict(gbm_1,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );gbm_test
# nnet_test<-confusionMatrix(predict(m_nnet4,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );nnet_test
# nnet_test2<-confusionMatrix(predict(m_nnet_smote,preprocessed_imputd_test),reference=preprocessed_imputd_test$final_bin, positive="pos" );nnet_test2
```

#### important variables
```{r}
# varImp(gbm_1)
# varImp(rf_1)
# varImp(logit_1)
# 
# important_variables<-c("temperature","sysbp","daily_sofa","meanartpress","bun","platelet","ptt","heartrate","pao2fio2Ratio","creatinine","diasbp","first_admit_age")
```


-------PCA---------
```{r}
# factor_vec_2class<-names(preprocessed_imputd[,sapply(preprocessed_imputd, is.factor)])
# pca_data_2class<-preprocessed_imputd[,!names(preprocessed_imputd) %in% factor_vec_2class]
# #
# PCA_2class<-prcomp(as.matrix(pca_data_2class), scale=FALSE)
# 
# summary(PCA_2class)
# sd(pca_data_2class$bilirubin)
# #prcomp(as.matrix(pca_data), scale = TRUE) %>% summary()
# 
# df_out_2class<- as.data.frame(PCA_2class$x)
# df_out_2class$group<- preprocessed_imputd$final_bin
# 
# #par(mfrow=c(3,2))
# p1<-ggplot(df_out_2class, aes(x=PC1, y=PC2, color=group)) +geom_point()
# p2<-ggplot(df_out_2class, aes(x=PC1, y=PC3, color=group)) +geom_point()
# p3<-ggplot(df_out_2class, aes(x=PC2, y=PC3, color=group)) +geom_point()
# 
# grid.arrange(p1,p2,p3, nrow=2)

```









-----------4 CLASS MODEL BUILDING------------------



```{r}
# #converting names to be used in caret
# full_imputed$final_bin<-factor(full_imputed$final_bin, levels=c("C_neg/A_full","C_neg/A_partial","C_pos/A_full"), labels=c("C_neg.A_full","C_neg.A_partial","C_pos.A_full"))
# full_imputed_test$final_bin<-factor(full_imputed_test$final_bin, levels=c("C_neg/A_full","C_neg/A_partial","C_pos/A_full"), labels=c("C_neg.A_full","C_neg.A_partial","C_pos.A_full"))
```



#### ranger rf

repeating same thing with class weight
```{r}
# table(full_imputed$final_bin)
# 
# library(ranger)
# 
# w2 <- 1/table(full_imputed$final_bin)
# w2<- w2/sum(w2)
# w2
# weights2 <- rep(0, nrow(full_imputed))
# weights2[full_imputed$final_bin== "C_neg.A_full"] <- w2['C_neg.A_full']
# weights2[full_imputed$final_bin== "C_neg.A_partial"] <- w2['C_neg.A_partial']
# weights2[full_imputed$final_bin== "C_pos.A_full"] <- w2['C_pos.A_full']
# table(weights2, full_imputed$final_bin)
# weights2
# table(full_imputed$final_bin)
# 
# ##Sampling without replacement is important here, as otherwise samples from the smaller classes will contain many more repetitions, and the class will still be underrepresented. 
# ranger3.1<- ranger(final_bin~.,full_imputed, case.weights=weights2, replace=FALSE, keep.inbag = FALSE); ranger3.1
# ranger3.1<- ranger(final_bin~.,full_imputed, case.weights=weights2)# replace=FALSE)
# ranger3.1
# 
# 
# table(full_imputed_test$final_bin,full_imputed_test$final_bin)
# ranger3.1_test<-confusionMatrix(predict(ranger3.1,full_imputed_test)$predictions, reference= full_imputed_test$final_bin );ranger3.1_test

```


#### NNET
```{r}
# 
# control <- trainControl(method = "repeatedcv",
#                      number = 10,
#                      repeats = 5,
#                      classProbs = TRUE,
#                      verboseIter = FALSE,
#                      sampling= "down", #comparing up vs smote
#                      summaryFunction= multiClassSummary
#                      )
# 
# set.seed(12345)
# metric<-"F1"
# #tuning <-  expand.grid(decay = c(0.01,0.05, 0.1, 0.5), size=c(5, 10, 15, 40))
# tuning <-  expand.grid(decay = c(0.01,0.5, 1, 5), size=c(5,15, 25, 35))
# cl <- makePSOCKcluster(cores)
# registerDoParallel(cl)
# m_nnet_full <- train(x= full_imputed[,-c(46)],
#                  y=full_imputed$final_bin,
#                  method="nnet",
#                  metric=metric,
#                  trControl=control,
#                  tuneGrid=tuning,
#                  trace=F )
# stopCluster(cl)
# 
# m_nnet_full$results %>% arrange(logLoss)
#  # predict(m_nnet_smote, preprocessed_imputd_test)
# 
# confusionMatrix(m_nnet_full)
# 
# table(full_imputed$final_bin)
# 
# 
# 
# #doSNOW::registerDoSNOW()
# 
# m_nnet_full_test<-confusionMatrix(predict(m_nnet_full,full_imputed_test),reference=full_imputed_test$final_bin, positive="pos" );m_nnet_full_test
```






##### results summary


```{r}
# table(full_imputed_test$final_bin,full_imputed_test$final_bin)
# ranger3.1_test<-confusionMatrix(predict(ranger3.1,full_imputed_test)$predictions, reference= full_imputed_test$final_bin );ranger3.1_test
# m_nnet_full_test
```


----PCA-----


```{r}
full_imputed
table(full_imputed$final_bin)

factor_vec<-names(full_imputed[,sapply(full_imputed, is.factor)])
pca_data<-full_imputed[,!names(full_imputed) %in% factor_vec]
#
#pca_data[,2:ncol(pca_data)]

#pca_data=pca_data[,!names(pca_data)%in% c("icustay_id","height","first_admit_age","weight")]
glimpse(pca_data)
summary(exp(pca_data)-1)
PCA<-prcomp(as.matrix(((pca_data))), scale=FALSE)
as.matrix((pca_data))
summary(PCA)
PCA
#prcomp(as.matrix(pca_data), scale = TRUE) %>% summary()

df_out<- as.data.frame(PCA$x)
df_out$group<- full_imputed$final_bin

# par(mfrow=c(2,2))
p1<-ggplot(df_out, aes(x=PC1, y=PC2, color=group)) +geom_point()
p2<-ggplot(df_out, aes(x=PC1, y=PC3, color=group)) +geom_point()
p3<-ggplot(df_out, aes(x=PC2, y=PC3, color=group)) +geom_point()
p4<-ggplot(df_out, aes(x=PC3, y=PC4, color=group)) +geom_point()
# 
# grid.arrange(p1,p2,p3,p4, nrow=2)

p1 #
```

```{r}
library(psych)
pca_data
fit3 = principal(pca_data[,c(1:9)], nfactor=2)
print(fit3$loadings, cutoff=0.3, sort=TRUE)

fit4 = principal(pca_data[,c(10:18)], nfactor=2)
print(fit4$loadings, cutoff=0.3, sort=TRUE)

fit5 = principal(pca_data[,c(19:26)], nfactor=2)
print(fit5$loadings, cutoff=0.3, sort=TRUE)
plot(fit3)

fit6 = principal(pca_data[,c(28:36)], nfactor=2)
print(fit6$loadings, cutoff=0.3, sort=TRUE)
plot(fit6)

fit7 = principal(pca_data[,c(1:9,28:36)], nfactor=2)
print(fit7$loadings, cutoff=0.3, sort=TRUE)




RC<-fit3$scores
# plot(fit3, ylim=range(-0.2:1.2), labels=names(logcereal[,c(2:8,10)]))
p<-ggplot(data=data.frame(RC),aes(x=RC[,1], y=RC[,2]))+ geom_point() 
p+labs(x="RC1", y="RC2")


RC<-fit3$scores

shelf<- as.factor(full_imputed$final_bin)
p<-ggplot(data=data.frame(RC),aes(x=RC[,1], y=RC[,2]))+
  geom_point(aes(color=shelf,
                 shape= shelf)) 
p+labs(x="RC1", y="RC2") #+scale_color_manual(breaks = c("1", "2", "3"), values=c("black", "red", "green3"))
```

#trying t-sne

```{r}
## calling the installed package
train<- pca_data[,c(1:9,28:36)] ## Choose the train.csv file downloaded from the link above  
library(Rtsne)
## Curating the database for analysis with both t-SNE and PCA
Labels<-full_imputed$final_bin
train$label<-as.factor(full_imputed$final_bin)
## for plotting
colors = rainbow(length(unique(train$label)))
names(colors) = unique(train$label)

## Executing the algorithm on curated data
tsne <- Rtsne(pca_data[, !names(pca_data) %in% c("label")], dims = 2, perplexity=5, verbose=TRUE, max_iter = 500,theta=0.4, pca_center=FALSE,check_duplicates=FALSE)
exeTimeTsne<- system.time(Rtsne(pca_data[, !names(pca_data) %in% c("label")], dims = 2, perplexity=5, verbose=TRUE, max_iter = 500,theta=0.4, pca_center=FALSE,check_duplicates=FALSE))

?Rtsne

## Plotting
plot(tsne$Y, t='n', main="tsne")
text(tsne$Y, labels=train$label, col=colors[train$label])


#exploring

train

```













```{r}
# important_variables<-c("temperature","sysbp","daily_sofa","meanartpress","bun","platelet","ptt","heartrate","pao2fio2Ratio","creatinine","first_admit_age")
# #important_variables<-c("temperature","sysbp","daily_sofa","meanartpress","bun")
# 
# table(full_imputed$final_bin)
# 
# factor_vec<-names(full_imputed[,sapply(full_imputed, is.factor)])
# pca_data<-full_imputed[,!names(full_imputed) %in% factor_vec]
# pca_data<-pca_data[,names(pca_data) %in% important_variables]
# 
# #
# PCA<-prcomp(as.matrix(pca_data), scale=FALSE)
# 
# summary(PCA)
# 
# df_out<- as.data.frame(PCA$x)
# df_out$group<- full_imputed$final_bin
# 
# ggplot(df_out, aes(x=PC1, y=PC2, color=group)) +geom_point()

```

why is this streaky?


2variable:

```{r}
# factor_vec_2class<-names(preprocessed_imputd[,sapply(preprocessed_imputd, is.factor)])
# pca_data_2class<-preprocessed_imputd[,!names(preprocessed_imputd) %in% factor_vec_2class]
# pca_data_2class<-pca_data_2class[,names(pca_data_2class) %in% important_variables]
# #
# PCA_2class<-prcomp(as.matrix(pca_data_2class), scale=FALSE)
# 
# summary(PCA_2class)
# sd(pca_data_2class$bilirubin)
# #prcomp(as.matrix(pca_data), scale = TRUE) %>% summary()
# 
# df_out_2class<- as.data.frame(PCA_2class$x)
# df_out_2class$group<- preprocessed_imputd$final_bin
# 
# #par(mfrow=c(3,2))
# p1<-ggplot(df_out_2class, aes(x=PC1, y=PC2, color=group)) +geom_point()
# p2<-ggplot(df_out_2class, aes(x=PC1, y=PC3, color=group)) +geom_point()
# p3<-ggplot(df_out_2class, aes(x=PC2, y=PC3, color=group)) +geom_point()
# 
# grid.arrange(p1,p2,p3, nrow=2)
# 
# print(PCA_2class$rotation, cutoff=0.3)




```






---------CLUSTERING----------

```{r}
summary.kmeans = function(fit)
{
  p = ncol(fit$centers)
  k = nrow(fit$centers)
  n = sum(fit$size)
  sse = sum(fit$withinss)
  xbar = t(fit$centers)%*%fit$size/n
  ssb = sum(fit$size*(fit$centers - rep(1,k) %*% t(xbar))^2)
  print(data.frame(
    n=c(fit$size, n),
    Pct=(round(c(fit$size, n)/n,2)),
    round(rbind(fit$centers, t(xbar)), 2),
    RMSE = round(sqrt(c(fit$withinss/(p*fit$size-1), sse/(p*(n-k)))), 4)
  ))
  cat("SSE = ", sse, "; SSB = ", ssb, "\n")
  cat("R-Squared = ", ssb/(ssb+sse), "\n")
  cat("Pseudo F = ", (ssb/(k-1))/(sse/(n-k)), "\n\n");
  invisible(list(sse=sse, ssb=ssb, Rsqr=ssb/(ssb+sse), F=(ssb/(k-1))/(sse/(n-k))))
}

plot.kmeans = function(fit,boxplot=F)
{
  require(lattice)
  p = ncol(fit$centers)
  k = nrow(fit$centers)
  plotdat = data.frame(
    mu=as.vector(fit$centers),
    clus=factor(rep(1:k, p)),
    var=factor( 0:(p*k-1) %/% k, labels=colnames(fit$centers))
  )
print(dotplot(var~mu|clus, data=plotdat,
  panel=function(...){
    panel.dotplot(...)
    panel.abline(v=0, lwd=.1)
  },
  layout=c(k,1),
  xlab="Cluster Mean"
))
invisible(plotdat)
}
```

```{r}
glimpse(pca_data)
```

```{r looking at number of clusters}

f= double(7)
for(K in 2:8)
  F[K-1] = summary(
    kmeans(pca_data, K, nstart=100))$F
plot(2:8, F, type="b", xlab="Number of clusters")


#so will try 2-3-4-5 clustering solution
```

```{r}
f= double(7)
for(K in 2:8)
  F[K-1] = summary(
    kmeans(exp(pca_data)-1, K, nstart=100))$F
plot(2:8, F, type="b", xlab="Number of clusters")

```


```{r}
set.seed(12345)
fit <- kmeans(pca_data, centers = 2, nstart = 100)
plot(fit)

set.seed(12345)
fit <- kmeans(pca_data, centers = 3, nstart = 100)
plot(fit)

set.seed(12345)
fit <- kmeans(pca_data, centers = 4, nstart = 100)
plot(fit)

set.seed(12345)
fit <- kmeans(pca_data, centers = 5, nstart = 100)
plot(fit)

```


```{r}
set.seed(12345)
fit <- kmeans(exp(pca_data)-1, centers = 2, nstart = 100)
plot(fit)

set.seed(12345)
fit <- kmeans(exp(pca_data)-1, centers = 3, nstart = 100)
plot(fit)

set.seed(12345)
fit <- kmeans(exp(pca_data)-1, centers = 4, nstart = 100)
plot(fit)

set.seed(12345)
fit <- kmeans(exp(pca_data)-1, centers = 5, nstart = 100)
plot(fit)
```


```{r}
library(mclust)
# library(cluster) 
# library(psych)
# glimpse(full_imputed)
# 
# 
# # cluster using mclust
m.full.fit= Mclust(pca_data[,c(1:9)], G=6, modelName = "VEV")
plot(m.full.fit)
m.full.fit

?
# 
# #not useful, clustering off PCA components.
# clusplot(pca_data,k.full.fit$cluster )

```




```{r}
pca_data
fit3 = principal(pca_data[,c(1:9)], nfactor=3) #1:9
print(fit3$loadings, cutoff=0.3, sort=TRUE)
RC<-fit3$scores
# plot(fit3, ylim=range(-0.2:1.2), labels=names(logcereal[,c(2:8,10)]))
p<-ggplot(data=data.frame(RC),aes(x=RC[,1], y=RC[,2]))+ geom_point(aes(color=shelf,
                 shape= shelf))
p+labs(x="RC1", y="RC2")

shelf<- as.factor(UScereal$shelf)
p<-ggplot(data=data.frame(RC),aes(x=RC[,1], y=RC[,2]))+
  geom_point(aes(color=shelf,
                 shape= shelf)) 
p+labs(x="RC1", y="RC2") +scale_color_manual(breaks = c("1", "2", "3"), values=c("black", "red", "green3"))

```



```{r class notes}
#use python, sklearn is mainstream.

#downsampling will work ~~ as well and be easier to learn. 

#class weights


# T SNE or PCA for feature visualization. to see if features are sufficient for seperation. 

# sometimes a little visualization can help see how difficult the classification problem is.
#maybe need additional features or feature transformation
#looking at clusters for above techniques and how well seperated they are. 


#dimensionality:
#T SNE doesn't assume linear association, whereas PCA assumes a linear correlation. 

##ie spectral clustering transformation.

```






notes:

lets go back to clinical problem. clinical problem: # of patients whose culture is negative but have a true infection not captured by regular culture. and a group of patient with neg culture which are indeed not infected (but are given prolonged antibiotics). 

way we are training this model: predict who will have a true infection. patients we are using as cases to fit model: patients who have positive infection. we will then use 
need high sensitivity and negative predictive value, so we are not calling a lot of false negatives. if has low specificity but high sensitivity, when we apply to group who will routinely recieving antibitiocis, we can stratify patients by likelyhood of having a true infection and thus tailor antibiotic course. 


try specifying class ratio in down sampling. Some of the variables are 


###change factor ordering. 



approach as a.



try a longitudinal approach:
impute with last one carry forward.
lal


hybrid approach:

variables lackign longitudinal dimension: treat them as static.
variables that have longitudinal nature: can use imputation methods to fill in. 

sampling frequency:

need to visualize current data. need to see matrix of patient by time. 

for each variable visualize the patient by time matrix. 

important_variables<-c("temperature","sysbp","daily_sofa","meanartpress","bun","platelet","ptt","heartrate","pao2fio2Ratio","creatinine","first_admit_age").