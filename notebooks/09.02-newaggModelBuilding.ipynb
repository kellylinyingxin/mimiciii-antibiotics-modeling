{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# description\n",
    "\n",
    "sklearn modeling of the median imputed training data. note the preprocessing of data from 07.20-worst_case_model was performed in R (09.newagg2_preprocessing_med_impute.rmd). this eventually will be converted over to python, but for now works in r. \n",
    "\n",
    "preprocessing includes variable formatting (categorical to factor variables in r, train/test split, and median imputation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: The 'cachedir' parameter has been deprecated in version 0.12 and will be removed in version 0.14.\n",
      "You provided \"cachedir='/tmp'\", use \"location='/tmp'\" instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.metrics import classification_report\n",
    "memory = Memory(cachedir='/tmp', verbose=0)\n",
    "#@memory.cache above any def fxn.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "\n",
    "#reducing warnings that are super common in my model\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 67.2 ms\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, Imputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "#from Compute_gower_distance import select_train_samples\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, accuracy_score, auc, precision_recall_fscore_support\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from xgboost import XGBClassifier #conda install -c conda-forge xgboost to install\n",
    "\n",
    "##adding these, lets see if it helps with xgboost crash\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "RANDOM_STATE = 15485867"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 99.4 ms\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling') #use to change working directory\n",
    "wd= os.getcwd() #'/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling'\n",
    "\n",
    "date=\"04042019\"\n",
    "final_pt_df2 = pd.read_csv(Path(wd + '/data/raw/csv/04042019_final_pt_df2_v.csv') , index_col=0) #only for patients with minimum vitals\n",
    "patients= list(final_pt_df2['subject_id'].unique())\n",
    "hadm_id= list(final_pt_df2['hadm_id'].unique())\n",
    "icustay_id= list(final_pt_df2['icustay_id'].unique())\n",
    "icustay_id= [int(x) for x in icustay_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 70.2 ms\n"
     ]
    }
   ],
   "source": [
    "train_data= pd.read_csv(\"/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling/models/imputation/04042019_newagg2_median_imputed_train.csv\") #two class training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# light data reformatting for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most data are already converted to median type zscores, however weight and admit age still need to be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.356708826689592 4.499809670330265 4.200204952921578 0.29960471740868666\n",
      "time: 9.86 ms\n"
     ]
    }
   ],
   "source": [
    "weight_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).median()\n",
    "weight_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.25)#.between(train_data['col'].quantile(.25), df['col'].quantile(.75), inclusive=True)]\n",
    "weight_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.75)\n",
    "weight_iqr=weight_quant3-weight_quant1; weight_iqr\n",
    "print(weight_median,weight_quant3,weight_quant1, weight_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.194943760778217 4.367991089683742 3.9691119690666907 0.39887912061705144\n",
      "time: 9.52 ms\n"
     ]
    }
   ],
   "source": [
    "age_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).median()\n",
    "age_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.25)\n",
    "age_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.75)\n",
    "age_iqr=age_quant3-age_quant1;\n",
    "print(age_median,age_quant3,age_quant1, age_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.9 ms\n"
     ]
    }
   ],
   "source": [
    "#converting to log scaled standardized data for age/weight\n",
    "train_data['weight']=train_data['weight'].apply(lambda x: (np.log(x+1)-weight_median)/weight_iqr)\n",
    "train_data['first_admit_age']=train_data['first_admit_age'].apply(lambda x: (np.log(x+1)-age_median)/age_iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot encoding categorical var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>amax_bun</th>\n",
       "      <th>amax_creatinine</th>\n",
       "      <th>amax_daily_sofa</th>\n",
       "      <th>amax_heartrate</th>\n",
       "      <th>amax_meanartpress</th>\n",
       "      <th>amax_platelet</th>\n",
       "      <th>amax_ptt</th>\n",
       "      <th>amax_sysbp</th>\n",
       "      <th>amax_temperature</th>\n",
       "      <th>...</th>\n",
       "      <th>any_vasoactive_False</th>\n",
       "      <th>any_vasoactive_True</th>\n",
       "      <th>leukocyte_False</th>\n",
       "      <th>leukocyte_True</th>\n",
       "      <th>pao2fio2Ratio_(0, 200]</th>\n",
       "      <th>pao2fio2Ratio_(200, 333]</th>\n",
       "      <th>pao2fio2Ratio_(333, 475]</th>\n",
       "      <th>pao2fio2Ratio_(475, 3000]</th>\n",
       "      <th>vent_recieved_False</th>\n",
       "      <th>vent_recieved_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200012</td>\n",
       "      <td>0.069095</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.077448</td>\n",
       "      <td>0.047571</td>\n",
       "      <td>-0.076639</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>-0.022901</td>\n",
       "      <td>0.021964</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200014</td>\n",
       "      <td>0.056406</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>-0.207519</td>\n",
       "      <td>0.021221</td>\n",
       "      <td>0.263979</td>\n",
       "      <td>-0.067398</td>\n",
       "      <td>-0.030164</td>\n",
       "      <td>0.118889</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200033</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.122666</td>\n",
       "      <td>0.125991</td>\n",
       "      <td>-0.061462</td>\n",
       "      <td>-0.034854</td>\n",
       "      <td>0.084386</td>\n",
       "      <td>0.061749</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200036</td>\n",
       "      <td>0.136269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.153843</td>\n",
       "      <td>-0.034114</td>\n",
       "      <td>0.105303</td>\n",
       "      <td>0.078839</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200059</td>\n",
       "      <td>0.287056</td>\n",
       "      <td>0.347655</td>\n",
       "      <td>0.403677</td>\n",
       "      <td>0.127583</td>\n",
       "      <td>0.196365</td>\n",
       "      <td>0.085552</td>\n",
       "      <td>0.521840</td>\n",
       "      <td>0.139305</td>\n",
       "      <td>0.018327</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   icustay_id  amax_bun  amax_creatinine  amax_daily_sofa  amax_heartrate  \\\n",
       "0      200012  0.069095         0.076014        -0.500000        0.077448   \n",
       "1      200014  0.056406        -0.164150        -0.207519        0.021221   \n",
       "2      200033 -0.068362        -0.253202        -0.500000        0.122666   \n",
       "3      200036  0.136269         0.000000        -0.500000        0.132424   \n",
       "4      200059  0.287056         0.347655         0.403677        0.127583   \n",
       "\n",
       "   amax_meanartpress  amax_platelet  amax_ptt  amax_sysbp  amax_temperature  \\\n",
       "0           0.047571      -0.076639  0.012640   -0.022901          0.021964   \n",
       "1           0.263979      -0.067398 -0.030164    0.118889          0.003685   \n",
       "2           0.125991      -0.061462 -0.034854    0.084386          0.061749   \n",
       "3           0.153843      -0.034114  0.105303    0.078839          0.015897   \n",
       "4           0.196365       0.085552  0.521840    0.139305          0.018327   \n",
       "\n",
       "          ...          any_vasoactive_False  any_vasoactive_True  \\\n",
       "0         ...                             1                    0   \n",
       "1         ...                             1                    0   \n",
       "2         ...                             0                    1   \n",
       "3         ...                             1                    0   \n",
       "4         ...                             0                    1   \n",
       "\n",
       "   leukocyte_False  leukocyte_True  pao2fio2Ratio_(0, 200]  \\\n",
       "0                1               0                       0   \n",
       "1                1               0                       0   \n",
       "2                1               0                       0   \n",
       "3                1               0                       0   \n",
       "4                1               0                       0   \n",
       "\n",
       "   pao2fio2Ratio_(200, 333]  pao2fio2Ratio_(333, 475]  \\\n",
       "0                         0                         0   \n",
       "1                         1                         0   \n",
       "2                         0                         0   \n",
       "3                         0                         0   \n",
       "4                         1                         0   \n",
       "\n",
       "   pao2fio2Ratio_(475, 3000]  vent_recieved_False  vent_recieved_True  \n",
       "0                          1                    1                   0  \n",
       "1                          0                    0                   1  \n",
       "2                          1                    0                   1  \n",
       "3                          1                    1                   0  \n",
       "4                          0                    0                   1  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28 ms\n"
     ]
    }
   ],
   "source": [
    "cols_to_transform=['any_vasoactive', 'leukocyte', 'pao2fio2Ratio', 'vent_recieved']\n",
    "train_data = pd.get_dummies(train_data, columns = cols_to_transform )\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['icustay_id',\n",
       " 'amax_bun',\n",
       " 'amax_creatinine',\n",
       " 'amax_daily_sofa',\n",
       " 'amax_heartrate',\n",
       " 'amax_meanartpress',\n",
       " 'amax_platelet',\n",
       " 'amax_ptt',\n",
       " 'amax_sysbp',\n",
       " 'amax_temperature',\n",
       " 'amin_bun',\n",
       " 'amin_creatinine',\n",
       " 'amin_daily_sofa',\n",
       " 'amin_heartrate',\n",
       " 'amin_meanartpress',\n",
       " 'amin_platelet',\n",
       " 'amin_ptt',\n",
       " 'amin_sysbp',\n",
       " 'amin_temperature',\n",
       " 'median_bun',\n",
       " 'median_creatinine',\n",
       " 'median_daily_sofa',\n",
       " 'median_heartrate',\n",
       " 'median_meanartpress',\n",
       " 'median_platelet',\n",
       " 'median_ptt',\n",
       " 'median_sysbp',\n",
       " 'median_temperature',\n",
       " 'std_bun',\n",
       " 'std_creatinine',\n",
       " 'std_daily_sofa',\n",
       " 'std_heartrate',\n",
       " 'std_meanartpress',\n",
       " 'std_platelet',\n",
       " 'std_ptt',\n",
       " 'std_sysbp',\n",
       " 'std_temperature',\n",
       " 'first_admit_age',\n",
       " 'weight',\n",
       " 'final_bin',\n",
       " 'any_vasoactive_False',\n",
       " 'any_vasoactive_True',\n",
       " 'leukocyte_False',\n",
       " 'leukocyte_True',\n",
       " 'pao2fio2Ratio_(0, 200]',\n",
       " 'pao2fio2Ratio_(200, 333]',\n",
       " 'pao2fio2Ratio_(333, 475]',\n",
       " 'pao2fio2Ratio_(475, 3000]',\n",
       " 'vent_recieved_False',\n",
       " 'vent_recieved_True']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.61 ms\n"
     ]
    }
   ],
   "source": [
    "list(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# binarizing outcome for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "#binarizing and poping outcome for training data\n",
    "train_data.loc[train_data['final_bin']==\"C_pos/A_full\",\"final_bin\"]=1\n",
    "train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"final_bin\"]=0\n",
    "train_data['final_bin']=pd.to_numeric(train_data['final_bin'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# establishing training data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.33 ms\n"
     ]
    }
   ],
   "source": [
    "x_train= train_data.copy()\n",
    "icustay_id=x_train.pop('icustay_id')\n",
    "y_train= x_train.pop(\"final_bin\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building a sklearn pipeline\n",
    "As the name suggests, pipeline class allows sticking multiple processes into a single scikit-learn estimator. pipeline class has fit, predict and score method just like any other estimator (ex. LinearRegression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement pipeline, as usual we separate features and labels from the data-set at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we needed our data to be scaled we would apply that here, but i've already done that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 704 µs\n"
     ]
    }
   ],
   "source": [
    "# if we needed our data to be scaled we would apply that here, but i've already done that.\n",
    "# from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create a pipeline object by providing with the list of steps. \n",
    "\n",
    "Here our steps are standard scalar and support vector machine. \n",
    "\n",
    "These steps are list of tuples consisting of name and an instance of the transformer or estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 66.8 ms\n"
     ]
    }
   ],
   "source": [
    "# # steps = [('scaler', StandardScaler()), ('SVM', SVC())] #so step 1 is known as scaler, which performs StandardScaler() function on the input. \n",
    "# from sklearn.svm import SVC\n",
    "# steps = [('SVM', SVC())] #removed step 1 since i already scaled my data\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# pipeline = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.46 ms\n"
     ]
    }
   ],
   "source": [
    "# steps = [('scaler', StandardScaler()), ('SVM', SVC())] #so step 1 is known as scaler, which performs StandardScaler() function on the input. \n",
    "from sklearn.svm import SVC\n",
    "steps = [('SVM', SVC(gamma=\"scale\"))] #removed step 1 since i already scaled my data. added gamma=scale\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings (‘scaler’, ‘SVM’) can be anything, as these are just names to identify clearly the transform or estimator. We can use make_pipeline instead of Pipeline to avoid naming the estimator or transformer. The final step has to be an estimator in this list of tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we needed to do train/test split (which i've already done), we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=30, stratify=Y) #It’s necessary to use stratify as I’ve mentioned before that the labels are imbalanced as most of the wine quality falls in the range 5,6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hypertuning:\n",
    "SVM is usually optimized using two parameters gamma,C . I will discuss in an upcoming post on how they exactly work, but here let’s define a parameter grid that we will use in GridSearchCV ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 808 µs\n"
     ]
    }
   ],
   "source": [
    "parameteres = {'SVM__kernel':('linear', 'rbf'), 'SVM__C':[0.1, 1, 10]} #i think i need to include the SVM__  because i'm passing a pipeline object in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the GridSearchCV object with pipeline and the parameter space with 5 folds cross validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.3 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5) #pipeline here is basically just adding the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to fit on the training data-set and test the algorithm on the training set. Also we can find the best fit parameters for the SVM as below. \n",
    "## NOTE: i need to figure out how to extract cv misclass/ other loss parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-da18ff0567c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#print(\"score = %3.2f\") %(grid.score(x_test,y_test))\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rid' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 33.5 s\n"
     ]
    }
   ],
   "source": [
    "grid.fit(x_train, y_train)\n",
    "#print(\"score = %3.2f\") %(grid.score(x_test,y_test))\\\n",
    "print(grid.score(x_train,y_ty_trainest))\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = %s \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-956531e2358d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score = %s \"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for %: 'NoneType' and 'float'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 503 ms\n"
     ]
    }
   ],
   "source": [
    "# print(\"score = %s \") %(grid.score(x_train,y_train))\n",
    "# print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7816642120765832\n",
      "{'SVM__C': 10, 'SVM__kernel': 'linear'}\n",
      "time: 475 ms\n"
     ]
    }
   ],
   "source": [
    "print(grid.score(x_train,y_train))\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/Users/geickelb1/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.73107057, 0.79110894, 1.71803112, 0.84989548]),\n",
       " 'mean_score_time': array([0.07952285, 0.13074217, 0.07977705, 0.11922569]),\n",
       " 'mean_test_score': array([0.7757732 , 0.76859352, 0.78019146, 0.77632548]),\n",
       " 'mean_train_score': array([0.77733802, 0.77264354, 0.78221657, 0.79768044]),\n",
       " 'param_SVM__C': masked_array(data=[1, 1, 10, 10],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_SVM__kernel': masked_array(data=['linear', 'rbf', 'linear', 'rbf'],\n",
       "              mask=[False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'SVM__C': 1, 'SVM__kernel': 'linear'},\n",
       "  {'SVM__C': 1, 'SVM__kernel': 'rbf'},\n",
       "  {'SVM__C': 10, 'SVM__kernel': 'linear'},\n",
       "  {'SVM__C': 10, 'SVM__kernel': 'rbf'}],\n",
       " 'rank_test_score': array([3, 4, 1, 2], dtype=int32),\n",
       " 'split0_test_score': array([0.76724931, 0.76172953, 0.7700092 , 0.77092916]),\n",
       " 'split0_train_score': array([0.78066743, 0.7735328 , 0.78688147, 0.79838895]),\n",
       " 'split1_test_score': array([0.78012879, 0.76816927, 0.78104876, 0.78656854]),\n",
       " 'split1_train_score': array([0.77445339, 0.77054085, 0.77928654, 0.79746835]),\n",
       " 'split2_test_score': array([0.77440147, 0.77163904, 0.78729282, 0.7771639 ]),\n",
       " 'split2_train_score': array([0.77565578, 0.77266452, 0.77956742, 0.79659457]),\n",
       " 'split3_test_score': array([0.76887661, 0.76335175, 0.78084715, 0.76427256]),\n",
       " 'split3_train_score': array([0.7804878 , 0.77542568, 0.78508974, 0.79889554]),\n",
       " 'split4_test_score': array([0.78821363, 0.77808471, 0.78176796, 0.78268877]),\n",
       " 'split4_train_score': array([0.77542568, 0.77105384, 0.78025771, 0.79705476]),\n",
       " 'std_fit_time': array([0.00966474, 0.06456838, 0.1400069 , 0.02205565]),\n",
       " 'std_score_time': array([0.0025298 , 0.01561365, 0.00470826, 0.00172564]),\n",
       " 'std_test_score': array([0.00769171, 0.00590098, 0.00561776, 0.00800641]),\n",
       " 'std_train_score': array([0.00267635, 0.00175954, 0.00314505, 0.00084782])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.51 ms\n"
     ]
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.87 ms\n"
     ]
    }
   ],
   "source": [
    "(list(x_train)) #5420 x 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local methods (trying functions written by postdoc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1)Receive a sample S for testing\n",
    "\n",
    "* 2)Use gower similarity to find a cohort of K similar case samples and K similar control sample from all original samples, which constructs a cohort of 2K samples.\n",
    "\n",
    "* 3)Build a predictive model based on the similar sample cohort and predict label for sample S individually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute_Gower_Distance.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.13 ms\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from sklearn.utils import validation\n",
    "from sklearn.metrics import pairwise\n",
    "from scipy.sparse import issparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 24.2 ms\n"
     ]
    }
   ],
   "source": [
    "def _return_float_dtype(X, Y):\n",
    "    ##used in grower distance, converts values to floats for formatting.\n",
    "    \"\"\"\n",
    "    1. If dtype of X and Y is float32, then dtype float32 is returned.\n",
    "    2. Else dtype float is returned.\n",
    "    \"\"\"\n",
    "    if not issparse(X) and not isinstance(X, np.ndarray):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "    if Y is None:\n",
    "        Y_dtype = X.dtype\n",
    "    elif not issparse(Y) and not isinstance(Y, np.ndarray):\n",
    "        Y = np.asarray(Y)\n",
    "        Y_dtype = Y.dtype\n",
    "    else:\n",
    "        Y_dtype = Y.dtype\n",
    "\n",
    "    if X.dtype == Y_dtype == np.float32:\n",
    "        dtype = np.float32\n",
    "    elif X.dtype == np.object and not issparse(X):\n",
    "        dtype = np.float\n",
    "        for col in range(X.shape[1]):\n",
    "            if not np.issubdtype(type(X[0, col]), np.number):\n",
    "                dtype = np.object\n",
    "                break\n",
    "    else:\n",
    "        dtype = np.float\n",
    "    return X, Y, dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 518 µs\n"
     ]
    }
   ],
   "source": [
    "# x_train_float, y_train_float, dtype =_return_float_dtype(X=x_train, Y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.1 ms\n"
     ]
    }
   ],
   "source": [
    "def check_pairwise_arrays(X, Y, precomputed=False, dtype=None):\n",
    "    ##used in grower distance, checks x and y dimensions against each otehr.\n",
    "    X, Y, dtype_float = _return_float_dtype(X, Y)\n",
    "\n",
    "    warn_on_dtype = dtype is not None\n",
    "    estimator = 'check_pairwise_arrays'\n",
    "    if dtype is None:\n",
    "        dtype = dtype_float\n",
    "    \n",
    "    ##Input validation on an array, list, sparse matrix or similar.\n",
    "    ##By default, the input is checked to be a non-empty 2D array containing only finite values.\n",
    "    \n",
    "    if Y is X or Y is None:\n",
    "        X = Y = validation.check_array(X, accept_sparse='csr', dtype=dtype,\n",
    "                            warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
    "    else:\n",
    "        X = validation.check_array(X, accept_sparse='csr', dtype=dtype,\n",
    "                        warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
    "        Y = validation.check_array(Y, accept_sparse='csr', dtype=dtype,\n",
    "                        warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
    "\n",
    "    if precomputed:\n",
    "        if X.shape[1] != Y.shape[0]:\n",
    "            raise ValueError(\"Precomputed metric requires shape \"\n",
    "                             \"(n_queries, n_indexed). Got (%d, %d) \"\n",
    "                             \"for %d indexed.\" %\n",
    "                             (X.shape[0], X.shape[1], Y.shape[0]))\n",
    "    elif X.shape[1] != Y.shape[1]:\n",
    "        raise ValueError(\"Incompatible dimension for X and Y matrices: \"\n",
    "                         \"X.shape[1] == %d while Y.shape[1] == %d\" % (\n",
    "                             X.shape[1], Y.shape[1]))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 431 µs\n"
     ]
    }
   ],
   "source": [
    "#check_pairwise_arrays(X=x_train_float, Y=y_train_float, precomputed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 240 ms\n"
     ]
    }
   ],
   "source": [
    "def gower_distances(X, Y=None, w=None, categorical_features=None):\n",
    "    \"\"\"\n",
    "    Computes the gower distances between X and Y\n",
    "\n",
    "    Read more in the :ref:`User Guide <metrics>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "\n",
    "    Y : array-like, shape (n_samples, n_features)\n",
    "\n",
    "    w:  array-like, shape (n_features)\n",
    "    According the Gower formula, w is an attribute weight.\n",
    "\n",
    "    categorical_features: array-like, shape (n_features)\n",
    "    Indicates with True/False wheter a column is a categorical attribute.\n",
    "    This is useful when categorical atributes are represented as integer\n",
    "    values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    similarities : ndarray, shape (n_samples, )\n",
    "\n",
    "    Notes\n",
    "    ------\n",
    "    Gower is a similarity measure for categorical, boolean and numerical mixed\n",
    "    data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    X, Y = check_pairwise_arrays(X, Y, dtype=(np.object, None)[issparse(X) or\n",
    "                                                               issparse(Y)])\n",
    "    rows, cols = X.shape\n",
    "\n",
    "    if categorical_features is None:\n",
    "        categorical_features = []\n",
    "        for col in range(cols):\n",
    "            if np.issubdtype(type(X[0, col]), np.number):\n",
    "                categorical_features.append(False)\n",
    "            else:\n",
    "                categorical_features.append(True)\n",
    "    # Calculates the normalized ranges and max values of numeric values\n",
    "    ranges_of_numeric = [0.0] * cols\n",
    "    max_of_numeric = [0.0] * cols\n",
    "    for col in range(cols):\n",
    "        if not categorical_features[col]:\n",
    "            max = None\n",
    "            min = None\n",
    "            if issparse(X):\n",
    "                col_array = X.getcol(col)\n",
    "                max = col_array.max() + 0.0\n",
    "                min = col_array.min() + 0.0\n",
    "            else:\n",
    "                col_array = X[:, col].astype(np.double)\n",
    "                max = np.nanmax(col_array)\n",
    "                min = np.nanmin(col_array)\n",
    "\n",
    "            if np.isnan(max):\n",
    "                max = 0.0\n",
    "            if np.isnan(min):\n",
    "                min = 0.0\n",
    "            max_of_numeric[col] = max\n",
    "            ranges_of_numeric[col] = (1 - min / max) if (max != 0) else 0.0\n",
    "\n",
    "    if w is None:\n",
    "        w = [1] * cols\n",
    "\n",
    "    yrows, ycols = Y.shape\n",
    "\n",
    "    dm = np.zeros((rows, yrows), dtype=np.double)\n",
    "\n",
    "    for i in range(0, rows):\n",
    "        j_start = i\n",
    "\n",
    "        # for non square results\n",
    "        if rows != yrows:\n",
    "            j_start = 0\n",
    "\n",
    "        for j in range(j_start, yrows):\n",
    "            sum_sij = 0.0\n",
    "            sum_wij = 0.0\n",
    "            for col in range(cols):\n",
    "                value_xi = X[i, col]\n",
    "                value_xj = Y[j, col]\n",
    "\n",
    "                if not categorical_features[col]:\n",
    "                    if (max_of_numeric[col] != 0):\n",
    "                        value_xi = value_xi / max_of_numeric[col]\n",
    "                        value_xj = value_xj / max_of_numeric[col]\n",
    "                    else:\n",
    "                        value_xi = 0\n",
    "                        value_xj = 0\n",
    "\n",
    "                    if ranges_of_numeric[col] != 0:\n",
    "                        sij = abs(value_xi - value_xj) / ranges_of_numeric[col]\n",
    "                    else:\n",
    "                        sij = 0\n",
    "                    wij = (w[col], 0)[np.isnan(value_xi) or np.isnan(value_xj)]\n",
    "                else:\n",
    "                    sij = (1.0, 0.0)[value_xi == value_xj]\n",
    "                    wij = (w[col], 0)[value_xi is None and value_xj is None]\n",
    "                sum_sij += (wij * sij)\n",
    "                sum_wij += wij\n",
    "\n",
    "            if sum_wij != 0:\n",
    "                dm[i, j] = (sum_sij / sum_wij)\n",
    "                if j < rows and i < yrows:\n",
    "                    dm[j, i] = dm[i, j]\n",
    "    return dm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.26 ms\n"
     ]
    }
   ],
   "source": [
    "# ##testing grower distance\n",
    "# x_train1=x_train.iloc[:100,1:20]\n",
    "# x_train2=x_train.iloc[101:201,1:20]\n",
    "# print(len(x_train1), #2715\n",
    "# len(x_train2)) #2715"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 431 µs\n"
     ]
    }
   ],
   "source": [
    "# gower_distances(X=x_train1, Y=x_train2, w=None, categorical_features=None) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 110 ms\n"
     ]
    }
   ],
   "source": [
    "def select_train_samples(sample_id, all_xy, m, time_interval):# m is number of similar cases or controls\n",
    "    num_control = m   # the ratio of case and control is 1:2, 1:3,1:4\n",
    "    \n",
    "    ####not sure what this is doing.\n",
    "    if time_interval == 24:\n",
    "        top_con_variables = [False]*128\n",
    "        mid_cat_variables = [True]*5\n",
    "        age_variable = [False]\n",
    "        next_cat_variables = [True]*10\n",
    "        last_con_variables = [False]*2\n",
    "\n",
    "        flag_cate_fea = top_con_variables + mid_cat_variables + age_variable + next_cat_variables + last_con_variables # 24,48, ...., Note that, the length of 24h  is different from other hours  in terms of columns\n",
    "    else:\n",
    "        top_con_variables = [False]*129  #there is another item in other hours\n",
    "        mid_cat_variables = [True]*5\n",
    "        age_variable = [False]\n",
    "        next_cat_variables = [True]*10\n",
    "        last_con_variables = [False]*2\n",
    "\n",
    "        flag_cate_fea = top_con_variables + mid_cat_variables + age_variable + next_cat_variables + last_con_variables # 24,48, ...., Note that, the length of 24h  is different from other hours  in terms of columns\n",
    "        \n",
    "    ##all_xy = all_xy.fillna(np.nan) # fill empty with nan\n",
    "\n",
    "    x_candidate_label = all_xy.loc[sample_id] # get the object sample\n",
    "    x_candidate = x_candidate_label.drop('label')\n",
    "    x_candidate_tem = x_candidate.as_matrix()\n",
    "    testing_sample = x_candidate_tem.reshape(1, -1)  # covert into ....\n",
    "\n",
    "    all_x_candidate_tem = all_xy.drop([sample_id], axis=0, inplace=False) # delete the object sample from whole set\n",
    "\n",
    "# select similar cases\n",
    "    all_cases = all_x_candidate_tem[all_x_candidate_tem.label == 1]\n",
    "    all_cases_candidate = all_cases.drop(['label'], axis=1, inplace=False)\n",
    "    gower_candidate_case = all_cases_candidate.values[:, :] # convert into ndarray\n",
    "\n",
    "    Gower_Distance_1 = gower_distances(gower_candidate_case, testing_sample, categorical_features = flag_cate_fea) # Gower_Distance_1 is ndarray\n",
    "    Gower_Distance_2 = list(Gower_Distance_1)\n",
    "    Gower_Distance_3 = pd.Series(Gower_Distance_2, index = all_cases_candidate.index)\n",
    "    Gower_Distance_4 = Gower_Distance_3.sort_values(ascending=False)\n",
    "\n",
    "    Id_selected_cases = Gower_Distance_4.index[:m].tolist() # the id set of the top m similar samples\n",
    "\n",
    "# select similar controls\n",
    "    all_controls = all_x_candidate_tem[all_x_candidate_tem.label == 0]\n",
    "    all_controls_candidate = all_controls.drop(['label'], axis=1, inplace=False)\n",
    "    gower_candidate_control = all_controls_candidate.values[:, :] # convert into ndarray\n",
    "\n",
    "    Gower_Distance_11 = gower_distances(gower_candidate_control, testing_sample,categorical_features = flag_cate_fea) # Gower_Distance_1 is ndarray\n",
    "    Gower_Distance_22 = list(Gower_Distance_11)\n",
    "    Gower_Distance_33 = pd.Series(Gower_Distance_22, index = all_controls_candidate.index)\n",
    "    Gower_Distance_44 = Gower_Distance_33.sort_values(ascending=False)\n",
    "\n",
    "    Id_selected_controls = Gower_Distance_44.index[:num_control].tolist() # the id set of the top m similar samples\n",
    "\n",
    "    train_set_id = Id_selected_controls+Id_selected_cases\n",
    "\n",
    "    train_set_id = np.array(train_set_id)\n",
    "    return train_set_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "individualization_predictor.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 266 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#folder = '/Users/xuzhenxing/Documents/mimic_AKI_data/real_time_prediction/features/all/dropped/xy'\n",
    "# folder = './xy'\n",
    "\n",
    "\n",
    "def preprocessing(folder, time_interval, isnormalized=True):\n",
    "    \"\"\"Data preprocessing, Preprocessing  missing data with mean imputation; Normalize continous feature with MinMaxScaler;\n",
    "    Normalize categorical feature with OneHotEncoder.\n",
    "\n",
    "    Args:\n",
    "        folder: dir path of source data;\n",
    "        time_interval: interval of time, can be 24,48,72,96,120,144.\n",
    "    Returns:\n",
    "        x: features\n",
    "        y: lables\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_xy = pd.read_csv(os.path.join(folder, 'all_{}hours_test_individualization_1thousand.csv'.format(time_interval)), index_col=0)\n",
    "    # print (all_xy.shape)\n",
    "    # print (all_xy.columns)\n",
    "\n",
    "    medi = ['diuretics', 'nsaid', 'radio', 'angiotensin']\n",
    "    pat = ['gender', 'age', 'ethnicity']\n",
    "    # Total 9 comorbidity\n",
    "    comm = ['congestive_heart_failure', 'peripheral_vascular', 'hypertension',\n",
    "            'diabetes', 'liver_disease', 'mi', 'cad', 'cirrhosis', 'jaundice']\n",
    "\n",
    "    # Total 8 chartevents\n",
    "    chart = ['DiasBP_min', 'DiasBP_max', 'DiasBP_first', 'DiasBP_last', 'DiasBP_slope', 'DiasBP_avg',\n",
    "             'Glucose_min', 'Glucose_max', 'Glucose_first', 'Glucose_last', 'Glucose_slope', 'Glucose_avg',\n",
    "             'HeartRate_min', 'HeartRate_max', 'HeartRate_first', 'HeartRate_last', 'HeartRate_slope', 'HeartRate_avg',\n",
    "             'MeanBP_min', 'MeanBP_max', 'MeanBP_first', 'MeanBP_last', 'MeanBP_slope', 'MeanBP_avg',\n",
    "             'RespRate_min', 'RespRate_max', 'RespRate_first', 'RespRate_last', 'RespRate_slope', 'RespRate_avg',\n",
    "             'SpO2_min', 'SpO2_max', 'SpO2_first', 'SpO2_last', 'SpO2_slope', 'SpO2_avg',\n",
    "             'SysBP_min', 'SysBP_max', 'SysBP_first', 'SysBP_last', 'SysBP_slope', 'SysBP_avg',\n",
    "             'Temp_min', 'Temp_max', 'Temp_first', 'Temp_last', 'Temp_slope', 'Temp_avg']\n",
    "\n",
    "    # Total 12 labvents\n",
    "    lab = ['BICARBONATE_first', 'BICARBONATE_last', 'BICARBONATE_min', 'BICARBONATE_max', 'BICARBONATE_avg',\n",
    "           'BICARBONATE_slope', 'BICARBONATE_count',\n",
    "           'BUN_first', 'BUN_last', 'BUN_min', 'BUN_max', 'BUN_avg', 'BUN_slope', 'BUN_count',\n",
    "           'CHLORIDE_first', 'CHLORIDE_last', 'CHLORIDE_min', 'CHLORIDE_max', 'CHLORIDE_avg', 'CHLORIDE_slope',\n",
    "           'CHLORIDE_count',\n",
    "           'CREATININE_first', 'CREATININE_last', 'CREATININE_min', 'CREATININE_max', 'CREATININE_avg',\n",
    "           'CREATININE_slope', 'CREATININE_count',\n",
    "           'HEMOGLOBIN_first', 'HEMOGLOBIN_last', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max', 'HEMOGLOBIN_avg',\n",
    "           'HEMOGLOBIN_slope', 'HEMOGLOBIN_count',\n",
    "           'INR_first', 'INR_last', 'INR_min', 'INR_max', 'INR_avg', 'INR_count',\n",
    "           'PLATELET_first', 'PLATELET_last', 'PLATELET_min', 'PLATELET_max', 'PLATELET_avg', 'PLATELET_slope',\n",
    "           'PLATELET_count',\n",
    "           'POTASSIUM_first', 'POTASSIUM_last', 'POTASSIUM_min', 'POTASSIUM_max', 'POTASSIUM_avg', 'POTASSIUM_slope',\n",
    "           'POTASSIUM_count',\n",
    "           'PT_first', 'PT_last', 'PT_min', 'PT_max', 'PT_avg', 'PT_count',\n",
    "           'PTT_first', 'PTT_last', 'PTT_min', 'PTT_max', 'PTT_avg', 'PTT_count',\n",
    "           'WBC_first', 'WBC_last', 'WBC_min', 'WBC_max', 'WBC_avg', 'WBC_slope', 'WBC_count',\n",
    "           'CALCIUM_first', 'CALCIUM_last', 'CALCIUM_min', 'CALCIUM_max', 'CALCIUM_avg', 'CALCIUM_count'\n",
    "           ]\n",
    "\n",
    "    if time_interval != 24:  # The 24h data lack of the feature 'CALCIUM_slope'\n",
    "        lab.append('CALCIUM_slope')\n",
    "    subset = medi + pat + comm + ['avg_urine'] + ['egfr_min'] + ['label'] # note that ['avg_urine'] + ['egfr_min'] is important, ignoring if they are empty.\n",
    "\n",
    "    all_xy = all_xy.dropna(subset=subset)\n",
    "\n",
    "    # print ('after dropping nan in the catergorical variables, the shape is {}'.format(all_xy.shape))\n",
    "\n",
    "    all_conti_x = all_xy[chart + lab + ['avg_urine'] + ['egfr_min'] + ['age']]\n",
    "    # print (all_conti_x.shape)\n",
    "    # print (all_conti_x)\n",
    "    all_categ_x = all_xy[['gender'] + ['ethnicity'] + medi + comm]\n",
    "    # print (all_categ_x.shape)\n",
    "    # print (all_categ_x)\n",
    "\n",
    "    # Using mean imputer after drop the nan data in medication, patient demographic data, avg_ureine, egfr_min and label\n",
    "    imp = Imputer(strategy='mean', axis=0)\n",
    "    all_conti_x_fitted = imp.fit_transform(all_conti_x)\n",
    "\n",
    "    def normalize(all_conti_x_fitted, all_categ_x):\n",
    "        # using the MinMaxScaler to normalization the all_x\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        all_conti_x_fitted = min_max_scaler.fit_transform(all_conti_x_fitted)\n",
    "        # print (all_conti_x_fitted.shape, all_conti_x_fitted)\n",
    "        # all_conti_x = DataFrame(all_conti_x_fitted, columns=all_conti_x.columns)\n",
    "        # print (all_conti_x.shape)\n",
    "\n",
    "        onehot_enc = OneHotEncoder(sparse=False)  # dense format\n",
    "        all_categ_x_fitted = onehot_enc.fit_transform(all_categ_x)\n",
    "        # print (all_categ_x_fitted.shape, all_categ_x_fitted)\n",
    "        return all_conti_x_fitted, all_categ_x_fitted\n",
    "\n",
    "    if isnormalized:\n",
    "        all_conti_x_fitted, all_categ_x_fitted = normalize(all_conti_x_fitted, all_categ_x)\n",
    "\n",
    "    x = np.hstack((all_conti_x_fitted, all_categ_x_fitted))\n",
    "    # y = all_xy['label']\n",
    "    # x = np.array(x)\n",
    "    # y = np.array(y)\n",
    "    # print (x.shape, y.shape)\n",
    "    # return x, y\n",
    "    y = all_xy['label']\n",
    "    z_icustay_id = y.index\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z_icustay_id = np.array(z_icustay_id)\n",
    "\n",
    "    print (x.shape, y.shape)\n",
    "    return x, y, z_icustay_id, all_xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 92.4 ms\n"
     ]
    }
   ],
   "source": [
    "def perf_model(pipe, param_grid, name, X_train, X_test,\n",
    "               y_train, y_test, scoring, verbose=0):\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=scoring, cv=5, n_jobs=-1, verbose=verbose)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = gs.predict(X_train)\n",
    "    y_test_pred = gs.predict(X_test)\n",
    "\n",
    "    acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "    acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_train, gs.predict_proba(X_train)[:, 1])\n",
    "    auc_train = auc(fpr, tpr)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, gs.predict_proba(X_test)[:, 1])\n",
    "    auc_test = auc(fpr, tpr)\n",
    "\n",
    "    confmat_train = confusion_matrix(y_true=y_train, y_pred=y_train_pred)\n",
    "    confmat_test = confusion_matrix(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "    print (' best parameter: ', gs.best_params_)\n",
    "    print (' training acc:%.2f auc:%.2f ' % (acc_train, auc_train))\n",
    "    print (' testing acc:%.2f auc:%.2f ' % (acc_test, auc_test))\n",
    "\n",
    "    print (' train confusion matrix:\\n', confmat_train)\n",
    "    print (' testing confusion matrix:\\n', confmat_test)\n",
    "    print (' classification report:\\n', classification_report(y_test, y_test_pred))\n",
    "\n",
    "    train_report = np.array(precision_recall_fscore_support(y_train, y_train_pred))\n",
    "    train_class1_report = train_report[:, 1]\n",
    "    train_metrics = list(train_class1_report[:-1])\n",
    "    train_metrics.extend([acc_train, auc_train])\n",
    "    print ('training metrics: precision, recall, f1-score, acc, auc')\n",
    "    print (train_metrics)\n",
    "\n",
    "    test_report = np.array(precision_recall_fscore_support(y_test, y_test_pred))\n",
    "    test_class1_report = test_report[:, 1]\n",
    "    test_metrics = list(test_class1_report[:-1])\n",
    "    test_metrics.extend([acc_test, auc_test])\n",
    "    print ('test metrics: precision, recall, f1-score, acc, auc')\n",
    "    print (test_metrics)\n",
    "\n",
    "    return train_metrics, test_metrics\n",
    "    \"\"\"\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (recall)\")\n",
    "\n",
    "    plt.plot(fpr, tpr, label=\"acc:%f auc:%f\" % (acc_test, auc_test))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_train, gs.predict_proba(X_train)[:,1])\n",
    "    average_precision = average_precision_score(y_test, gs.predict_proba(X_test)[:,1])\n",
    "    plt.xlabel(\"precision\")\n",
    "    plt.ylabel(\"recall\")\n",
    "    plt.step(precision, recall, where='post', label='AP={0:0.2f}'.format(average_precision))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.36 ms\n"
     ]
    }
   ],
   "source": [
    "def try_dbdt(X_train, X_test, y_train, y_test, scoring):\n",
    "    gbm = GradientBoostingClassifier(learning_rate=0.05, n_estimators=120, min_samples_leaf=60,\n",
    "                                     max_features=9, subsample=0.7, random_state=10)\n",
    "\n",
    "    param_grid = {'max_depth': list(range(3, 14, 2)), 'min_samples_split': list(range(100, 801, 200))}\n",
    "    train_metrics, test_metrics = perf_model(gbm, param_grid, 'GBDT', X_train, X_test, y_train, y_test, scoring, 0)\n",
    "    return train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.8 ms\n"
     ]
    }
   ],
   "source": [
    "#issue im having is that \n",
    "\n",
    "def try_models_cross(X_train, X_test, y_train, y_test, scoring):#  select data cross 5 Fold\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, stratify=Y, random_state=RANDOM_STATE)\n",
    "    # \"\"\"\n",
    "    # print ('\\n\\nLinear Logistic Regression with L1 Penalty')\n",
    "    # lgr_l1_train_metrics, lgr_l1_test_metrics = try_lgr_l1(X_train, X_test, y_train, y_test, scoring)\n",
    "    #\n",
    "    # print ('\\n\\nLinear Logistic Regression with L2 Penalty')\n",
    "    # lgr_l2_train_metrics, lgr_l2_test_metrics = try_lgr_l2(X_train, X_test, y_train, y_test, scoring)\n",
    "    #\n",
    "    # print ('\\n\\nStochastic Gradient Descent')\n",
    "    # Elastic_train_metrics, Elastic_test_metrics = try_sgd(X_train, X_test, y_train, y_test, scoring)\n",
    "    #\n",
    "    # print ('\\n\\nRandom Forest')\n",
    "    # rf_train_metrics, rf_test_metrics = try_rf(X_train, X_test, y_train, y_test, scoring)\n",
    "    # #\n",
    "    print ('\\n\\nGradient Boosting Decision tree')\n",
    "    xgboost_train_metrics, xgboost_test_metrics = try_dbdt(X_train, X_test, y_train, y_test, scoring)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 422 µs\n"
     ]
    }
   ],
   "source": [
    "#y_train.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.46 ms\n"
     ]
    }
   ],
   "source": [
    "# #y: one hot encoding my y labels\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# y=y_train.reshape(-1,1)\n",
    "# enc.fit(y)\n",
    "# y=enc.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.65 ms\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# from sklearn.exceptions import DataConversionWarning\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "# warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formatting my data to fit his scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.19 ms\n"
     ]
    }
   ],
   "source": [
    "x_train= train_data.copy()\n",
    "icustay_id=x_train.pop('icustay_id')\n",
    "y_train= x_train.pop(\"final_bin\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10.5 ms\n"
     ]
    }
   ],
   "source": [
    "x_train= train_data.iloc[:,[1,2,3,4,5,6,7,8,9,38,39,40,41]] ###drastically reducing my dataframe size to test algorithm\n",
    "#x_train= train_data.iloc[:,[1,2,3,4,5]] ###drastically reducing my dataframe size to test algorithm\n",
    "x=np.array(x_train).copy()\n",
    "\n",
    "y=y_train.copy()\n",
    "\n",
    "z_icustay_id= icustay_id.copy()#icustay_id.index.to_series()#np.array(icustay_id)\n",
    "all_xy=x_train.set_index(z_icustay_id) #in dataframe > csv format, idk if this will be an issue. NEEDS TO HAVE ICUSTAY_ID AS INDEX\n",
    "all_xy['label']=y_train #has the outcome annotated as label\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5) #Stratified K-Folds cross-validator\n",
    "time_interval=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06909452,  0.07601444, -0.5       ,  0.07744773,  0.04757099],\n",
       "       [ 0.05640602, -0.16415031, -0.20751875,  0.02122094,  0.26397869],\n",
       "       [-0.06836176, -0.25320238, -0.5       ,  0.1226662 ,  0.12599099],\n",
       "       ...,\n",
       "       [ 0.35024169,  2.64174705,  0.16096405,  0.04745763,  0.03246678],\n",
       "       [-0.40528818, -0.25320238, -0.5       ,  0.05675854,  0.01663438],\n",
       "       [-0.35541805, -0.16415031, -1.        ,  0.05058858,  0.0724392 ]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.67 ms\n"
     ]
    }
   ],
   "source": [
    "x\n",
    "#len(y) #5432"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying his code on just one cv split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.73 ms\n"
     ]
    }
   ],
   "source": [
    "#running this prior to modeling so i can test only last split\n",
    "train_index=0\n",
    "test_index=0\n",
    "for train_index, test_index in skf.split(x, y):\n",
    "    train_index=train_index\n",
    "    test_index=test_index\n",
    "X_train_0, X_test_0 = x[train_index], x[test_index] #assigning x_train and x_test sets within this cv fold\n",
    "y_train_0, y_test_0 = y[train_index], y[test_index] #assigning y_train and y_test sets within this cv fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04319947,  0.14849202,  0.16096405, ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.2318563 ,  0.14849202, -1.        , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.04987013, -0.07991428,  0.16096405, ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.35024169,  2.64174705,  0.16096405, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.40528818, -0.25320238, -0.5       , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.35541805, -0.16415031, -1.        , ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.88 ms\n"
     ]
    }
   ],
   "source": [
    "x[train_index]\n",
    "x[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the result of non-individual predictor using xgboost:\n",
      "the Accuracy is: 1.0\n",
      "the classification_report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      1.00      1.00       280\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1086\n",
      "   macro avg       1.00      1.00      1.00      1086\n",
      "weighted avg       1.00      1.00      1.00      1086\n",
      "\n",
      "the AUC is: 0.9999999999999999\n",
      "this is the result of non-individual predictor using lr:\n",
      "the Accuracy is: 1.0\n",
      "the classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      1.00      1.00       280\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1086\n",
      "   macro avg       1.00      1.00      1.00      1086\n",
      "weighted avg       1.00      1.00      1.00      1086\n",
      "\n",
      "the AUC is: 1.0\n",
      "time: 211 ms\n"
     ]
    }
   ],
   "source": [
    "# using non-individual predictor for classification\n",
    "\n",
    "xgboost_random = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "                            min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                            objective='binary:logistic', nthread=4, scale_pos_weight=0.5, seed=27) #dropping scale weight greatly increases classification performance. 0.5 gives 79%accuracy, 77% accuracy for class=1.\n",
    "xgboost_random.fit(x[train_index], y[train_index])\n",
    "y_pred_random = xgboost_random.predict(x[test_index])\n",
    "y_proba_random = xgboost_random.predict_proba(x[test_index])[:,1]\n",
    "\n",
    "#y_test_random = y[test_index]\n",
    "\n",
    "print ('this is the result of non-individual predictor using xgboost:')\n",
    "print ('the Accuracy is:',accuracy_score(y[test_index], y_pred_random))\n",
    "print ('the classification_report:\\n', classification_report(y[test_index], y_pred_random))\n",
    "print ('the AUC is:', roc_auc_score(y[test_index], y_proba_random))\n",
    "\n",
    "logreg_random = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                            intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "logreg_random.fit(X_train_0, y_train_0)\n",
    "lr_y_pred_random = logreg_random.predict(X_test_0)\n",
    "lr_y_pred_proba_random = logreg_random.predict_proba(X_test_0)[:, 1]\n",
    "#y_test_random = y[test_index]\n",
    "\n",
    "\n",
    "print ('this is the result of non-individual predictor using lr:')\n",
    "print ('the Accuracy is:',accuracy_score(y_test_random, lr_y_pred_random))\n",
    "print ('the classification_report: \\n', classification_report(y_test_random, lr_y_pred_random))\n",
    "print ('the AUC is:', roc_auc_score(y_test_random, lr_y_pred_proba_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.34 ms\n"
     ]
    }
   ],
   "source": [
    "sum(y_pred_random -y[test_index]) #this is an issue, why am i getting this model out of my training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ugh wtf, both of my models are predicting 100% accuracy.  \n",
    "\n",
    "since i was getting good results, i wrapped it all in a function, maybe tinkered with .copy() and y_train, but all my code looks good above. \n",
    "\n",
    "i have played around with the xtrain, ytrrain, xtest, ytest\n",
    "\n",
    "i've reduced the dimensions on xtrain... ><\n",
    "\n",
    "\n",
    "\n",
    "    update: ok so i reduced xtrain down to only 5 variables and got 77% accuracy. this can't be right but it's good to know the classifier changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.27 ms\n"
     ]
    }
   ],
   "source": [
    "y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4346"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.82 ms\n"
     ]
    }
   ],
   "source": [
    "len(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.00012000e+05,  6.90945218e-02,  7.60144407e-02, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  0.00000000e+00],\n",
       "       [ 2.00014000e+05,  5.64060151e-02, -1.64150312e-01, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "       [ 2.00033000e+05, -6.83617572e-02, -2.53202377e-01, ...,\n",
       "         1.00000000e+00,  0.00000000e+00,  1.00000000e+00],\n",
       "       ...,\n",
       "       [ 2.79891000e+05,  1.36269065e-01,  6.31709361e-01, ...,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00],\n",
       "       [ 2.79924000e+05,  2.80649839e-01,  1.83632914e+00, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  0.00000000e+00],\n",
       "       [ 2.80048000e+05, -6.83617572e-02,  2.84054612e-01, ...,\n",
       "         1.00000000e+00,  1.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.87 ms\n"
     ]
    }
   ],
   "source": [
    "len(X_train_0) #4346\n",
    "X_train_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.78 ms\n"
     ]
    }
   ],
   "source": [
    "y_train_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.25 ms\n"
     ]
    }
   ],
   "source": [
    "# y_train_0\n",
    "# y_test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%\n",
      "#####################\n",
      "this is the results of the 1 fold in 5 folds:\n",
      "the number of testing samples in this fold: 1086\n",
      "time: 12.7 ms\n"
     ]
    }
   ],
   "source": [
    "print('%%%%%')\n",
    "num_fold = 0\n",
    "# for train_index, test_index in skf.split(x, y):\n",
    "#     #train_index: the index of training samples within this cv split\n",
    "#     #test_index: the index of test samples within this cv split\n",
    "\n",
    "#     X_train_0, X_test_0 = x[train_index], x[test_index] #assigning x_train and x_test sets within this cv fold\n",
    "#     y_train_0, y_test_0 = y[train_index], y[test_index] #assigning y_train and y_test sets within this cv fold\n",
    "\n",
    "print('#####################')\n",
    "\n",
    "num_fold = num_fold + 1\n",
    "print('this is the results of the {} fold in 5 folds:'.format(num_fold)) \n",
    "\n",
    "print('the number of testing samples in this fold:', test_index.size)\n",
    "\n",
    "train_z_icustay_id = z_icustay_id[train_index] # the icustay_id of samples in training set from 5 fold\n",
    "test_z_icustay_id = z_icustay_id[test_index] # the icustay_id of samples in testing set from 5 fold\n",
    "\n",
    "xg_one_fold_pred = [] # obtain the pred label of testing samples for one fold using xgboost\n",
    "xg_one_fold_proba = [] # obtain the proba  of testing samples for one fold using xgboost\n",
    "\n",
    "lr_one_fold_pred = [] # obtain the pred label of testing samples for one fold using lr\n",
    "lr_one_fold_proba = [] # obtain the proba  of testing samples for one fold using lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "def single_split_training(m=250):\n",
    "    \n",
    "    x_train= train_data.copy()\n",
    "    icustay_id=x_train.pop('icustay_id')\n",
    "    y_train= x_train.pop(\"final_bin\").values\n",
    "    \n",
    "    x_train= train_data.iloc[:,[1,2,3,4,5,6,7,8,9,38,39,40,41]].copy() ###drastically reducing my dataframe size to test algorithm\n",
    "    x=np.array(x_train)\n",
    "\n",
    "    y=y_train.copy()\n",
    "    \n",
    "    z_icustay_id= icustay_id#icustay_id.index.to_series()#np.array(icustay_id)\n",
    "    all_xy=x_train.set_index(icustay_id) #in dataframe > csv format, idk if this will be an issue. NEEDS TO HAVE ICUSTAY_ID AS INDEX\n",
    "    all_xy['label']=y_train #has the outcome annotated as label\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5) #Stratified K-Folds cross-validator\n",
    "    time_interval=4\n",
    "    \n",
    "    \n",
    "    #######\n",
    "    \n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        X_train_0, X_test_0 = x[train_index], x[test_index] #assigning x_train and x_test sets within this cv fold\n",
    "        y_train_0, y_test_0 = y[train_index], y[test_index] #assigning y_train and y_test sets within this cv fold\n",
    "    \n",
    "    #######\n",
    "    num_fold = 0\n",
    "    num_fold = num_fold + 1 ##silly to keep but it's from the loop\n",
    "    print('this is the results of the {} fold in 5 folds:'.format(num_fold)) \n",
    "\n",
    "    print('the number of testing samples in this fold:', test_index.size)\n",
    "\n",
    "    train_z_icustay_id = z_icustay_id[train_index] # the icustay_id of samples in training set from 5 fold\n",
    "    test_z_icustay_id = z_icustay_id[test_index] # the icustay_id of samples in testing set from 5 fold\n",
    "\n",
    "    xg_one_fold_pred = [] # obtain the pred label of testing samples for one fold using xgboost\n",
    "    xg_one_fold_proba = [] # obtain the proba  of testing samples for one fold using xgboost\n",
    "\n",
    "    lr_one_fold_pred = [] # obtain the pred label of testing samples for one fold using lr\n",
    "    lr_one_fold_proba = [] # obtain the proba  of testing samples for one fold using lr\n",
    "\n",
    "        ######\n",
    "\n",
    "    \n",
    "    indicator_time = 0 # the indicator\n",
    "    for i, j in zip(test_z_icustay_id, test_index):  #looping through the zipped indicies of the test indicies/test icustay_id\n",
    "\n",
    "        testing_sample_id = i #numerical index of first 1/2 of data ##??? this seems to be instead the    \n",
    "        all_xy_0 = all_xy.loc[train_z_icustay_id] # select all TRAINING samples from  the current fold using icustay_id index\n",
    "        all_xy_training = all_xy_0.append(all_xy.loc[i]) # append the current ith testing sample to the training set. \n",
    "\n",
    "        ###important parameter. was at 400, i changed to X\n",
    "        m = m  # m is the number of similar cases or similar controls\n",
    "\n",
    "        X_test_00 = x[j]\n",
    "        y_test = y[j]\n",
    "\n",
    "        X_test = X_test_00.reshape(1, -1)\n",
    "\n",
    "        # print 'start selecting......'\n",
    "\n",
    "        Id_train_set = select_train_samples(testing_sample_id, all_xy_training, m, time_interval)  #  individulization\n",
    "\n",
    "        ix = np.isin(z_icustay_id, Id_train_set)\n",
    "        Id_train_set_index = list(np.where(ix))\n",
    "\n",
    "        # Id_train_set_index = np.argwhere(z_icustay_id == Id_train_set)\n",
    "\n",
    "        X_train = x[Id_train_set_index]\n",
    "        y_train = y[Id_train_set_index]\n",
    "\n",
    "        # print 'start training......'\n",
    "\n",
    "        # scoring = 'roc_auc'\n",
    "\n",
    "    # xgboost\n",
    "\n",
    "        xgboost_mod = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "                      min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                      objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "        xgboost_mod.fit(X_train, y_train)\n",
    "        xg_y_pred = xgboost_mod.predict(X_test)\n",
    "        xg_y_pred_proba = xgboost_mod.predict_proba(X_test)[:,1]\n",
    "\n",
    "        xg_one_fold_pred.append(xg_y_pred)\n",
    "        xg_one_fold_proba.append(xg_y_pred_proba)\n",
    "\n",
    "    # lr \n",
    "\n",
    "        logreg = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                                    intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "        logreg.fit(X_train, y_train)\n",
    "        lr_y_pred = logreg.predict(X_test)\n",
    "        lr_y_pred_proba = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "        lr_one_fold_pred.append(lr_y_pred)\n",
    "        lr_one_fold_proba.append(lr_y_pred_proba)\n",
    "\n",
    "        indicator_time = indicator_time + 1\n",
    "        # print 'the next testing sample and total samples:', indicator_time, test_index.size\n",
    "\n",
    "    xg_y_individual_pred = np.array(xg_one_fold_pred)\n",
    "    xg_y_individual_proba = np.array(xg_one_fold_proba)\n",
    "\n",
    "    lr_y_individual_pred = np.array(lr_one_fold_pred)\n",
    "    lr_y_individual_proba = np.array(lr_one_fold_proba)\n",
    "\n",
    "    one_fold_y_test = y[test_index]\n",
    "\n",
    "    print ('this is the result of individual predictor using xgboost:')\n",
    "    print ('the acc of one fold:', accuracy_score(one_fold_y_test, xg_y_individual_pred))\n",
    "    print ('the classification_report :', classification_report(one_fold_y_test, xg_y_individual_pred))\n",
    "    print ('the auc of one fold:', roc_auc_score(one_fold_y_test, xg_y_individual_proba))\n",
    "\n",
    "    print ('this is the result of individual predictor using lr:')\n",
    "    print ('the acc of one fold:', accuracy_score(one_fold_y_test, lr_y_individual_pred))\n",
    "    print ('the classification_report :', classification_report(one_fold_y_test, lr_y_individual_pred))\n",
    "    print ('the auc of one fold:', roc_auc_score(one_fold_y_test, lr_y_individual_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the results of the 1 fold in 5 folds:\n",
      "the number of testing samples in this fold: 1086\n",
      "this is the result of individual predictor using xgboost:\n",
      "the acc of one fold: 1.0\n",
      "the classification_report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      1.00      1.00       280\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1086\n",
      "   macro avg       1.00      1.00      1.00      1086\n",
      "weighted avg       1.00      1.00      1.00      1086\n",
      "\n",
      "the auc of one fold: 1.0\n",
      "this is the result of individual predictor using lr:\n",
      "the acc of one fold: 1.0\n",
      "the classification_report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      1.00      1.00       280\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1086\n",
      "   macro avg       1.00      1.00      1.00      1086\n",
      "weighted avg       1.00      1.00      1.00      1086\n",
      "\n",
      "the auc of one fold: 1.0\n",
      "time: 7min 57s\n"
     ]
    }
   ],
   "source": [
    "single_split_training(m=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using m=150, took 9.1 min to run. same with 200...:\n",
    "this is the result of individual predictor using xgboost:\n",
    "the acc of one fold: 1.0\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       806\n",
    "           1       1.00      1.00      1.00       280\n",
    "\n",
    "   micro avg       1.00      1.00      1.00      1086\n",
    "   macro avg       1.00      1.00      1.00      1086\n",
    "weighted avg       1.00      1.00      1.00      1086\n",
    "\n",
    "the auc of one fold: 1.0\n",
    "this is the result of individual predictor using lr:\n",
    "the acc of one fold: 1.0\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       806\n",
    "           1       1.00      1.00      1.00       280\n",
    "\n",
    "   micro avg       1.00      1.00      1.00      1086\n",
    "   macro avg       1.00      1.00      1.00      1086\n",
    "weighted avg       1.00      1.00      1.00      1086\n",
    "\n",
    "the auc of one fold: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using m=100, took 10.5 min to run.:\n",
    "this is the result of individual predictor using xgboost:\n",
    "the acc of one fold: 1.0\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       806\n",
    "           1       1.00      1.00      1.00       280\n",
    "\n",
    "   micro avg       1.00      1.00      1.00      1086\n",
    "   macro avg       1.00      1.00      1.00      1086\n",
    "weighted avg       1.00      1.00      1.00      1086\n",
    "\n",
    "the auc of one fold: 1.0\n",
    "this is the result of individual predictor using lr:\n",
    "the acc of one fold: 1.0\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00       806\n",
    "           1       1.00      1.00      1.00       280\n",
    "\n",
    "   micro avg       1.00      1.00      1.00      1086\n",
    "   macro avg       1.00      1.00      1.00      1086\n",
    "weighted avg       1.00      1.00      1.00      1086\n",
    "\n",
    "the auc of one fold: 1.0\n",
    "time: 9min 2s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using m=400, took 10.5 min to run.:\n",
    "this is the result of individual predictor using xgboost:\n",
    "the acc of one fold: 0.567219152854512\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.48      0.62       806\n",
    "           1       0.35      0.81      0.49       280\n",
    "\n",
    "   micro avg       0.57      0.57      0.57      1086\n",
    "   macro avg       0.62      0.65      0.56      1086\n",
    "weighted avg       0.74      0.57      0.59      1086\n",
    "\n",
    "the auc of one fold: 0.7220932293512938\n",
    "this is the result of individual predictor using lr:\n",
    "the acc of one fold: 0.6233885819521179\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.87      0.58      0.70       806\n",
    "           1       0.38      0.75      0.51       280\n",
    "\n",
    "   micro avg       0.62      0.62      0.62      1086\n",
    "   macro avg       0.63      0.66      0.60      1086\n",
    "weighted avg       0.74      0.62      0.65      1086\n",
    "\n",
    "the auc of one fold: 0.6647022332506205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using m=250, took 9min to run.:\n",
    "this is the result of individual predictor using xgboost:\n",
    "the acc of one fold: 0.6390423572744015\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.86      0.62      0.72       806\n",
    "           1       0.39      0.71      0.50       280\n",
    "\n",
    "   micro avg       0.64      0.64      0.64      1086\n",
    "   macro avg       0.62      0.66      0.61      1086\n",
    "weighted avg       0.74      0.64      0.66      1086\n",
    "\n",
    "the auc of one fold: 0.7258995037220843\n",
    "this is the result of individual predictor using lr:\n",
    "the acc of one fold: 0.6712707182320442\n",
    "the classification_report :               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.84      0.69      0.76       806\n",
    "           1       0.41      0.62      0.49       280\n",
    "\n",
    "   micro avg       0.67      0.67      0.67      1086\n",
    "   macro avg       0.62      0.65      0.62      1086\n",
    "weighted avg       0.73      0.67      0.69      1086"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the result of non-individual predictor using xgboost:\n",
      "the Accuracy is: 1.0\n",
      "the classification_report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      1.00      1.00       280\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1086\n",
      "   macro avg       1.00      1.00      1.00      1086\n",
      "weighted avg       1.00      1.00      1.00      1086\n",
      "\n",
      "the AUC is: 1.0\n",
      "this is the result of non-individual predictor using lr:\n",
      "the Accuracy is: 1.0\n",
      "the classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      1.00      1.00       280\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1086\n",
      "   macro avg       1.00      1.00      1.00      1086\n",
      "weighted avg       1.00      1.00      1.00      1086\n",
      "\n",
      "the AUC is: 1.0\n",
      "time: 230 ms\n"
     ]
    }
   ],
   "source": [
    "# using non-individual predictor for classification\n",
    "\n",
    "xgboost_random = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "                            min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                            objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27) #dropping scale weight greatly increases classification performance. 0.5 gives 79%accuracy, 77% accuracy for class=1.\n",
    "xgboost_random.fit(X_train_0, y_train_0)\n",
    "y_pred_random = xgboost_random.predict(X_test_0)\n",
    "y_proba_random = xgboost_random.predict_proba(X_test_0)[:,1]\n",
    "\n",
    "y_test_random = y[test_index]\n",
    "\n",
    "print ('this is the result of non-individual predictor using xgboost:')\n",
    "print ('the Accuracy is:',accuracy_score(y_test_random, y_pred_random))\n",
    "print ('the classification_report:\\n', classification_report(y_test_random, y_pred_random))\n",
    "print ('the AUC is:', roc_auc_score(y_test_random, y_proba_random))\n",
    "\n",
    "logreg_random = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                            intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "logreg_random.fit(X_train_0, y_train_0)\n",
    "lr_y_pred_random = logreg_random.predict(X_test_0)\n",
    "lr_y_pred_proba_random = logreg_random.predict_proba(X_test_0)[:, 1]\n",
    "#y_test_random = y[test_index]\n",
    "\n",
    "\n",
    "print ('this is the result of non-individual predictor using lr:')\n",
    "print ('the Accuracy is:',accuracy_score(y_test_random, lr_y_pred_random))\n",
    "print ('the classification_report: \\n', classification_report(y_test_random, lr_y_pred_random))\n",
    "print ('the AUC is:', roc_auc_score(y_test_random, lr_y_pred_proba_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the result of non-individual predictor using lr:\n",
      "the Accuracy is: 1.0\n",
      "the classification_report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       806\n",
      "           1       1.00      1.00      1.00       280\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      1086\n",
      "   macro avg       1.00      1.00      1.00      1086\n",
      "weighted avg       1.00      1.00      1.00      1086\n",
      "\n",
      "the AUC is: 1.0\n",
      "time: 34.3 ms\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.99234101e-01, 9.99006320e-01, 9.99210966e-01, ...,\n",
       "       6.67983969e-04, 6.04632349e-04, 5.07187275e-04])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.19 ms\n"
     ]
    }
   ],
   "source": [
    "logreg_random.predict_proba(X_test_0)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.42 ms\n"
     ]
    }
   ],
   "source": [
    "logreg_random.predict(X_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04319947,  0.14849202,  0.16096405, ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.2318563 ,  0.14849202, -1.        , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.04987013, -0.07991428,  0.16096405, ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.35024169,  2.64174705,  0.16096405, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.40528818, -0.25320238, -0.5       , ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.35541805, -0.16415031, -1.        , ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.45 ms\n"
     ]
    }
   ],
   "source": [
    "X_test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%\n",
      "***************\n",
      "#####################\n",
      "this is the results of the 1 fold in 5 folds:\n",
      "the number of testing samples in this fold: 1087\n",
      "this is the result of individual predictor using xgboost:\n",
      "the acc of one fold: 0.641214351425943\n",
      "the classification_report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.64      0.73       807\n",
      "           1       0.38      0.64      0.48       280\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      1087\n",
      "   macro avg       0.61      0.64      0.60      1087\n",
      "weighted avg       0.72      0.64      0.66      1087\n",
      "\n",
      "the auc of one fold: 0.6857983713931669\n",
      "this is the result of individual predictor using lr:\n",
      "the acc of one fold: 0.6669733210671573\n",
      "the classification_report :               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.71      0.76       807\n",
      "           1       0.39      0.55      0.46       280\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      1087\n",
      "   macro avg       0.61      0.63      0.61      1087\n",
      "weighted avg       0.71      0.67      0.68      1087\n",
      "\n",
      "the auc of one fold: 0.6276132943883873\n"
     ]
    }
   ],
   "source": [
    "for time_interval in [4]:  #for his he used a bunch of different time_intervals. i may want to adopt this later on. ,48,72,96,120,144]:\n",
    "    #x, y, z_icustay_id, all_xy = preprocessing(folder, time_interval)  # all_xy is for compute gower distance\n",
    "    # x= [[1,3,4,5],[2,3,4,6],[1,3,5,8],[1,4,7,8]] ; x is numpy array, each item represents the value of feature\n",
    "    # y = [1,0,1,1] ; y is label\n",
    "    # z_icustay_id = [1234,345,678,991] ; is the id for each ICU stay\n",
    "    # all_xy contains feature, label, and icustay_id, but, all_xy is csv format ##NEEDS TO HAVE ICUSTAY_ID AS INDEX\n",
    "\n",
    "    \n",
    "    ###formatting my data to fit his scheme\n",
    "    x_train= x_train.iloc[:,[1,2,3,4,5,6,7,8,9,38,39,40,41]] ###drastically reducing my dataframe size to test algorithm\n",
    "    x=np.array(x_train)\n",
    "    #y: one hot encoding my y labels\n",
    "\n",
    "#     from sklearn.preprocessing import OneHotEncoder\n",
    "#     enc = OneHotEncoder(handle_unknown='ignore')\n",
    "#     y=y_train.reshape(-1,1)\n",
    "#     enc.fit(y)\n",
    "#     y=enc.transform(y).toarray()\n",
    "\n",
    "    y=y_train\n",
    "    \n",
    "    z_icustay_id= icustay_id#icustay_id.index.to_series()#np.array(icustay_id)\n",
    "    all_xy=x_train.set_index(icustay_id) #in dataframe > csv format, idk if this will be an issue. NEEDS TO HAVE ICUSTAY_ID AS INDEX\n",
    "    all_xy['label']=y_train #has the outcome annotated as label\n",
    "\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5) #Stratified K-Folds cross-validator\n",
    "    print('%%%%%')\n",
    "    num_fold = 0\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        #train_index: the index of training samples within this cv split\n",
    "        #test_index: the index of test samples within this cv split\n",
    "        \n",
    "        print('***************')\n",
    "        # print 'This is the '+ str(i)+' times result of '+str(n_fold)+' fold'\n",
    "        X_train_0, X_test_0 = x[train_index], x[test_index] #assigning x_train and x_test sets within this cv fold\n",
    "        y_train_0, y_test_0 = y[train_index], y[test_index] #assigning y_train and y_test sets within this cv fold\n",
    "\n",
    "        print('#####################')\n",
    "\n",
    "        num_fold = num_fold + 1\n",
    "        print('this is the results of the {} fold in 5 folds:'.format(num_fold)) \n",
    "\n",
    "        print('the number of testing samples in this fold:', test_index.size)\n",
    "\n",
    "        train_z_icustay_id = z_icustay_id[train_index] # the icustay_id of samples in training set from 5 fold\n",
    "        test_z_icustay_id = z_icustay_id[test_index] # the icustay_id of samples in testing set from 5 fold\n",
    "\n",
    "        xg_one_fold_pred = [] # obtain the pred label of testing samples for one fold using xgboost\n",
    "        xg_one_fold_proba = [] # obtain the proba  of testing samples for one fold using xgboost\n",
    "\n",
    "        lr_one_fold_pred = [] # obtain the pred label of testing samples for one fold using lr\n",
    "        lr_one_fold_proba = [] # obtain the proba  of testing samples for one fold using lr\n",
    "\n",
    "        indicator_time = 0 # the indicator\n",
    "        for i, j in zip(test_z_icustay_id, test_index):  #looping through the zipped indicies of the test indicies/test icustay_id\n",
    "\n",
    "            testing_sample_id = i #numerical index of first 1/2 of data ##??? this seems to be instead the    \n",
    "            all_xy_0 = all_xy.loc[train_z_icustay_id] # select all TRAINING samples from  the current fold using icustay_id index\n",
    "            all_xy_training = all_xy_0.append(all_xy.loc[i]) # append the current ith testing sample to the training set. \n",
    "                       \n",
    "            ###important parameter. was at 400, i changed to X\n",
    "            m = 250  # m is the number of similar cases or similar controls\n",
    "\n",
    "            X_test_00 = x[j]\n",
    "            y_test = y[j]\n",
    "\n",
    "            X_test = X_test_00.reshape(1, -1)\n",
    "\n",
    "            # print 'start selecting......'\n",
    "\n",
    "            Id_train_set = select_train_samples(testing_sample_id, all_xy_training, m, time_interval)  #  individulization\n",
    "\n",
    "            ix = np.isin(z_icustay_id, Id_train_set)\n",
    "            Id_train_set_index = list(np.where(ix))\n",
    "\n",
    "            # Id_train_set_index = np.argwhere(z_icustay_id == Id_train_set)\n",
    "\n",
    "            X_train = x[Id_train_set_index]\n",
    "            y_train = y[Id_train_set_index]\n",
    "\n",
    "            # print 'start training......'\n",
    "\n",
    "            # scoring = 'roc_auc'\n",
    "\n",
    "# xgboost\n",
    "\n",
    "            xgboost_mod = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "                          min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                          objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "            xgboost_mod.fit(X_train, y_train)\n",
    "            xg_y_pred = xgboost_mod.predict(X_test)\n",
    "            xg_y_pred_proba = xgboost_mod.predict_proba(X_test)[:,1]\n",
    "\n",
    "            xg_one_fold_pred.append(xg_y_pred)\n",
    "            xg_one_fold_proba.append(xg_y_pred_proba)\n",
    "\n",
    "# lr \n",
    "\n",
    "            logreg = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                                        intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "            logreg.fit(X_train, y_train)\n",
    "            lr_y_pred = logreg.predict(X_test)\n",
    "            lr_y_pred_proba = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "            lr_one_fold_pred.append(lr_y_pred)\n",
    "            lr_one_fold_proba.append(lr_y_pred_proba)\n",
    "\n",
    "            indicator_time = indicator_time + 1\n",
    "            # print 'the next testing sample and total samples:', indicator_time, test_index.size\n",
    "\n",
    "        xg_y_individual_pred = np.array(xg_one_fold_pred)\n",
    "        xg_y_individual_proba = np.array(xg_one_fold_proba)\n",
    "\n",
    "        lr_y_individual_pred = np.array(lr_one_fold_pred)\n",
    "        lr_y_individual_proba = np.array(lr_one_fold_proba)\n",
    "\n",
    "        one_fold_y_test = y[test_index]\n",
    "\n",
    "        print ('this is the result of individual predictor using xgboost:')\n",
    "        print ('the accuracy of one fold:', accuracy_score(one_fold_y_test, xg_y_individual_pred))\n",
    "        print ('the classification_report: \\n', classification_report(one_fold_y_test, xg_y_individual_pred))\n",
    "        print ('the AUC of one fold:', roc_auc_score(one_fold_y_test, xg_y_individual_proba))\n",
    "\n",
    "        print ('this is the result of individual predictor using lr:')\n",
    "        print ('the accuracy of one fold:', accuracy_score(one_fold_y_test, lr_y_individual_pred))\n",
    "        print ('the classification_report: \\n', classification_report(one_fold_y_test, lr_y_individual_pred))\n",
    "        print ('the AUC of one fold:', roc_auc_score(one_fold_y_test, lr_y_individual_pred))\n",
    "\n",
    "# using non-individual predictor for classification\n",
    "\n",
    "        xgboost_random = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "                                    min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                    objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "        xgboost_random.fit(X_train_0, y_train_0)\n",
    "        y_pred_random = xgboost_random.predict(X_test_0)\n",
    "        y_proba_random = xgboost_random.predict_proba(X_test_0)[:,1]\n",
    "\n",
    "        y_test_random = y[test_index]\n",
    "\n",
    "        print ('this is the result of non-individual predictor using xgboost:')\n",
    "        print ('the accuracy is:',accuracy_score(y_test_random, y_pred_random))\n",
    "        print ('the classification_report: \\n', classification_report(y_test_random, y_pred_random))\n",
    "        print ('the AUC is:', roc_auc_score(y_test_random, y_proba_random))\n",
    "\n",
    "        logreg_random = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                                    intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "        logreg_random.fit(X_train_0, y_train_0)\n",
    "        lr_y_pred_random = logreg_random.predict(X_test_0)\n",
    "        lr_y_pred_proba_random = logreg_random.predict_proba(X_test_0)[:, 1]\n",
    "\n",
    "        print ('this is the result of non-individual predictor using lr:')\n",
    "        print ('the accuracy is:',accuracy_score(y_test_random, lr_y_pred_random))\n",
    "        print ('the classification_report: \\n', classification_report(y_test_random, lr_y_pred_random))\n",
    "        print ('the AUC is:', roc_auc_score(y_test_random, lr_y_pred_proba_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07601444, -0.5       ,  0.07744773, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.16415031, -0.20751875,  0.02122094, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.25320238, -0.5       ,  0.1226662 , ...,  1.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 2.64174705,  0.16096405,  0.04745763, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.25320238, -0.5       ,  0.05675854, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.16415031, -1.        ,  0.05058858, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.54 ms\n"
     ]
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.07 ms\n"
     ]
    }
   ],
   "source": [
    "y_train_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.66 ms\n"
     ]
    }
   ],
   "source": [
    "y_test_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for i, j in zip(test_z_icustay_id, test_index):  #looping through the zipped indicies of training/test set for this cv fold. \n",
    "#     # i_index = np.where(test_z_icustay_id == i)\n",
    "#     # tem_test_z_icustay_id = np.delete(test_z_icustay_id, i_index)\n",
    "#     testing_sample_id = i #numerical index of first 1/2 of data\n",
    "\n",
    "#     all_xy_0 = all_xy.loc[train_z_icustay_id] # select training samples from  5 fold\n",
    "#     all_xy_training = all_xy_0.append(all_xy.loc[i]) # note that , containing the i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=3\n",
    "# j=4304\n",
    "# testing_sample_id = i #numerical index of first 1/2 of data\n",
    "\n",
    "# all_xy_0 = all_xy.loc[train_z_icustay_id] # select training samples from  5 fold\n",
    "# all_xy_training = all_xy_0.append(all_xy.loc[i]) # note that , containing the i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id_train_set = select_train_samples(testing_sample_id, all_xy_training, m, time_interval)\n",
    "# #testing_sample_id\n",
    "# #all_xy_training\n",
    "# #m= # m is the number of similar cases or similar controls\n",
    "# #time_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_xy_0 = all_xy.loc[train_z_icustay_id] # select training samples from  5 fold\n",
    "# all_xy_training = all_xy_0.append(all_xy.loc[i]) # note that , containing the i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.97 ms\n"
     ]
    }
   ],
   "source": [
    "# test_z_icustay_id = z_icustay_id[test_index] # the icustay_id of samples in testing set from 5 fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing training  loop without looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.92 ms\n"
     ]
    }
   ],
   "source": [
    "# #test_z_icustay_id = 0#z_icustay_id[test_index] # the icustay_id of samples in TESTING set from 5 fold\n",
    "\n",
    "# train_index_tester=0\n",
    "# test_index_tester=0\n",
    "# for train_index, test_index in skf.split(x, y_train):\n",
    "#     #print(train_index, test_index )\n",
    "#     train_index_tester=train_index\n",
    "#     test_index_tester=test_index\n",
    "#     #test_z_icustay_id=  z_icustay_id[test_index] # the icustay_id of samples in TESTING set from 5 fold\n",
    "\n",
    "    \n",
    "#     #print(train_index) #the index of training samples within this cv split\n",
    "#     #print(test_index)#the index of test samples within this cv split\n",
    "#     #print(test_z_icustay_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "this is the results of the 2 fold in 5 folds:\n",
      "the number of testing samples in this fold: 1086\n",
      "time: 12.7 ms\n"
     ]
    }
   ],
   "source": [
    "# #works\n",
    "# X_train_0, X_test_0 = x[train_index_tester], x[test_index_tester] #assigning x_train and x_test sets within this cv fold\n",
    "# y_train_0, y_test_0 = y[train_index_tester], y[test_index_tester] #assigning y_train and y_test sets within this cv fold\n",
    "\n",
    "# print('#####################')\n",
    "\n",
    "# num_fold = num_fold + 1\n",
    "# print('this is the results of the {} fold in 5 folds:'.format(num_fold)) \n",
    "\n",
    "# print('the number of testing samples in this fold:', test_index.size)\n",
    "\n",
    "# train_z_icustay_id = z_icustay_id[train_index] # the icustay_id of samples in training set from 5 fold\n",
    "# test_z_icustay_id = z_icustay_id[test_index] # the icustay_id of samples in testing set from 5 fold\n",
    "\n",
    "# xg_one_fold_pred = [] # obtain the pred label of testing samples for one fold using xgboost\n",
    "# xg_one_fold_proba = [] # obtain the proba  of testing samples for one fold using xgboost\n",
    "\n",
    "# lr_one_fold_pred = [] # obtain the pred label of testing samples for one fold using lr\n",
    "# lr_one_fold_proba = [] # obtain the proba  of testing samples for one fold using lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278924\n",
      "4298\n",
      "278932\n",
      "4299\n",
      "279048\n",
      "4303\n",
      "279057\n",
      "4304\n",
      "279084\n",
      "4305\n",
      "279089\n",
      "4306\n",
      "279153\n",
      "4310\n",
      "279172\n",
      "4311\n",
      "279261\n",
      "4317\n",
      "279334\n",
      "4322\n",
      "279396\n",
      "4323\n",
      "279413\n",
      "4325\n",
      "279443\n",
      "4331\n",
      "279496\n",
      "4335\n",
      "279804\n",
      "4354\n",
      "279925\n",
      "4360\n",
      "280054\n",
      "4362\n",
      "280112\n",
      "4363\n",
      "280124\n",
      "4364\n",
      "280162\n",
      "4365\n",
      "280168\n",
      "4366\n",
      "280185\n",
      "4367\n",
      "280191\n",
      "4368\n",
      "280193\n",
      "4369\n",
      "280199\n",
      "4370\n",
      "280210\n",
      "4371\n",
      "280230\n",
      "4372\n",
      "280231\n",
      "4373\n",
      "280236\n",
      "4374\n",
      "280237\n",
      "4375\n",
      "280245\n",
      "4376\n",
      "280264\n",
      "4377\n",
      "280317\n",
      "4378\n",
      "280321\n",
      "4379\n",
      "280322\n",
      "4380\n",
      "280353\n",
      "4381\n",
      "280377\n",
      "4382\n",
      "280389\n",
      "4383\n",
      "280390\n",
      "4384\n",
      "280391\n",
      "4385\n",
      "280411\n",
      "4386\n",
      "280433\n",
      "4387\n",
      "280435\n",
      "4388\n",
      "280445\n",
      "4389\n",
      "280492\n",
      "4390\n",
      "280540\n",
      "4391\n",
      "280545\n",
      "4392\n",
      "280552\n",
      "4393\n",
      "280569\n",
      "4394\n",
      "280576\n",
      "4395\n",
      "280595\n",
      "4396\n",
      "280602\n",
      "4397\n",
      "280605\n",
      "4398\n",
      "280608\n",
      "4399\n",
      "280630\n",
      "4400\n",
      "280635\n",
      "4401\n",
      "280673\n",
      "4402\n",
      "280682\n",
      "4403\n",
      "280701\n",
      "4404\n",
      "280715\n",
      "4405\n",
      "280773\n",
      "4406\n",
      "280783\n",
      "4407\n",
      "280788\n",
      "4408\n",
      "280829\n",
      "4409\n",
      "280834\n",
      "4410\n",
      "280844\n",
      "4411\n",
      "280888\n",
      "4412\n",
      "280892\n",
      "4413\n",
      "280899\n",
      "4414\n",
      "280900\n",
      "4415\n",
      "280928\n",
      "4416\n",
      "280943\n",
      "4417\n",
      "280948\n",
      "4418\n",
      "280951\n",
      "4419\n",
      "280957\n",
      "4420\n",
      "280963\n",
      "4421\n",
      "280967\n",
      "4422\n",
      "280982\n",
      "4423\n",
      "281000\n",
      "4424\n",
      "281085\n",
      "4425\n",
      "281092\n",
      "4426\n",
      "281098\n",
      "4427\n",
      "281099\n",
      "4428\n",
      "281125\n",
      "4429\n",
      "281137\n",
      "4430\n",
      "281151\n",
      "4431\n",
      "281155\n",
      "4432\n",
      "281164\n",
      "4433\n",
      "281223\n",
      "4434\n",
      "281231\n",
      "4435\n",
      "281325\n",
      "4436\n",
      "281340\n",
      "4437\n",
      "281361\n",
      "4438\n",
      "281396\n",
      "4439\n",
      "281420\n",
      "4440\n",
      "281528\n",
      "4441\n",
      "281543\n",
      "4442\n",
      "281555\n",
      "4443\n",
      "281559\n",
      "4444\n",
      "281572\n",
      "4445\n",
      "281585\n",
      "4446\n",
      "281590\n",
      "4447\n",
      "281619\n",
      "4448\n",
      "281626\n",
      "4449\n",
      "281689\n",
      "4450\n",
      "281736\n",
      "4451\n",
      "281743\n",
      "4452\n",
      "281770\n",
      "4453\n",
      "281809\n",
      "4454\n",
      "281839\n",
      "4455\n",
      "281841\n",
      "4456\n",
      "281856\n",
      "4457\n",
      "281894\n",
      "4458\n",
      "281905\n",
      "4459\n",
      "281906\n",
      "4460\n",
      "281907\n",
      "4461\n",
      "281968\n",
      "4462\n",
      "281984\n",
      "4463\n",
      "281987\n",
      "4464\n",
      "281990\n",
      "4465\n",
      "282000\n",
      "4466\n",
      "282018\n",
      "4467\n",
      "282091\n",
      "4468\n",
      "282104\n",
      "4469\n",
      "282112\n",
      "4470\n",
      "282154\n",
      "4471\n",
      "282170\n",
      "4472\n",
      "282214\n",
      "4473\n",
      "282245\n",
      "4474\n",
      "282247\n",
      "4475\n",
      "282258\n",
      "4476\n",
      "282395\n",
      "4477\n",
      "282396\n",
      "4478\n",
      "282403\n",
      "4479\n",
      "282442\n",
      "4480\n",
      "282445\n",
      "4481\n",
      "282467\n",
      "4482\n",
      "282469\n",
      "4483\n",
      "282506\n",
      "4484\n",
      "282515\n",
      "4485\n",
      "282516\n",
      "4486\n",
      "282542\n",
      "4487\n",
      "282563\n",
      "4488\n",
      "282580\n",
      "4489\n",
      "282609\n",
      "4490\n",
      "282646\n",
      "4491\n",
      "282651\n",
      "4492\n",
      "282652\n",
      "4493\n",
      "282672\n",
      "4494\n",
      "282677\n",
      "4495\n",
      "282707\n",
      "4496\n",
      "282716\n",
      "4497\n",
      "282735\n",
      "4498\n",
      "282781\n",
      "4499\n",
      "282793\n",
      "4500\n",
      "282804\n",
      "4501\n",
      "282816\n",
      "4502\n",
      "282822\n",
      "4503\n",
      "282824\n",
      "4504\n",
      "282836\n",
      "4505\n",
      "282846\n",
      "4506\n",
      "282855\n",
      "4507\n",
      "282891\n",
      "4508\n",
      "282902\n",
      "4509\n",
      "282943\n",
      "4510\n",
      "282956\n",
      "4511\n",
      "282972\n",
      "4512\n",
      "282984\n",
      "4513\n",
      "283009\n",
      "4514\n",
      "283019\n",
      "4515\n",
      "283042\n",
      "4516\n",
      "283067\n",
      "4517\n",
      "283102\n",
      "4518\n",
      "283132\n",
      "4519\n",
      "283153\n",
      "4520\n",
      "283166\n",
      "4521\n",
      "283180\n",
      "4522\n",
      "283184\n",
      "4523\n",
      "283187\n",
      "4524\n",
      "283198\n",
      "4525\n",
      "283214\n",
      "4526\n",
      "283217\n",
      "4527\n",
      "283238\n",
      "4528\n",
      "283260\n",
      "4529\n",
      "283261\n",
      "4530\n",
      "283293\n",
      "4531\n",
      "283295\n",
      "4532\n",
      "283318\n",
      "4533\n",
      "283330\n",
      "4534\n",
      "283341\n",
      "4535\n",
      "283362\n",
      "4536\n",
      "283363\n",
      "4537\n",
      "283382\n",
      "4538\n",
      "283439\n",
      "4539\n",
      "283477\n",
      "4540\n",
      "283478\n",
      "4541\n",
      "283495\n",
      "4542\n",
      "283527\n",
      "4543\n",
      "283529\n",
      "4544\n",
      "283545\n",
      "4545\n",
      "283551\n",
      "4546\n",
      "283552\n",
      "4547\n",
      "283588\n",
      "4548\n",
      "283590\n",
      "4549\n",
      "283612\n",
      "4550\n",
      "283666\n",
      "4551\n",
      "283704\n",
      "4552\n",
      "283706\n",
      "4553\n",
      "283707\n",
      "4554\n",
      "283713\n",
      "4555\n",
      "283746\n",
      "4556\n",
      "283756\n",
      "4557\n",
      "283757\n",
      "4558\n",
      "283764\n",
      "4559\n",
      "283777\n",
      "4560\n",
      "283819\n",
      "4561\n",
      "283824\n",
      "4562\n",
      "283826\n",
      "4563\n",
      "283829\n",
      "4564\n",
      "283836\n",
      "4565\n",
      "283859\n",
      "4566\n",
      "283894\n",
      "4567\n",
      "283903\n",
      "4568\n",
      "283924\n",
      "4569\n",
      "283967\n",
      "4570\n",
      "283969\n",
      "4571\n",
      "283973\n",
      "4572\n",
      "283977\n",
      "4573\n",
      "284027\n",
      "4574\n",
      "284036\n",
      "4575\n",
      "284067\n",
      "4576\n",
      "284072\n",
      "4577\n",
      "284112\n",
      "4578\n",
      "284127\n",
      "4579\n",
      "284129\n",
      "4580\n",
      "284165\n",
      "4581\n",
      "284171\n",
      "4582\n",
      "284179\n",
      "4583\n",
      "284182\n",
      "4584\n",
      "284184\n",
      "4585\n",
      "284192\n",
      "4586\n",
      "284200\n",
      "4587\n",
      "284215\n",
      "4588\n",
      "284225\n",
      "4589\n",
      "284239\n",
      "4590\n",
      "284258\n",
      "4591\n",
      "284265\n",
      "4592\n",
      "284321\n",
      "4593\n",
      "284326\n",
      "4594\n",
      "284334\n",
      "4595\n",
      "284344\n",
      "4596\n",
      "284373\n",
      "4597\n",
      "284381\n",
      "4598\n",
      "284405\n",
      "4599\n",
      "284429\n",
      "4600\n",
      "284464\n",
      "4601\n",
      "284481\n",
      "4602\n",
      "284487\n",
      "4603\n",
      "284495\n",
      "4604\n",
      "284503\n",
      "4605\n",
      "284532\n",
      "4606\n",
      "284535\n",
      "4607\n",
      "284540\n",
      "4608\n",
      "284541\n",
      "4609\n",
      "284544\n",
      "4610\n",
      "284548\n",
      "4611\n",
      "284579\n",
      "4612\n",
      "284612\n",
      "4613\n",
      "284621\n",
      "4614\n",
      "284643\n",
      "4615\n",
      "284681\n",
      "4616\n",
      "284690\n",
      "4617\n",
      "284700\n",
      "4618\n",
      "284757\n",
      "4619\n",
      "284773\n",
      "4620\n",
      "284776\n",
      "4621\n",
      "284792\n",
      "4622\n",
      "284823\n",
      "4623\n",
      "284880\n",
      "4624\n",
      "284899\n",
      "4625\n",
      "284903\n",
      "4626\n",
      "284913\n",
      "4627\n",
      "284950\n",
      "4628\n",
      "284953\n",
      "4629\n",
      "284983\n",
      "4630\n",
      "284993\n",
      "4631\n",
      "284994\n",
      "4632\n",
      "285002\n",
      "4633\n",
      "285051\n",
      "4634\n",
      "285054\n",
      "4635\n",
      "285061\n",
      "4636\n",
      "285089\n",
      "4637\n",
      "285112\n",
      "4638\n",
      "285114\n",
      "4639\n",
      "285151\n",
      "4640\n",
      "285158\n",
      "4641\n",
      "285170\n",
      "4642\n",
      "285191\n",
      "4643\n",
      "285194\n",
      "4644\n",
      "285270\n",
      "4645\n",
      "285276\n",
      "4646\n",
      "285279\n",
      "4647\n",
      "285299\n",
      "4648\n",
      "285308\n",
      "4649\n",
      "285309\n",
      "4650\n",
      "285326\n",
      "4651\n",
      "285335\n",
      "4652\n",
      "285348\n",
      "4653\n",
      "285373\n",
      "4654\n",
      "285383\n",
      "4655\n",
      "285385\n",
      "4656\n",
      "285452\n",
      "4657\n",
      "285453\n",
      "4658\n",
      "285488\n",
      "4659\n",
      "285531\n",
      "4660\n",
      "285537\n",
      "4661\n",
      "285549\n",
      "4662\n",
      "285564\n",
      "4663\n",
      "285630\n",
      "4664\n",
      "285660\n",
      "4665\n",
      "285664\n",
      "4666\n",
      "285697\n",
      "4667\n",
      "285700\n",
      "4668\n",
      "285717\n",
      "4669\n",
      "285722\n",
      "4670\n",
      "285754\n",
      "4671\n",
      "285783\n",
      "4672\n",
      "285789\n",
      "4673\n",
      "285797\n",
      "4674\n",
      "285826\n",
      "4675\n",
      "285859\n",
      "4676\n",
      "285934\n",
      "4677\n",
      "285947\n",
      "4678\n",
      "285980\n",
      "4679\n",
      "285981\n",
      "4680\n",
      "285988\n",
      "4681\n",
      "286002\n",
      "4682\n",
      "286042\n",
      "4683\n",
      "286055\n",
      "4684\n",
      "286061\n",
      "4685\n",
      "286071\n",
      "4686\n",
      "286098\n",
      "4687\n",
      "286103\n",
      "4688\n",
      "286106\n",
      "4689\n",
      "286113\n",
      "4690\n",
      "286123\n",
      "4691\n",
      "286149\n",
      "4692\n",
      "286159\n",
      "4693\n",
      "286178\n",
      "4694\n",
      "286183\n",
      "4695\n",
      "286188\n",
      "4696\n",
      "286252\n",
      "4697\n",
      "286254\n",
      "4698\n",
      "286276\n",
      "4699\n",
      "286300\n",
      "4700\n",
      "286326\n",
      "4701\n",
      "286346\n",
      "4702\n",
      "286367\n",
      "4703\n",
      "286369\n",
      "4704\n",
      "286393\n",
      "4705\n",
      "286410\n",
      "4706\n",
      "286433\n",
      "4707\n",
      "286444\n",
      "4708\n",
      "286454\n",
      "4709\n",
      "286499\n",
      "4710\n",
      "286523\n",
      "4711\n",
      "286531\n",
      "4712\n",
      "286534\n",
      "4713\n",
      "286561\n",
      "4714\n",
      "286565\n",
      "4715\n",
      "286570\n",
      "4716\n",
      "286586\n",
      "4717\n",
      "286600\n",
      "4718\n",
      "286613\n",
      "4719\n",
      "286621\n",
      "4720\n",
      "286628\n",
      "4721\n",
      "286629\n",
      "4722\n",
      "286636\n",
      "4723\n",
      "286656\n",
      "4724\n",
      "286677\n",
      "4725\n",
      "286698\n",
      "4726\n",
      "286706\n",
      "4727\n",
      "286710\n",
      "4728\n",
      "286713\n",
      "4729\n",
      "286722\n",
      "4730\n",
      "286756\n",
      "4731\n",
      "286773\n",
      "4732\n",
      "286778\n",
      "4733\n",
      "286792\n",
      "4734\n",
      "286803\n",
      "4735\n",
      "286832\n",
      "4736\n",
      "286851\n",
      "4737\n",
      "286858\n",
      "4738\n",
      "286863\n",
      "4739\n",
      "286864\n",
      "4740\n",
      "286867\n",
      "4741\n",
      "286886\n",
      "4742\n",
      "286933\n",
      "4743\n",
      "286941\n",
      "4744\n",
      "286952\n",
      "4745\n",
      "286956\n",
      "4746\n",
      "286962\n",
      "4747\n",
      "286983\n",
      "4748\n",
      "287008\n",
      "4749\n",
      "287039\n",
      "4750\n",
      "287042\n",
      "4751\n",
      "287054\n",
      "4752\n",
      "287059\n",
      "4753\n",
      "287085\n",
      "4754\n",
      "287101\n",
      "4755\n",
      "287117\n",
      "4756\n",
      "287125\n",
      "4757\n",
      "287130\n",
      "4758\n",
      "287170\n",
      "4759\n",
      "287191\n",
      "4760\n",
      "287212\n",
      "4761\n",
      "287246\n",
      "4762\n",
      "287261\n",
      "4763\n",
      "287277\n",
      "4764\n",
      "287291\n",
      "4765\n",
      "287296\n",
      "4766\n",
      "287326\n",
      "4767\n",
      "287361\n",
      "4768\n",
      "287365\n",
      "4769\n",
      "287369\n",
      "4770\n",
      "287378\n",
      "4771\n",
      "287500\n",
      "4772\n",
      "287546\n",
      "4773\n",
      "287637\n",
      "4774\n",
      "287657\n",
      "4775\n",
      "287661\n",
      "4776\n",
      "287687\n",
      "4777\n",
      "287691\n",
      "4778\n",
      "287702\n",
      "4779\n",
      "287725\n",
      "4780\n",
      "287743\n",
      "4781\n",
      "287767\n",
      "4782\n",
      "287795\n",
      "4783\n",
      "287827\n",
      "4784\n",
      "287870\n",
      "4785\n",
      "287875\n",
      "4786\n",
      "287886\n",
      "4787\n",
      "287890\n",
      "4788\n",
      "287913\n",
      "4789\n",
      "287920\n",
      "4790\n",
      "287931\n",
      "4791\n",
      "288018\n",
      "4792\n",
      "288024\n",
      "4793\n",
      "288086\n",
      "4794\n",
      "288116\n",
      "4795\n",
      "288155\n",
      "4796\n",
      "288179\n",
      "4797\n",
      "288209\n",
      "4798\n",
      "288234\n",
      "4799\n",
      "288238\n",
      "4800\n",
      "288240\n",
      "4801\n",
      "288281\n",
      "4802\n",
      "288306\n",
      "4803\n",
      "288341\n",
      "4804\n",
      "288355\n",
      "4805\n",
      "288392\n",
      "4806\n",
      "288403\n",
      "4807\n",
      "288404\n",
      "4808\n",
      "288415\n",
      "4809\n",
      "288423\n",
      "4810\n",
      "288468\n",
      "4811\n",
      "288492\n",
      "4812\n",
      "288514\n",
      "4813\n",
      "288551\n",
      "4814\n",
      "288559\n",
      "4815\n",
      "288564\n",
      "4816\n",
      "288565\n",
      "4817\n",
      "288566\n",
      "4818\n",
      "288579\n",
      "4819\n",
      "288595\n",
      "4820\n",
      "288605\n",
      "4821\n",
      "288628\n",
      "4822\n",
      "288629\n",
      "4823\n",
      "288673\n",
      "4824\n",
      "288724\n",
      "4825\n",
      "288751\n",
      "4826\n",
      "288772\n",
      "4827\n",
      "288798\n",
      "4828\n",
      "288829\n",
      "4829\n",
      "288846\n",
      "4830\n",
      "288884\n",
      "4831\n",
      "288900\n",
      "4832\n",
      "288916\n",
      "4833\n",
      "288929\n",
      "4834\n",
      "288945\n",
      "4835\n",
      "288971\n",
      "4836\n",
      "288976\n",
      "4837\n",
      "288996\n",
      "4838\n",
      "289003\n",
      "4839\n",
      "289009\n",
      "4840\n",
      "289012\n",
      "4841\n",
      "289039\n",
      "4842\n",
      "289080\n",
      "4843\n",
      "289123\n",
      "4844\n",
      "289149\n",
      "4845\n",
      "289193\n",
      "4846\n",
      "289231\n",
      "4847\n",
      "289276\n",
      "4848\n",
      "289289\n",
      "4849\n",
      "289320\n",
      "4850\n",
      "289326\n",
      "4851\n",
      "289331\n",
      "4852\n",
      "289338\n",
      "4853\n",
      "289391\n",
      "4854\n",
      "289400\n",
      "4855\n",
      "289413\n",
      "4856\n",
      "289414\n",
      "4857\n",
      "289488\n",
      "4858\n",
      "289491\n",
      "4859\n",
      "289500\n",
      "4860\n",
      "289510\n",
      "4861\n",
      "289541\n",
      "4862\n",
      "289579\n",
      "4863\n",
      "289587\n",
      "4864\n",
      "289597\n",
      "4865\n",
      "289616\n",
      "4866\n",
      "289625\n",
      "4867\n",
      "289645\n",
      "4868\n",
      "289668\n",
      "4869\n",
      "289689\n",
      "4870\n",
      "289810\n",
      "4871\n",
      "289813\n",
      "4872\n",
      "289845\n",
      "4873\n",
      "289874\n",
      "4874\n",
      "289880\n",
      "4875\n",
      "289885\n",
      "4876\n",
      "289889\n",
      "4877\n",
      "289918\n",
      "4878\n",
      "289937\n",
      "4879\n",
      "289961\n",
      "4880\n",
      "289971\n",
      "4881\n",
      "289982\n",
      "4882\n",
      "289992\n",
      "4883\n",
      "289993\n",
      "4884\n",
      "289999\n",
      "4885\n",
      "290007\n",
      "4886\n",
      "290009\n",
      "4887\n",
      "290053\n",
      "4888\n",
      "290057\n",
      "4889\n",
      "290058\n",
      "4890\n",
      "290069\n",
      "4891\n",
      "290071\n",
      "4892\n",
      "290079\n",
      "4893\n",
      "290124\n",
      "4894\n",
      "290167\n",
      "4895\n",
      "290181\n",
      "4896\n",
      "290198\n",
      "4897\n",
      "290227\n",
      "4898\n",
      "290242\n",
      "4899\n",
      "290288\n",
      "4900\n",
      "290310\n",
      "4901\n",
      "290337\n",
      "4902\n",
      "290359\n",
      "4903\n",
      "290365\n",
      "4904\n",
      "290369\n",
      "4905\n",
      "290373\n",
      "4906\n",
      "290389\n",
      "4907\n",
      "290431\n",
      "4908\n",
      "290440\n",
      "4909\n",
      "290444\n",
      "4910\n",
      "290478\n",
      "4911\n",
      "290485\n",
      "4912\n",
      "290511\n",
      "4913\n",
      "290524\n",
      "4914\n",
      "290582\n",
      "4915\n",
      "290586\n",
      "4916\n",
      "290601\n",
      "4917\n",
      "290602\n",
      "4918\n",
      "290604\n",
      "4919\n",
      "290622\n",
      "4920\n",
      "290640\n",
      "4921\n",
      "290644\n",
      "4922\n",
      "290659\n",
      "4923\n",
      "290662\n",
      "4924\n",
      "290664\n",
      "4925\n",
      "290672\n",
      "4926\n",
      "290677\n",
      "4927\n",
      "290689\n",
      "4928\n",
      "290721\n",
      "4929\n",
      "290748\n",
      "4930\n",
      "290835\n",
      "4931\n",
      "290883\n",
      "4932\n",
      "290910\n",
      "4933\n",
      "290943\n",
      "4934\n",
      "290948\n",
      "4935\n",
      "291003\n",
      "4936\n",
      "291016\n",
      "4937\n",
      "291069\n",
      "4938\n",
      "291074\n",
      "4939\n",
      "291090\n",
      "4940\n",
      "291107\n",
      "4941\n",
      "291117\n",
      "4942\n",
      "291141\n",
      "4943\n",
      "291142\n",
      "4944\n",
      "291165\n",
      "4945\n",
      "291208\n",
      "4946\n",
      "291228\n",
      "4947\n",
      "291232\n",
      "4948\n",
      "291283\n",
      "4949\n",
      "291306\n",
      "4950\n",
      "291331\n",
      "4951\n",
      "291381\n",
      "4952\n",
      "291390\n",
      "4953\n",
      "291413\n",
      "4954\n",
      "291423\n",
      "4955\n",
      "291441\n",
      "4956\n",
      "291444\n",
      "4957\n",
      "291446\n",
      "4958\n",
      "291455\n",
      "4959\n",
      "291457\n",
      "4960\n",
      "291479\n",
      "4961\n",
      "291487\n",
      "4962\n",
      "291491\n",
      "4963\n",
      "291503\n",
      "4964\n",
      "291505\n",
      "4965\n",
      "291523\n",
      "4966\n",
      "291567\n",
      "4967\n",
      "291601\n",
      "4968\n",
      "291603\n",
      "4969\n",
      "291604\n",
      "4970\n",
      "291628\n",
      "4971\n",
      "291657\n",
      "4972\n",
      "291688\n",
      "4973\n",
      "291722\n",
      "4974\n",
      "291749\n",
      "4975\n",
      "291777\n",
      "4976\n",
      "291793\n",
      "4977\n",
      "291799\n",
      "4978\n",
      "291801\n",
      "4979\n",
      "291825\n",
      "4980\n",
      "291847\n",
      "4981\n",
      "291858\n",
      "4982\n",
      "291859\n",
      "4983\n",
      "291893\n",
      "4984\n",
      "291908\n",
      "4985\n",
      "291929\n",
      "4986\n",
      "291930\n",
      "4987\n",
      "291965\n",
      "4988\n",
      "291983\n",
      "4989\n",
      "292021\n",
      "4990\n",
      "292027\n",
      "4991\n",
      "292052\n",
      "4992\n",
      "292125\n",
      "4993\n",
      "292140\n",
      "4994\n",
      "292143\n",
      "4995\n",
      "292153\n",
      "4996\n",
      "292165\n",
      "4997\n",
      "292180\n",
      "4998\n",
      "292214\n",
      "4999\n",
      "292218\n",
      "5000\n",
      "292226\n",
      "5001\n",
      "292254\n",
      "5002\n",
      "292274\n",
      "5003\n",
      "292284\n",
      "5004\n",
      "292297\n",
      "5005\n",
      "292308\n",
      "5006\n",
      "292341\n",
      "5007\n",
      "292355\n",
      "5008\n",
      "292381\n",
      "5009\n",
      "292409\n",
      "5010\n",
      "292410\n",
      "5011\n",
      "292449\n",
      "5012\n",
      "292467\n",
      "5013\n",
      "292476\n",
      "5014\n",
      "292494\n",
      "5015\n",
      "292507\n",
      "5016\n",
      "292515\n",
      "5017\n",
      "292533\n",
      "5018\n",
      "292588\n",
      "5019\n",
      "292600\n",
      "5020\n",
      "292622\n",
      "5021\n",
      "292668\n",
      "5022\n",
      "292685\n",
      "5023\n",
      "292767\n",
      "5024\n",
      "292769\n",
      "5025\n",
      "292775\n",
      "5026\n",
      "292792\n",
      "5027\n",
      "292840\n",
      "5028\n",
      "292841\n",
      "5029\n",
      "292866\n",
      "5030\n",
      "292867\n",
      "5031\n",
      "292870\n",
      "5032\n",
      "292898\n",
      "5033\n",
      "292910\n",
      "5034\n",
      "292936\n",
      "5035\n",
      "292954\n",
      "5036\n",
      "292962\n",
      "5037\n",
      "292972\n",
      "5038\n",
      "293002\n",
      "5039\n",
      "293045\n",
      "5040\n",
      "293050\n",
      "5041\n",
      "293071\n",
      "5042\n",
      "293087\n",
      "5043\n",
      "293118\n",
      "5044\n",
      "293146\n",
      "5045\n",
      "293148\n",
      "5046\n",
      "293150\n",
      "5047\n",
      "293178\n",
      "5048\n",
      "293185\n",
      "5049\n",
      "293223\n",
      "5050\n",
      "293246\n",
      "5051\n",
      "293295\n",
      "5052\n",
      "293300\n",
      "5053\n",
      "293309\n",
      "5054\n",
      "293317\n",
      "5055\n",
      "293320\n",
      "5056\n",
      "293330\n",
      "5057\n",
      "293345\n",
      "5058\n",
      "293348\n",
      "5059\n",
      "293364\n",
      "5060\n",
      "293392\n",
      "5061\n",
      "293393\n",
      "5062\n",
      "293457\n",
      "5063\n",
      "293470\n",
      "5064\n",
      "293482\n",
      "5065\n",
      "293525\n",
      "5066\n",
      "293567\n",
      "5067\n",
      "293575\n",
      "5068\n",
      "293638\n",
      "5069\n",
      "293640\n",
      "5070\n",
      "293646\n",
      "5071\n",
      "293673\n",
      "5072\n",
      "293674\n",
      "5073\n",
      "293688\n",
      "5074\n",
      "293705\n",
      "5075\n",
      "293718\n",
      "5076\n",
      "293735\n",
      "5077\n",
      "293745\n",
      "5078\n",
      "293786\n",
      "5079\n",
      "293838\n",
      "5080\n",
      "293841\n",
      "5081\n",
      "293876\n",
      "5082\n",
      "293948\n",
      "5083\n",
      "293956\n",
      "5084\n",
      "293958\n",
      "5085\n",
      "293962\n",
      "5086\n",
      "294026\n",
      "5087\n",
      "294033\n",
      "5088\n",
      "294035\n",
      "5089\n",
      "294038\n",
      "5090\n",
      "294047\n",
      "5091\n",
      "294060\n",
      "5092\n",
      "294064\n",
      "5093\n",
      "294072\n",
      "5094\n",
      "294078\n",
      "5095\n",
      "294084\n",
      "5096\n",
      "294116\n",
      "5097\n",
      "294119\n",
      "5098\n",
      "294140\n",
      "5099\n",
      "294189\n",
      "5100\n",
      "294198\n",
      "5101\n",
      "294206\n",
      "5102\n",
      "294229\n",
      "5103\n",
      "294255\n",
      "5104\n",
      "294273\n",
      "5105\n",
      "294278\n",
      "5106\n",
      "294299\n",
      "5107\n",
      "294310\n",
      "5108\n",
      "294320\n",
      "5109\n",
      "294335\n",
      "5110\n",
      "294354\n",
      "5111\n",
      "294369\n",
      "5112\n",
      "294370\n",
      "5113\n",
      "294398\n",
      "5114\n",
      "294413\n",
      "5115\n",
      "294420\n",
      "5116\n",
      "294421\n",
      "5117\n",
      "294431\n",
      "5118\n",
      "294458\n",
      "5119\n",
      "294474\n",
      "5120\n",
      "294522\n",
      "5121\n",
      "294551\n",
      "5122\n",
      "294555\n",
      "5123\n",
      "294556\n",
      "5124\n",
      "294575\n",
      "5125\n",
      "294610\n",
      "5126\n",
      "294624\n",
      "5127\n",
      "294634\n",
      "5128\n",
      "294654\n",
      "5129\n",
      "294691\n",
      "5130\n",
      "294696\n",
      "5131\n",
      "294713\n",
      "5132\n",
      "294725\n",
      "5133\n",
      "294727\n",
      "5134\n",
      "294729\n",
      "5135\n",
      "294742\n",
      "5136\n",
      "294769\n",
      "5137\n",
      "294774\n",
      "5138\n",
      "294778\n",
      "5139\n",
      "294781\n",
      "5140\n",
      "294784\n",
      "5141\n",
      "294788\n",
      "5142\n",
      "294804\n",
      "5143\n",
      "294821\n",
      "5144\n",
      "294847\n",
      "5145\n",
      "294873\n",
      "5146\n",
      "294890\n",
      "5147\n",
      "294915\n",
      "5148\n",
      "294918\n",
      "5149\n",
      "294921\n",
      "5150\n",
      "294938\n",
      "5151\n",
      "294962\n",
      "5152\n",
      "294980\n",
      "5153\n",
      "295000\n",
      "5154\n",
      "295002\n",
      "5155\n",
      "295041\n",
      "5156\n",
      "295046\n",
      "5157\n",
      "295076\n",
      "5158\n",
      "295089\n",
      "5159\n",
      "295099\n",
      "5160\n",
      "295120\n",
      "5161\n",
      "295167\n",
      "5162\n",
      "295185\n",
      "5163\n",
      "295228\n",
      "5164\n",
      "295275\n",
      "5165\n",
      "295341\n",
      "5166\n",
      "295348\n",
      "5167\n",
      "295371\n",
      "5168\n",
      "295388\n",
      "5169\n",
      "295397\n",
      "5170\n",
      "295485\n",
      "5171\n",
      "295493\n",
      "5172\n",
      "295499\n",
      "5173\n",
      "295503\n",
      "5174\n",
      "295511\n",
      "5175\n",
      "295527\n",
      "5176\n",
      "295547\n",
      "5177\n",
      "295569\n",
      "5178\n",
      "295573\n",
      "5179\n",
      "295619\n",
      "5180\n",
      "295627\n",
      "5181\n",
      "295632\n",
      "5182\n",
      "295634\n",
      "5183\n",
      "295640\n",
      "5184\n",
      "295674\n",
      "5185\n",
      "295684\n",
      "5186\n",
      "295691\n",
      "5187\n",
      "295694\n",
      "5188\n",
      "295717\n",
      "5189\n",
      "295735\n",
      "5190\n",
      "295745\n",
      "5191\n",
      "295748\n",
      "5192\n",
      "295754\n",
      "5193\n",
      "295759\n",
      "5194\n",
      "295793\n",
      "5195\n",
      "295796\n",
      "5196\n",
      "295799\n",
      "5197\n",
      "295886\n",
      "5198\n",
      "295928\n",
      "5199\n",
      "295968\n",
      "5200\n",
      "296050\n",
      "5201\n",
      "296069\n",
      "5202\n",
      "296070\n",
      "5203\n",
      "296076\n",
      "5204\n",
      "296087\n",
      "5205\n",
      "296119\n",
      "5206\n",
      "296126\n",
      "5207\n",
      "296134\n",
      "5208\n",
      "296145\n",
      "5209\n",
      "296160\n",
      "5210\n",
      "296170\n",
      "5211\n",
      "296191\n",
      "5212\n",
      "296196\n",
      "5213\n",
      "296202\n",
      "5214\n",
      "296208\n",
      "5215\n",
      "296217\n",
      "5216\n",
      "296218\n",
      "5217\n",
      "296225\n",
      "5218\n",
      "296265\n",
      "5219\n",
      "296266\n",
      "5220\n",
      "296267\n",
      "5221\n",
      "296271\n",
      "5222\n",
      "296303\n",
      "5223\n",
      "296338\n",
      "5224\n",
      "296363\n",
      "5225\n",
      "296370\n",
      "5226\n",
      "296417\n",
      "5227\n",
      "296423\n",
      "5228\n",
      "296430\n",
      "5229\n",
      "296446\n",
      "5230\n",
      "296460\n",
      "5231\n",
      "296464\n",
      "5232\n",
      "296470\n",
      "5233\n",
      "296489\n",
      "5234\n",
      "296517\n",
      "5235\n",
      "296540\n",
      "5236\n",
      "296585\n",
      "5237\n",
      "296608\n",
      "5238\n",
      "296621\n",
      "5239\n",
      "296624\n",
      "5240\n",
      "296645\n",
      "5241\n",
      "296674\n",
      "5242\n",
      "296681\n",
      "5243\n",
      "296711\n",
      "5244\n",
      "296722\n",
      "5245\n",
      "296728\n",
      "5246\n",
      "296743\n",
      "5247\n",
      "296851\n",
      "5248\n",
      "296901\n",
      "5249\n",
      "296909\n",
      "5250\n",
      "296917\n",
      "5251\n",
      "296921\n",
      "5252\n",
      "296922\n",
      "5253\n",
      "296933\n",
      "5254\n",
      "296939\n",
      "5255\n",
      "296952\n",
      "5256\n",
      "296963\n",
      "5257\n",
      "296978\n",
      "5258\n",
      "296982\n",
      "5259\n",
      "296983\n",
      "5260\n",
      "296986\n",
      "5261\n",
      "297017\n",
      "5262\n",
      "297023\n",
      "5263\n",
      "297026\n",
      "5264\n",
      "297063\n",
      "5265\n",
      "297068\n",
      "5266\n",
      "297084\n",
      "5267\n",
      "297148\n",
      "5268\n",
      "297162\n",
      "5269\n",
      "297166\n",
      "5270\n",
      "297168\n",
      "5271\n",
      "297173\n",
      "5272\n",
      "297186\n",
      "5273\n",
      "297189\n",
      "5274\n",
      "297222\n",
      "5275\n",
      "297243\n",
      "5276\n",
      "297273\n",
      "5277\n",
      "297307\n",
      "5278\n",
      "297325\n",
      "5279\n",
      "297347\n",
      "5280\n",
      "297365\n",
      "5281\n",
      "297400\n",
      "5282\n",
      "297406\n",
      "5283\n",
      "297407\n",
      "5284\n",
      "297412\n",
      "5285\n",
      "297417\n",
      "5286\n",
      "297445\n",
      "5287\n",
      "297454\n",
      "5288\n",
      "297464\n",
      "5289\n",
      "297483\n",
      "5290\n",
      "297484\n",
      "5291\n",
      "297507\n",
      "5292\n",
      "297512\n",
      "5293\n",
      "297540\n",
      "5294\n",
      "297603\n",
      "5295\n",
      "297632\n",
      "5296\n",
      "297641\n",
      "5297\n",
      "297663\n",
      "5298\n",
      "297685\n",
      "5299\n",
      "297694\n",
      "5300\n",
      "297732\n",
      "5301\n",
      "297740\n",
      "5302\n",
      "297756\n",
      "5303\n",
      "297774\n",
      "5304\n",
      "297775\n",
      "5305\n",
      "297778\n",
      "5306\n",
      "297809\n",
      "5307\n",
      "297810\n",
      "5308\n",
      "297850\n",
      "5309\n",
      "297851\n",
      "5310\n",
      "297891\n",
      "5311\n",
      "297911\n",
      "5312\n",
      "297917\n",
      "5313\n",
      "297918\n",
      "5314\n",
      "297928\n",
      "5315\n",
      "297933\n",
      "5316\n",
      "297940\n",
      "5317\n",
      "297958\n",
      "5318\n",
      "297991\n",
      "5319\n",
      "298013\n",
      "5320\n",
      "298018\n",
      "5321\n",
      "298036\n",
      "5322\n",
      "298044\n",
      "5323\n",
      "298135\n",
      "5324\n",
      "298151\n",
      "5325\n",
      "298171\n",
      "5326\n",
      "298177\n",
      "5327\n",
      "298181\n",
      "5328\n",
      "298190\n",
      "5329\n",
      "298212\n",
      "5330\n",
      "298225\n",
      "5331\n",
      "298242\n",
      "5332\n",
      "298255\n",
      "5333\n",
      "298289\n",
      "5334\n",
      "298297\n",
      "5335\n",
      "298308\n",
      "5336\n",
      "298314\n",
      "5337\n",
      "298344\n",
      "5338\n",
      "298352\n",
      "5339\n",
      "298379\n",
      "5340\n",
      "298383\n",
      "5341\n",
      "298396\n",
      "5342\n",
      "298473\n",
      "5343\n",
      "298478\n",
      "5344\n",
      "298481\n",
      "5345\n",
      "298489\n",
      "5346\n",
      "298491\n",
      "5347\n",
      "298512\n",
      "5348\n",
      "298545\n",
      "5349\n",
      "298574\n",
      "5350\n",
      "298577\n",
      "5351\n",
      "298593\n",
      "5352\n",
      "298638\n",
      "5353\n",
      "298698\n",
      "5354\n",
      "298726\n",
      "5355\n",
      "298751\n",
      "5356\n",
      "298753\n",
      "5357\n",
      "298759\n",
      "5358\n",
      "298772\n",
      "5359\n",
      "298784\n",
      "5360\n",
      "298807\n",
      "5361\n",
      "298815\n",
      "5362\n",
      "298819\n",
      "5363\n",
      "298834\n",
      "5364\n",
      "298857\n",
      "5365\n",
      "298909\n",
      "5366\n",
      "298920\n",
      "5367\n",
      "298930\n",
      "5368\n",
      "298955\n",
      "5369\n",
      "298997\n",
      "5370\n",
      "299008\n",
      "5371\n",
      "299033\n",
      "5372\n",
      "299046\n",
      "5373\n",
      "299051\n",
      "5374\n",
      "299054\n",
      "5375\n",
      "299079\n",
      "5376\n",
      "299102\n",
      "5377\n",
      "299112\n",
      "5378\n",
      "299121\n",
      "5379\n",
      "299140\n",
      "5380\n",
      "299165\n",
      "5381\n",
      "299180\n",
      "5382\n",
      "299187\n",
      "5383\n",
      "299190\n",
      "5384\n",
      "299200\n",
      "5385\n",
      "299210\n",
      "5386\n",
      "299274\n",
      "5387\n",
      "299319\n",
      "5388\n",
      "299336\n",
      "5389\n",
      "299338\n",
      "5390\n",
      "299347\n",
      "5391\n",
      "299394\n",
      "5392\n",
      "299403\n",
      "5393\n",
      "299407\n",
      "5394\n",
      "299445\n",
      "5395\n",
      "299482\n",
      "5396\n",
      "299490\n",
      "5397\n",
      "299509\n",
      "5398\n",
      "299529\n",
      "5399\n",
      "299543\n",
      "5400\n",
      "299554\n",
      "5401\n",
      "299557\n",
      "5402\n",
      "299593\n",
      "5403\n",
      "299614\n",
      "5404\n",
      "299629\n",
      "5405\n",
      "299630\n",
      "5406\n",
      "299645\n",
      "5407\n",
      "299654\n",
      "5408\n",
      "299655\n",
      "5409\n",
      "299685\n",
      "5410\n",
      "299695\n",
      "5411\n",
      "299715\n",
      "5412\n",
      "299728\n",
      "5413\n",
      "299734\n",
      "5414\n",
      "299736\n",
      "5415\n",
      "299751\n",
      "5416\n",
      "299765\n",
      "5417\n",
      "299767\n",
      "5418\n",
      "299800\n",
      "5419\n",
      "299806\n",
      "5420\n",
      "299826\n",
      "5421\n",
      "299828\n",
      "5422\n",
      "299832\n",
      "5423\n",
      "299853\n",
      "5424\n",
      "299863\n",
      "5425\n",
      "299883\n",
      "5426\n",
      "299888\n",
      "5427\n",
      "299913\n",
      "5428\n",
      "299914\n",
      "5429\n",
      "299917\n",
      "5430\n",
      "299995\n",
      "5431\n",
      "time: 585 ms\n"
     ]
    }
   ],
   "source": [
    "# for i, j in zip(test_z_icustay_id, test_index_tester):  #looping through the zipped indicies of the test indicies/test icustay_id\n",
    "#     print(i) #icustay_id\n",
    "#     print(j) #index of icustay_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amax_bun</th>\n",
       "      <th>amax_creatinine</th>\n",
       "      <th>amax_daily_sofa</th>\n",
       "      <th>amax_heartrate</th>\n",
       "      <th>amax_meanartpress</th>\n",
       "      <th>amax_platelet</th>\n",
       "      <th>amax_ptt</th>\n",
       "      <th>amax_sysbp</th>\n",
       "      <th>amax_temperature</th>\n",
       "      <th>amin_bun</th>\n",
       "      <th>...</th>\n",
       "      <th>any_vasoactive_False</th>\n",
       "      <th>any_vasoactive_True</th>\n",
       "      <th>leukocyte_False</th>\n",
       "      <th>leukocyte_True</th>\n",
       "      <th>pao2fio2Ratio_(0, 200]</th>\n",
       "      <th>pao2fio2Ratio_(200, 333]</th>\n",
       "      <th>pao2fio2Ratio_(333, 475]</th>\n",
       "      <th>pao2fio2Ratio_(475, 3000]</th>\n",
       "      <th>vent_recieved_False</th>\n",
       "      <th>vent_recieved_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>0.043199</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.031318</td>\n",
       "      <td>0.032467</td>\n",
       "      <td>-0.018116</td>\n",
       "      <td>-0.026686</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>-0.006575</td>\n",
       "      <td>-0.049870</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>0.231856</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.104827</td>\n",
       "      <td>0.059648</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>-0.030164</td>\n",
       "      <td>0.061535</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.136269</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>-0.049870</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>-0.018847</td>\n",
       "      <td>0.104986</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>0.274991</td>\n",
       "      <td>0.032390</td>\n",
       "      <td>0.059787</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4304</th>\n",
       "      <td>0.093070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.207519</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>0.108071</td>\n",
       "      <td>-0.007729</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>0.122124</td>\n",
       "      <td>0.028005</td>\n",
       "      <td>0.069095</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4305</th>\n",
       "      <td>0.267443</td>\n",
       "      <td>0.826712</td>\n",
       "      <td>0.660964</td>\n",
       "      <td>0.102193</td>\n",
       "      <td>0.079212</td>\n",
       "      <td>0.089090</td>\n",
       "      <td>0.018799</td>\n",
       "      <td>0.051457</td>\n",
       "      <td>0.043986</td>\n",
       "      <td>0.155882</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>0.317313</td>\n",
       "      <td>0.347655</td>\n",
       "      <td>0.660964</td>\n",
       "      <td>0.117672</td>\n",
       "      <td>0.082546</td>\n",
       "      <td>0.037520</td>\n",
       "      <td>0.082133</td>\n",
       "      <td>0.075082</td>\n",
       "      <td>0.007768</td>\n",
       "      <td>0.253675</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4310</th>\n",
       "      <td>-0.240028</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.096860</td>\n",
       "      <td>0.038845</td>\n",
       "      <td>0.150447</td>\n",
       "      <td>-0.031331</td>\n",
       "      <td>0.030192</td>\n",
       "      <td>0.017923</td>\n",
       "      <td>-0.274114</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>0.436722</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>0.157795</td>\n",
       "      <td>0.098730</td>\n",
       "      <td>-0.280688</td>\n",
       "      <td>0.102890</td>\n",
       "      <td>0.073184</td>\n",
       "      <td>0.082385</td>\n",
       "      <td>0.360513</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>0.056406</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.074574</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>-0.173527</td>\n",
       "      <td>-0.016447</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.079290</td>\n",
       "      <td>-0.049870</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4322</th>\n",
       "      <td>0.394093</td>\n",
       "      <td>1.836329</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.024623</td>\n",
       "      <td>0.169487</td>\n",
       "      <td>-0.025345</td>\n",
       "      <td>0.448032</td>\n",
       "      <td>0.089829</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>0.174374</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4323</th>\n",
       "      <td>-0.087975</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.246092</td>\n",
       "      <td>0.161758</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>-0.056746</td>\n",
       "      <td>0.128490</td>\n",
       "      <td>0.069572</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4325</th>\n",
       "      <td>0.253675</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.148804</td>\n",
       "      <td>0.079212</td>\n",
       "      <td>0.062143</td>\n",
       "      <td>-0.093668</td>\n",
       "      <td>0.053501</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>0.183238</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>0.155882</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.041101</td>\n",
       "      <td>0.207891</td>\n",
       "      <td>0.026397</td>\n",
       "      <td>-0.122554</td>\n",
       "      <td>0.164011</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4335</th>\n",
       "      <td>0.081304</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.110028</td>\n",
       "      <td>0.142984</td>\n",
       "      <td>0.015641</td>\n",
       "      <td>-0.030164</td>\n",
       "      <td>0.096932</td>\n",
       "      <td>0.036418</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4354</th>\n",
       "      <td>0.398618</td>\n",
       "      <td>0.631709</td>\n",
       "      <td>0.953445</td>\n",
       "      <td>0.146515</td>\n",
       "      <td>0.293194</td>\n",
       "      <td>0.033879</td>\n",
       "      <td>0.351600</td>\n",
       "      <td>0.123729</td>\n",
       "      <td>0.082385</td>\n",
       "      <td>0.155882</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360</th>\n",
       "      <td>0.216448</td>\n",
       "      <td>0.780201</td>\n",
       "      <td>1.229716</td>\n",
       "      <td>0.085914</td>\n",
       "      <td>0.140212</td>\n",
       "      <td>-0.064408</td>\n",
       "      <td>0.368658</td>\n",
       "      <td>0.053501</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.136269</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362</th>\n",
       "      <td>0.056406</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>0.792481</td>\n",
       "      <td>0.117672</td>\n",
       "      <td>0.409648</td>\n",
       "      <td>-0.055692</td>\n",
       "      <td>-0.016447</td>\n",
       "      <td>0.117257</td>\n",
       "      <td>0.025592</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363</th>\n",
       "      <td>0.650824</td>\n",
       "      <td>1.812174</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.014987</td>\n",
       "      <td>0.085848</td>\n",
       "      <td>0.086975</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>-0.007399</td>\n",
       "      <td>0.587274</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4364</th>\n",
       "      <td>0.069095</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>-0.207519</td>\n",
       "      <td>0.117672</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.036616</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.049399</td>\n",
       "      <td>0.015897</td>\n",
       "      <td>-0.049870</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4365</th>\n",
       "      <td>0.375333</td>\n",
       "      <td>1.995678</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>0.115144</td>\n",
       "      <td>0.115155</td>\n",
       "      <td>-0.057119</td>\n",
       "      <td>0.087224</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>0.267443</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4366</th>\n",
       "      <td>0.498358</td>\n",
       "      <td>1.906626</td>\n",
       "      <td>0.850220</td>\n",
       "      <td>-0.043064</td>\n",
       "      <td>0.082546</td>\n",
       "      <td>-0.268272</td>\n",
       "      <td>0.157471</td>\n",
       "      <td>0.049399</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>0.115390</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.107438</td>\n",
       "      <td>0.120125</td>\n",
       "      <td>0.146246</td>\n",
       "      <td>-0.038412</td>\n",
       "      <td>0.100420</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.043199</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>-0.240028</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.038896</td>\n",
       "      <td>0.083650</td>\n",
       "      <td>0.186570</td>\n",
       "      <td>-0.069505</td>\n",
       "      <td>0.047327</td>\n",
       "      <td>0.028005</td>\n",
       "      <td>-0.312219</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4369</th>\n",
       "      <td>0.029431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.113148</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.309433</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>0.032818</td>\n",
       "      <td>-0.155149</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>0.389504</td>\n",
       "      <td>0.578891</td>\n",
       "      <td>0.292481</td>\n",
       "      <td>0.185358</td>\n",
       "      <td>0.131754</td>\n",
       "      <td>0.123841</td>\n",
       "      <td>0.438890</td>\n",
       "      <td>0.067418</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>0.224244</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>0.448488</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.096860</td>\n",
       "      <td>0.164355</td>\n",
       "      <td>0.156087</td>\n",
       "      <td>-0.024385</td>\n",
       "      <td>0.089829</td>\n",
       "      <td>0.014681</td>\n",
       "      <td>0.416109</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>0.477918</td>\n",
       "      <td>2.217184</td>\n",
       "      <td>0.292481</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.012554</td>\n",
       "      <td>0.110387</td>\n",
       "      <td>-0.144237</td>\n",
       "      <td>0.041019</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>0.407482</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>0.155882</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.403677</td>\n",
       "      <td>0.146515</td>\n",
       "      <td>0.164355</td>\n",
       "      <td>0.038420</td>\n",
       "      <td>0.242461</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.064884</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4374</th>\n",
       "      <td>0.380126</td>\n",
       "      <td>1.079914</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.104986</td>\n",
       "      <td>0.178573</td>\n",
       "      <td>-0.100677</td>\n",
       "      <td>0.086212</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>0.069095</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4375</th>\n",
       "      <td>0.246564</td>\n",
       "      <td>0.780201</td>\n",
       "      <td>0.403677</td>\n",
       "      <td>0.080295</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.005320</td>\n",
       "      <td>0.067418</td>\n",
       "      <td>0.019945</td>\n",
       "      <td>0.069095</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>0.174374</td>\n",
       "      <td>0.467559</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.074574</td>\n",
       "      <td>0.137417</td>\n",
       "      <td>0.095307</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.152645</td>\n",
       "      <td>0.043986</td>\n",
       "      <td>-0.015784</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217747</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.091435</td>\n",
       "      <td>0.089116</td>\n",
       "      <td>0.068445</td>\n",
       "      <td>0.103696</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.068402</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>-0.209194</td>\n",
       "      <td>-0.448205</td>\n",
       "      <td>-0.207519</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.069000</td>\n",
       "      <td>0.047211</td>\n",
       "      <td>-0.014210</td>\n",
       "      <td>0.041019</td>\n",
       "      <td>0.015898</td>\n",
       "      <td>-0.355418</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>0.260632</td>\n",
       "      <td>0.631709</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.122666</td>\n",
       "      <td>0.172023</td>\n",
       "      <td>0.105266</td>\n",
       "      <td>0.058293</td>\n",
       "      <td>0.088026</td>\n",
       "      <td>0.133613</td>\n",
       "      <td>0.104422</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406</th>\n",
       "      <td>0.456100</td>\n",
       "      <td>2.325925</td>\n",
       "      <td>0.292481</td>\n",
       "      <td>-0.011172</td>\n",
       "      <td>0.142984</td>\n",
       "      <td>0.022551</td>\n",
       "      <td>0.036670</td>\n",
       "      <td>0.140818</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.328666</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>0.043199</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151076</td>\n",
       "      <td>0.095556</td>\n",
       "      <td>0.044613</td>\n",
       "      <td>0.024854</td>\n",
       "      <td>0.069353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>0.411824</td>\n",
       "      <td>1.787638</td>\n",
       "      <td>1.043731</td>\n",
       "      <td>0.246092</td>\n",
       "      <td>0.324817</td>\n",
       "      <td>-0.192038</td>\n",
       "      <td>0.373279</td>\n",
       "      <td>0.041019</td>\n",
       "      <td>0.123516</td>\n",
       "      <td>0.056406</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>0.529792</td>\n",
       "      <td>0.871874</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>-0.030723</td>\n",
       "      <td>0.012554</td>\n",
       "      <td>-0.026575</td>\n",
       "      <td>0.041482</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>-0.012767</td>\n",
       "      <td>0.504894</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>-0.209194</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>-0.207519</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.040106</td>\n",
       "      <td>-0.027812</td>\n",
       "      <td>-0.017570</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>-0.274114</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5411</th>\n",
       "      <td>0.253675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.041101</td>\n",
       "      <td>0.118146</td>\n",
       "      <td>0.078289</td>\n",
       "      <td>-0.072111</td>\n",
       "      <td>0.084386</td>\n",
       "      <td>0.023980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5412</th>\n",
       "      <td>0.305548</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.903677</td>\n",
       "      <td>0.223545</td>\n",
       "      <td>0.134597</td>\n",
       "      <td>-0.061462</td>\n",
       "      <td>-0.017570</td>\n",
       "      <td>0.091621</td>\n",
       "      <td>0.075411</td>\n",
       "      <td>0.246564</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5413</th>\n",
       "      <td>0.200269</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.903677</td>\n",
       "      <td>0.193375</td>\n",
       "      <td>0.296670</td>\n",
       "      <td>-0.108998</td>\n",
       "      <td>0.473858</td>\n",
       "      <td>0.014325</td>\n",
       "      <td>0.082385</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5414</th>\n",
       "      <td>-0.068362</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>-0.207519</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.082546</td>\n",
       "      <td>-0.052867</td>\n",
       "      <td>0.244104</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>-0.155149</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.170817</td>\n",
       "      <td>0.153843</td>\n",
       "      <td>0.065318</td>\n",
       "      <td>-0.069505</td>\n",
       "      <td>0.120511</td>\n",
       "      <td>0.041998</td>\n",
       "      <td>-0.240028</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5416</th>\n",
       "      <td>0.311484</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>0.403677</td>\n",
       "      <td>0.130013</td>\n",
       "      <td>0.028580</td>\n",
       "      <td>0.046349</td>\n",
       "      <td>-0.099264</td>\n",
       "      <td>0.047327</td>\n",
       "      <td>0.038014</td>\n",
       "      <td>0.191865</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5417</th>\n",
       "      <td>0.043199</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>0.172023</td>\n",
       "      <td>-0.018116</td>\n",
       "      <td>-0.085433</td>\n",
       "      <td>0.105574</td>\n",
       "      <td>0.055853</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5418</th>\n",
       "      <td>-0.209194</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.071674</td>\n",
       "      <td>0.101873</td>\n",
       "      <td>-0.008858</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>0.025994</td>\n",
       "      <td>-0.209194</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5419</th>\n",
       "      <td>0.155882</td>\n",
       "      <td>0.217747</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.044295</td>\n",
       "      <td>0.098730</td>\n",
       "      <td>0.036616</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.076966</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.136269</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5420</th>\n",
       "      <td>-0.049870</td>\n",
       "      <td>0.467559</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.139548</td>\n",
       "      <td>0.164355</td>\n",
       "      <td>0.081225</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.096932</td>\n",
       "      <td>0.081225</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5421</th>\n",
       "      <td>0.274114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.085914</td>\n",
       "      <td>0.040106</td>\n",
       "      <td>-0.094746</td>\n",
       "      <td>0.067307</td>\n",
       "      <td>0.036736</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5422</th>\n",
       "      <td>0.104422</td>\n",
       "      <td>0.780201</td>\n",
       "      <td>0.403677</td>\n",
       "      <td>0.080295</td>\n",
       "      <td>0.047571</td>\n",
       "      <td>-0.103537</td>\n",
       "      <td>0.014705</td>\n",
       "      <td>0.011986</td>\n",
       "      <td>0.007768</td>\n",
       "      <td>0.029431</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>0.299501</td>\n",
       "      <td>1.297662</td>\n",
       "      <td>0.792481</td>\n",
       "      <td>0.017782</td>\n",
       "      <td>0.054870</td>\n",
       "      <td>-0.192038</td>\n",
       "      <td>0.301125</td>\n",
       "      <td>0.027977</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>0.246564</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>0.360513</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.056759</td>\n",
       "      <td>0.164355</td>\n",
       "      <td>0.019618</td>\n",
       "      <td>0.091413</td>\n",
       "      <td>0.110639</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>0.069095</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5425</th>\n",
       "      <td>0.200269</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.729716</td>\n",
       "      <td>0.141888</td>\n",
       "      <td>0.125991</td>\n",
       "      <td>-0.081420</td>\n",
       "      <td>0.048113</td>\n",
       "      <td>0.061535</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.093070</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>-0.207519</td>\n",
       "      <td>0.172936</td>\n",
       "      <td>0.058460</td>\n",
       "      <td>0.054813</td>\n",
       "      <td>0.022847</td>\n",
       "      <td>0.027977</td>\n",
       "      <td>-0.008636</td>\n",
       "      <td>-0.131174</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5427</th>\n",
       "      <td>-0.049870</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.080295</td>\n",
       "      <td>0.098730</td>\n",
       "      <td>0.016643</td>\n",
       "      <td>-0.051765</td>\n",
       "      <td>0.041019</td>\n",
       "      <td>0.063709</td>\n",
       "      <td>-0.131174</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5428</th>\n",
       "      <td>0.484876</td>\n",
       "      <td>0.631709</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.112596</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>0.113523</td>\n",
       "      <td>-0.025534</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>0.017923</td>\n",
       "      <td>0.420338</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5429</th>\n",
       "      <td>0.350242</td>\n",
       "      <td>2.641747</td>\n",
       "      <td>0.160964</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>0.032467</td>\n",
       "      <td>0.089790</td>\n",
       "      <td>0.015733</td>\n",
       "      <td>0.063509</td>\n",
       "      <td>0.009804</td>\n",
       "      <td>0.208459</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5430</th>\n",
       "      <td>-0.405288</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.056759</td>\n",
       "      <td>0.016634</td>\n",
       "      <td>-0.035398</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>-0.028245</td>\n",
       "      <td>0.071521</td>\n",
       "      <td>-0.464272</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5431</th>\n",
       "      <td>-0.355418</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.050589</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>0.064529</td>\n",
       "      <td>0.012640</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>0.034019</td>\n",
       "      <td>-0.355418</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1086 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      amax_bun  amax_creatinine  amax_daily_sofa  amax_heartrate  \\\n",
       "4298  0.043199         0.148492         0.160964        0.031318   \n",
       "4299  0.231856         0.148492        -1.000000        0.104827   \n",
       "4303 -0.049870        -0.079914         0.160964       -0.018847   \n",
       "4304  0.093070         0.000000        -0.207519        0.037873   \n",
       "4305  0.267443         0.826712         0.660964        0.102193   \n",
       "4306  0.317313         0.347655         0.660964        0.117672   \n",
       "4310 -0.240028        -0.253202         0.160964        0.096860   \n",
       "4311  0.436722         0.284055         0.953445        0.157795   \n",
       "4317  0.056406         0.148492         0.500000        0.074574   \n",
       "4322  0.394093         1.836329        -1.000000        0.024623   \n",
       "4323 -0.087975        -0.253202         0.000000        0.246092   \n",
       "4325  0.253675        -0.164150         0.500000        0.148804   \n",
       "4331  0.155882         0.076014        -0.500000        0.041101   \n",
       "4335  0.081304        -0.164150        -0.500000        0.110028   \n",
       "4354  0.398618         0.631709         0.953445        0.146515   \n",
       "4360  0.216448         0.780201         1.229716        0.085914   \n",
       "4362  0.056406         0.284055         0.792481        0.117672   \n",
       "4363  0.650824         1.812174         0.500000       -0.014987   \n",
       "4364  0.069095         0.076014        -0.207519        0.117672   \n",
       "4365  0.375333         1.995678         0.584963        0.115144   \n",
       "4366  0.498358         1.906626         0.850220       -0.043064   \n",
       "4367  0.115390         0.148492         0.160964        0.107438   \n",
       "4368 -0.240028        -0.253202        -0.500000       -0.038896   \n",
       "4369  0.029431         0.000000         0.160964        0.047458   \n",
       "4370  0.389504         0.578891         0.292481        0.185358   \n",
       "4371  0.448488         1.000000         0.160964        0.096860   \n",
       "4372  0.477918         2.217184         0.292481        0.014305   \n",
       "4373  0.155882         0.148492         0.403677        0.146515   \n",
       "4374  0.380126         1.079914         0.160964        0.003638   \n",
       "4375  0.246564         0.780201         0.403677        0.080295   \n",
       "...        ...              ...              ...             ...   \n",
       "5402  0.174374         0.467559         0.160964        0.074574   \n",
       "5403  0.000000         0.217747         0.160964        0.091435   \n",
       "5404 -0.209194        -0.448205        -0.207519        0.132424   \n",
       "5405  0.260632         0.631709         0.160964        0.122666   \n",
       "5406  0.456100         2.325925         0.292481       -0.011172   \n",
       "5407  0.043199         0.076014         0.000000        0.151076   \n",
       "5408  0.411824         1.787638         1.043731        0.246092   \n",
       "5409  0.529792         0.871874         0.160964       -0.030723   \n",
       "5410 -0.209194        -0.079914        -0.207519        0.014305   \n",
       "5411  0.253675         0.000000        -0.500000        0.041101   \n",
       "5412  0.305548         0.076014         0.903677        0.223545   \n",
       "5413  0.200269         0.076014         0.903677        0.193375   \n",
       "5414 -0.068362        -0.164150        -0.207519        0.014305   \n",
       "5415 -0.155149        -0.079914         0.000000        0.170817   \n",
       "5416  0.311484         0.284055         0.403677        0.130013   \n",
       "5417  0.043199        -0.079914         0.000000        0.027988   \n",
       "5418 -0.209194        -0.164150         0.160964        0.071674   \n",
       "5419  0.155882         0.217747         0.160964        0.044295   \n",
       "5420 -0.049870         0.467559         0.160964        0.139548   \n",
       "5421  0.274114         0.000000         0.160964        0.085914   \n",
       "5422  0.104422         0.780201         0.403677        0.080295   \n",
       "5423  0.299501         1.297662         0.792481        0.017782   \n",
       "5424  0.360513         0.148492         0.160964        0.056759   \n",
       "5425  0.200269        -0.079914         0.729716        0.141888   \n",
       "5426  0.000000         0.284055        -0.207519        0.172936   \n",
       "5427 -0.049870        -0.079914         0.500000        0.080295   \n",
       "5428  0.484876         0.631709         0.160964        0.112596   \n",
       "5429  0.350242         2.641747         0.160964        0.047458   \n",
       "5430 -0.405288        -0.253202        -0.500000        0.056759   \n",
       "5431 -0.355418        -0.164150        -1.000000        0.050589   \n",
       "\n",
       "      amax_meanartpress  amax_platelet  amax_ptt  amax_sysbp  \\\n",
       "4298           0.032467      -0.018116 -0.026686    0.002437   \n",
       "4299           0.059648       0.102000 -0.030164    0.061535   \n",
       "4303           0.104986       0.037520  0.274991    0.032390   \n",
       "4304           0.108071      -0.007729  0.004263    0.122124   \n",
       "4305           0.079212       0.089090  0.018799    0.051457   \n",
       "4306           0.082546       0.037520  0.082133    0.075082   \n",
       "4310           0.038845       0.150447 -0.031331    0.030192   \n",
       "4311           0.098730      -0.280688  0.102890    0.073184   \n",
       "4317           0.033752      -0.173527 -0.016447    0.011986   \n",
       "4322           0.169487      -0.025345  0.448032    0.089829   \n",
       "4323           0.161758       0.021578 -0.056746    0.128490   \n",
       "4325           0.079212       0.062143 -0.093668    0.053501   \n",
       "4331           0.207891       0.026397 -0.122554    0.164011   \n",
       "4335           0.142984       0.015641 -0.030164    0.096932   \n",
       "4354           0.293194       0.033879  0.351600    0.123729   \n",
       "4360           0.140212      -0.064408  0.368658    0.053501   \n",
       "4362           0.409648      -0.055692 -0.016447    0.117257   \n",
       "4363           0.085848       0.086975  0.033752    0.071275   \n",
       "4364           0.008422       0.036616  0.012640    0.049399   \n",
       "4365           0.115155      -0.057119  0.087224    0.071275   \n",
       "4366           0.082546      -0.268272  0.157471    0.049399   \n",
       "4367           0.120125       0.146246 -0.038412    0.100420   \n",
       "4368           0.083650       0.186570 -0.069505    0.047327   \n",
       "4369           0.113148       0.038420  0.309433    0.071275   \n",
       "4370           0.131754       0.123841  0.438890    0.067418   \n",
       "4371           0.164355       0.156087 -0.024385    0.089829   \n",
       "4372           0.012554       0.110387 -0.144237    0.041019   \n",
       "4373           0.164355       0.038420  0.242461    0.004854   \n",
       "4374           0.104986       0.178573 -0.100677    0.086212   \n",
       "4375           0.095556       0.012607  0.005320    0.067418   \n",
       "...                 ...            ...       ...         ...   \n",
       "5402           0.137417       0.095307  0.014705    0.152645   \n",
       "5403           0.089116       0.068445  0.103696    0.007250   \n",
       "5404           0.069000       0.047211 -0.014210    0.041019   \n",
       "5405           0.172023       0.105266  0.058293    0.088026   \n",
       "5406           0.142984       0.022551  0.036670    0.140818   \n",
       "5407           0.095556       0.044613  0.024854    0.069353   \n",
       "5408           0.324817      -0.192038  0.373279    0.041019   \n",
       "5409           0.012554      -0.026575  0.041482    0.007250   \n",
       "5410           0.040106      -0.027812 -0.017570    0.021231   \n",
       "5411           0.118146       0.078289 -0.072111    0.084386   \n",
       "5412           0.134597      -0.061462 -0.017570    0.091621   \n",
       "5413           0.296670      -0.108998  0.473858    0.014325   \n",
       "5414           0.082546      -0.052867  0.244104    0.043137   \n",
       "5415           0.153843       0.065318 -0.069505    0.120511   \n",
       "5416           0.028580       0.046349 -0.099264    0.047327   \n",
       "5417           0.172023      -0.018116 -0.085433    0.105574   \n",
       "5418           0.101873      -0.008858  0.012640    0.071275   \n",
       "5419           0.098730       0.036616  0.012640    0.076966   \n",
       "5420           0.164355       0.081225  0.035700    0.096932   \n",
       "5421           0.040106      -0.094746  0.067307    0.036736   \n",
       "5422           0.047571      -0.103537  0.014705    0.011986   \n",
       "5423           0.054870      -0.192038  0.301125    0.027977   \n",
       "5424           0.164355       0.019618  0.091413    0.110639   \n",
       "5425           0.125991      -0.081420  0.048113    0.061535   \n",
       "5426           0.058460       0.054813  0.022847    0.027977   \n",
       "5427           0.098730       0.016643 -0.051765    0.041019   \n",
       "5428           0.072439       0.113523 -0.025534    0.071275   \n",
       "5429           0.032467       0.089790  0.015733    0.063509   \n",
       "5430           0.016634      -0.035398  0.012640   -0.028245   \n",
       "5431           0.072439       0.064529  0.012640    0.071275   \n",
       "\n",
       "      amax_temperature  amin_bun         ...          any_vasoactive_False  \\\n",
       "4298         -0.006575 -0.049870         ...                             1   \n",
       "4299          0.009804  0.136269         ...                             1   \n",
       "4303          0.059787 -0.108854         ...                             1   \n",
       "4304          0.028005  0.069095         ...                             1   \n",
       "4305          0.043986  0.155882         ...                             0   \n",
       "4306          0.007768  0.253675         ...                             0   \n",
       "4310          0.017923 -0.274114         ...                             1   \n",
       "4311          0.082385  0.360513         ...                             0   \n",
       "4317          0.079290 -0.049870         ...                             1   \n",
       "4322         -0.000410  0.174374         ...                             1   \n",
       "4323          0.069572 -0.108854         ...                             1   \n",
       "4325         -0.000410  0.183238         ...                             0   \n",
       "4331          0.015897 -0.087975         ...                             1   \n",
       "4335          0.036418 -0.068362         ...                             1   \n",
       "4354          0.082385  0.155882         ...                             0   \n",
       "4360          0.003685  0.136269         ...                             0   \n",
       "4362          0.025592 -0.108854         ...                             0   \n",
       "4363         -0.007399  0.587274         ...                             1   \n",
       "4364          0.015897 -0.049870         ...                             1   \n",
       "4365          0.013869  0.267443         ...                             0   \n",
       "4366          0.003685  0.456100         ...                             1   \n",
       "4367          0.011838  0.043199         ...                             0   \n",
       "4368          0.028005 -0.312219         ...                             1   \n",
       "4369          0.032818 -0.155149         ...                             0   \n",
       "4370          0.013869  0.224244         ...                             1   \n",
       "4371          0.014681  0.416109         ...                             1   \n",
       "4372         -0.000410  0.407482         ...                             1   \n",
       "4373          0.064884  0.015050         ...                             0   \n",
       "4374          0.040008  0.069095         ...                             1   \n",
       "4375          0.019945  0.069095         ...                             1   \n",
       "...                ...       ...         ...                           ...   \n",
       "5402          0.043986 -0.015784         ...                             0   \n",
       "5403          0.068402 -0.087975         ...                             0   \n",
       "5404          0.015898 -0.355418         ...                             0   \n",
       "5405          0.133613  0.104422         ...                             1   \n",
       "5406          0.001639  0.328666         ...                             1   \n",
       "5407          0.000000 -0.087975         ...                             0   \n",
       "5408          0.123516  0.056406         ...                             0   \n",
       "5409         -0.012767  0.504894         ...                             1   \n",
       "5410          0.009804 -0.274114         ...                             1   \n",
       "5411          0.023980  0.000000         ...                             1   \n",
       "5412          0.075411  0.246564         ...                             0   \n",
       "5413          0.082385 -0.087975         ...                             0   \n",
       "5414          0.040008 -0.108854         ...                             0   \n",
       "5415          0.041998 -0.240028         ...                             1   \n",
       "5416          0.038014  0.191865         ...                             0   \n",
       "5417          0.055853 -0.068362         ...                             1   \n",
       "5418          0.025994 -0.209194         ...                             1   \n",
       "5419          0.001639  0.136269         ...                             1   \n",
       "5420          0.081225 -0.108854         ...                             1   \n",
       "5421         -0.000410 -0.108854         ...                             1   \n",
       "5422          0.007768  0.029431         ...                             1   \n",
       "5423         -0.000410  0.246564         ...                             1   \n",
       "5424          0.013869  0.069095         ...                             1   \n",
       "5425          0.003685  0.093070         ...                             0   \n",
       "5426         -0.008636 -0.131174         ...                             1   \n",
       "5427          0.063709 -0.131174         ...                             0   \n",
       "5428          0.017923  0.420338         ...                             1   \n",
       "5429          0.009804  0.208459         ...                             1   \n",
       "5430          0.071521 -0.464272         ...                             1   \n",
       "5431          0.034019 -0.355418         ...                             1   \n",
       "\n",
       "      any_vasoactive_True  leukocyte_False  leukocyte_True  \\\n",
       "4298                    0                1               0   \n",
       "4299                    0                1               0   \n",
       "4303                    0                0               1   \n",
       "4304                    0                1               0   \n",
       "4305                    1                0               1   \n",
       "4306                    1                1               0   \n",
       "4310                    0                0               1   \n",
       "4311                    1                0               1   \n",
       "4317                    0                1               0   \n",
       "4322                    0                1               0   \n",
       "4323                    0                1               0   \n",
       "4325                    1                1               0   \n",
       "4331                    0                0               1   \n",
       "4335                    0                1               0   \n",
       "4354                    1                1               0   \n",
       "4360                    1                1               0   \n",
       "4362                    1                1               0   \n",
       "4363                    0                1               0   \n",
       "4364                    0                1               0   \n",
       "4365                    1                1               0   \n",
       "4366                    0                1               0   \n",
       "4367                    1                1               0   \n",
       "4368                    0                0               1   \n",
       "4369                    1                1               0   \n",
       "4370                    0                1               0   \n",
       "4371                    0                1               0   \n",
       "4372                    0                1               0   \n",
       "4373                    1                0               1   \n",
       "4374                    0                1               0   \n",
       "4375                    0                0               1   \n",
       "...                   ...              ...             ...   \n",
       "5402                    1                1               0   \n",
       "5403                    1                1               0   \n",
       "5404                    1                1               0   \n",
       "5405                    0                0               1   \n",
       "5406                    0                1               0   \n",
       "5407                    1                1               0   \n",
       "5408                    1                1               0   \n",
       "5409                    0                1               0   \n",
       "5410                    0                1               0   \n",
       "5411                    0                1               0   \n",
       "5412                    1                1               0   \n",
       "5413                    1                1               0   \n",
       "5414                    1                1               0   \n",
       "5415                    0                1               0   \n",
       "5416                    1                1               0   \n",
       "5417                    0                1               0   \n",
       "5418                    0                1               0   \n",
       "5419                    0                1               0   \n",
       "5420                    0                0               1   \n",
       "5421                    0                1               0   \n",
       "5422                    0                1               0   \n",
       "5423                    0                1               0   \n",
       "5424                    0                1               0   \n",
       "5425                    1                1               0   \n",
       "5426                    0                1               0   \n",
       "5427                    1                1               0   \n",
       "5428                    0                1               0   \n",
       "5429                    0                1               0   \n",
       "5430                    0                1               0   \n",
       "5431                    0                1               0   \n",
       "\n",
       "      pao2fio2Ratio_(0, 200]  pao2fio2Ratio_(200, 333]  \\\n",
       "4298                       0                         0   \n",
       "4299                       0                         0   \n",
       "4303                       1                         0   \n",
       "4304                       0                         0   \n",
       "4305                       0                         1   \n",
       "4306                       0                         0   \n",
       "4310                       0                         0   \n",
       "4311                       1                         0   \n",
       "4317                       1                         0   \n",
       "4322                       0                         0   \n",
       "4323                       0                         0   \n",
       "4325                       0                         0   \n",
       "4331                       0                         0   \n",
       "4335                       0                         0   \n",
       "4354                       0                         0   \n",
       "4360                       1                         0   \n",
       "4362                       1                         0   \n",
       "4363                       0                         0   \n",
       "4364                       0                         0   \n",
       "4365                       1                         0   \n",
       "4366                       0                         0   \n",
       "4367                       0                         0   \n",
       "4368                       0                         0   \n",
       "4369                       0                         0   \n",
       "4370                       1                         0   \n",
       "4371                       0                         0   \n",
       "4372                       0                         0   \n",
       "4373                       1                         0   \n",
       "4374                       0                         0   \n",
       "4375                       0                         0   \n",
       "...                      ...                       ...   \n",
       "5402                       0                         0   \n",
       "5403                       0                         0   \n",
       "5404                       0                         0   \n",
       "5405                       0                         0   \n",
       "5406                       0                         0   \n",
       "5407                       0                         0   \n",
       "5408                       1                         0   \n",
       "5409                       0                         0   \n",
       "5410                       0                         0   \n",
       "5411                       0                         0   \n",
       "5412                       1                         0   \n",
       "5413                       0                         0   \n",
       "5414                       0                         0   \n",
       "5415                       0                         0   \n",
       "5416                       0                         1   \n",
       "5417                       0                         0   \n",
       "5418                       0                         0   \n",
       "5419                       0                         0   \n",
       "5420                       1                         0   \n",
       "5421                       0                         0   \n",
       "5422                       0                         0   \n",
       "5423                       0                         0   \n",
       "5424                       0                         0   \n",
       "5425                       1                         0   \n",
       "5426                       0                         0   \n",
       "5427                       0                         1   \n",
       "5428                       0                         0   \n",
       "5429                       0                         0   \n",
       "5430                       0                         0   \n",
       "5431                       0                         0   \n",
       "\n",
       "      pao2fio2Ratio_(333, 475]  pao2fio2Ratio_(475, 3000]  \\\n",
       "4298                         0                          1   \n",
       "4299                         0                          1   \n",
       "4303                         0                          0   \n",
       "4304                         0                          1   \n",
       "4305                         0                          0   \n",
       "4306                         0                          1   \n",
       "4310                         0                          1   \n",
       "4311                         0                          0   \n",
       "4317                         0                          0   \n",
       "4322                         0                          1   \n",
       "4323                         0                          1   \n",
       "4325                         1                          0   \n",
       "4331                         0                          1   \n",
       "4335                         0                          1   \n",
       "4354                         0                          1   \n",
       "4360                         0                          0   \n",
       "4362                         0                          0   \n",
       "4363                         0                          1   \n",
       "4364                         0                          1   \n",
       "4365                         0                          0   \n",
       "4366                         0                          1   \n",
       "4367                         0                          1   \n",
       "4368                         0                          1   \n",
       "4369                         0                          1   \n",
       "4370                         0                          0   \n",
       "4371                         0                          1   \n",
       "4372                         0                          1   \n",
       "4373                         0                          0   \n",
       "4374                         0                          1   \n",
       "4375                         0                          1   \n",
       "...                        ...                        ...   \n",
       "5402                         0                          1   \n",
       "5403                         0                          1   \n",
       "5404                         0                          1   \n",
       "5405                         0                          1   \n",
       "5406                         0                          1   \n",
       "5407                         0                          1   \n",
       "5408                         0                          0   \n",
       "5409                         0                          1   \n",
       "5410                         0                          1   \n",
       "5411                         0                          1   \n",
       "5412                         0                          0   \n",
       "5413                         0                          1   \n",
       "5414                         0                          1   \n",
       "5415                         0                          1   \n",
       "5416                         0                          0   \n",
       "5417                         0                          1   \n",
       "5418                         0                          1   \n",
       "5419                         0                          1   \n",
       "5420                         0                          0   \n",
       "5421                         0                          1   \n",
       "5422                         0                          1   \n",
       "5423                         0                          1   \n",
       "5424                         0                          1   \n",
       "5425                         0                          0   \n",
       "5426                         0                          1   \n",
       "5427                         0                          0   \n",
       "5428                         0                          1   \n",
       "5429                         0                          1   \n",
       "5430                         0                          1   \n",
       "5431                         0                          1   \n",
       "\n",
       "      vent_recieved_False  vent_recieved_True  \n",
       "4298                    1                   0  \n",
       "4299                    0                   1  \n",
       "4303                    0                   1  \n",
       "4304                    1                   0  \n",
       "4305                    0                   1  \n",
       "4306                    0                   1  \n",
       "4310                    1                   0  \n",
       "4311                    0                   1  \n",
       "4317                    1                   0  \n",
       "4322                    1                   0  \n",
       "4323                    0                   1  \n",
       "4325                    0                   1  \n",
       "4331                    1                   0  \n",
       "4335                    1                   0  \n",
       "4354                    0                   1  \n",
       "4360                    0                   1  \n",
       "4362                    0                   1  \n",
       "4363                    0                   1  \n",
       "4364                    1                   0  \n",
       "4365                    1                   0  \n",
       "4366                    1                   0  \n",
       "4367                    0                   1  \n",
       "4368                    1                   0  \n",
       "4369                    0                   1  \n",
       "4370                    1                   0  \n",
       "4371                    0                   1  \n",
       "4372                    1                   0  \n",
       "4373                    1                   0  \n",
       "4374                    1                   0  \n",
       "4375                    1                   0  \n",
       "...                   ...                 ...  \n",
       "5402                    0                   1  \n",
       "5403                    1                   0  \n",
       "5404                    0                   1  \n",
       "5405                    1                   0  \n",
       "5406                    0                   1  \n",
       "5407                    0                   1  \n",
       "5408                    0                   1  \n",
       "5409                    1                   0  \n",
       "5410                    1                   0  \n",
       "5411                    1                   0  \n",
       "5412                    0                   1  \n",
       "5413                    0                   1  \n",
       "5414                    0                   1  \n",
       "5415                    0                   1  \n",
       "5416                    0                   1  \n",
       "5417                    0                   1  \n",
       "5418                    1                   0  \n",
       "5419                    1                   0  \n",
       "5420                    0                   1  \n",
       "5421                    0                   1  \n",
       "5422                    1                   0  \n",
       "5423                    1                   0  \n",
       "5424                    0                   1  \n",
       "5425                    0                   1  \n",
       "5426                    1                   0  \n",
       "5427                    1                   0  \n",
       "5428                    0                   1  \n",
       "5429                    1                   0  \n",
       "5430                    1                   0  \n",
       "5431                    0                   1  \n",
       "\n",
       "[1086 rows x 48 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 53.6 ms\n"
     ]
    }
   ],
   "source": [
    "# all_xy.loc[test_index_tester]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I is 200012\n",
      "testing_sample_id is 200012\n",
      "all_xy_0 is             amax_bun  amax_creatinine  amax_daily_sofa  amax_heartrate  \\\n",
      "icustay_id                                                               \n",
      "219346      0.299501         0.826712         0.660964        0.094159   \n",
      "219360     -0.240028        -0.253202         0.160964        0.068747   \n",
      "219373     -0.155149         0.000000        -1.000000        0.096860   \n",
      "219384     -0.131174        -0.347655         0.292481        0.074574   \n",
      "219385      0.403080         2.038366         0.160964        0.166536   \n",
      "219391      0.260632         0.076014        -0.500000        0.037873   \n",
      "219411      0.104422         0.148492         0.000000        0.053689   \n",
      "219412     -0.087975         0.076014         0.403677        0.157795   \n",
      "219441     -0.181044        -0.347655         0.000000        0.125134   \n",
      "219481      0.174374        -0.079914        -0.500000        0.102193   \n",
      "219551     -0.108854        -0.079914         0.160964        0.074574   \n",
      "219612     -0.032379        -0.079914         0.160964        0.083117   \n",
      "219620      0.146224        -0.253202         0.160964        0.014305   \n",
      "219628     -0.108854        -0.164150         0.292481        0.044295   \n",
      "219630      0.081304         0.467559         0.292481       -0.118566   \n",
      "219652     -0.068362        -0.164150        -1.000000        0.053689   \n",
      "219666      0.029431        -0.555695        -0.207519        0.127583   \n",
      "219674     -0.240028        -0.253202         0.160964       -0.007404   \n",
      "219712      0.015050        -0.253202        -0.207519        0.031318   \n",
      "219769     -0.155149        -0.164150        -0.500000        0.059799   \n",
      "219794      0.216448         0.000000         0.000000        0.107438   \n",
      "219830      0.104422         0.000000        -0.500000        0.088687   \n",
      "219869      0.043199        -0.253202        -0.207519        0.104827   \n",
      "219917      0.293338         0.000000         0.000000        0.120179   \n",
      "219918      0.136269         0.408760         0.584963        0.107438   \n",
      "219923      0.056406         0.000000         0.500000        0.172936   \n",
      "219943     -0.087975        -0.164150         0.000000        0.083117   \n",
      "219945     -0.032379         0.217747         0.403677        0.125134   \n",
      "219961      0.056406         0.148492         0.403677        0.104827   \n",
      "219987      0.311484         2.079914         0.660964        0.096860   \n",
      "...              ...              ...              ...             ...   \n",
      "299557      0.174374         0.467559         0.160964        0.074574   \n",
      "299593      0.000000         0.217747         0.160964        0.091435   \n",
      "299614     -0.209194        -0.448205        -0.207519        0.132424   \n",
      "299629      0.260632         0.631709         0.160964        0.122666   \n",
      "299630      0.456100         2.325925         0.292481       -0.011172   \n",
      "299645      0.043199         0.076014         0.000000        0.151076   \n",
      "299654      0.411824         1.787638         1.043731        0.246092   \n",
      "299655      0.529792         0.871874         0.160964       -0.030723   \n",
      "299685     -0.209194        -0.079914        -0.207519        0.014305   \n",
      "299695      0.253675         0.000000        -0.500000        0.041101   \n",
      "299715      0.305548         0.076014         0.903677        0.223545   \n",
      "299728      0.200269         0.076014         0.903677        0.193375   \n",
      "299734     -0.068362        -0.164150        -0.207519        0.014305   \n",
      "299736     -0.155149        -0.079914         0.000000        0.170817   \n",
      "299751      0.311484         0.284055         0.403677        0.130013   \n",
      "299765      0.043199        -0.079914         0.000000        0.027988   \n",
      "299767     -0.209194        -0.164150         0.160964        0.071674   \n",
      "299800      0.155882         0.217747         0.160964        0.044295   \n",
      "299806     -0.049870         0.467559         0.160964        0.139548   \n",
      "299826      0.274114         0.000000         0.160964        0.085914   \n",
      "299828      0.104422         0.780201         0.403677        0.080295   \n",
      "299832      0.299501         1.297662         0.792481        0.017782   \n",
      "299853      0.360513         0.148492         0.160964        0.056759   \n",
      "299863      0.200269        -0.079914         0.729716        0.141888   \n",
      "299883      0.000000         0.284055        -0.207519        0.172936   \n",
      "299888     -0.049870        -0.079914         0.500000        0.080295   \n",
      "299913      0.484876         0.631709         0.160964        0.112596   \n",
      "299914      0.350242         2.641747         0.160964        0.047458   \n",
      "299917     -0.405288        -0.253202        -0.500000        0.056759   \n",
      "299995     -0.355418        -0.164150        -1.000000        0.050589   \n",
      "\n",
      "            amax_meanartpress  amax_platelet  amax_ptt  amax_sysbp  \\\n",
      "icustay_id                                                           \n",
      "219346               0.207891      -0.601942  0.045286    0.009628   \n",
      "219360               0.134597      -0.018116  0.012640    0.100420   \n",
      "219373               0.117153      -0.030310 -0.078711    0.065470   \n",
      "219384               0.108071      -0.013434 -0.036036    0.098681   \n",
      "219385               0.219026       0.167399 -0.119559    0.103866   \n",
      "219391               0.101873       0.020600 -0.084079    0.103866   \n",
      "219411               0.133653      -0.030310 -0.018697    0.095173   \n",
      "219412               0.210148       0.029235  0.012640    0.196913   \n",
      "219441               0.160889       0.114145  0.012640    0.105574   \n",
      "219481               0.056071       0.125022  0.012640    0.011986   \n",
      "219551               0.008422      -0.370230  0.086380   -0.025561   \n",
      "219612               0.082546      -0.024122  0.225030    0.095173   \n",
      "219620               0.062011      -0.027812  0.012640    0.071275   \n",
      "219628               0.128885      -0.126316  0.107699    0.134719   \n",
      "219630               0.179517       0.120259  0.014705    0.091621   \n",
      "219652               0.047571       0.166920  0.012640    0.067418   \n",
      "219666               0.062011      -0.057119 -0.027841    0.014325   \n",
      "219674               0.054870       0.036616  0.169083    0.014325   \n",
      "219712               0.069000      -0.057119  0.216875    0.034571   \n",
      "219769               0.054870       0.281735  0.012640    0.038885   \n",
      "219794               0.116157      -0.289405 -0.036036    0.053501   \n",
      "219830               0.092352       0.104617 -0.014210    0.084386   \n",
      "219869               0.101873      -0.008858 -0.045635    0.091621   \n",
      "219917               0.089116       0.211562 -0.085433    0.091621   \n",
      "219918               0.142984       0.098010  0.542324    0.123729   \n",
      "219923               0.134597      -0.145234 -0.030164    0.113967   \n",
      "219943               0.191639      -0.054275 -0.007582    0.018947   \n",
      "219945               0.079212      -0.012281 -0.064347    0.053501   \n",
      "219961               0.032467      -0.186576 -0.054247   -0.002457   \n",
      "219987               0.148458      -0.075070 -0.004313    0.063509   \n",
      "...                       ...            ...       ...         ...   \n",
      "299557               0.137417       0.095307  0.014705    0.152645   \n",
      "299593               0.089116       0.068445  0.103696    0.007250   \n",
      "299614               0.069000       0.047211 -0.014210    0.041019   \n",
      "299629               0.172023       0.105266  0.058293    0.088026   \n",
      "299630               0.142984       0.022551  0.036670    0.140818   \n",
      "299645               0.095556       0.044613  0.024854    0.069353   \n",
      "299654               0.324817      -0.192038  0.373279    0.041019   \n",
      "299655               0.012554      -0.026575  0.041482    0.007250   \n",
      "299685               0.040106      -0.027812 -0.017570    0.021231   \n",
      "299695               0.118146       0.078289 -0.072111    0.084386   \n",
      "299715               0.134597      -0.061462 -0.017570    0.091621   \n",
      "299728               0.296670      -0.108998  0.473858    0.014325   \n",
      "299734               0.082546      -0.052867  0.244104    0.043137   \n",
      "299736               0.153843       0.065318 -0.069505    0.120511   \n",
      "299751               0.028580       0.046349 -0.099264    0.047327   \n",
      "299765               0.172023      -0.018116 -0.085433    0.105574   \n",
      "299767               0.101873      -0.008858  0.012640    0.071275   \n",
      "299800               0.098730       0.036616  0.012640    0.076966   \n",
      "299806               0.164355       0.081225  0.035700    0.096932   \n",
      "299826               0.040106      -0.094746  0.067307    0.036736   \n",
      "299828               0.047571      -0.103537  0.014705    0.011986   \n",
      "299832               0.054870      -0.192038  0.301125    0.027977   \n",
      "299853               0.164355       0.019618  0.091413    0.110639   \n",
      "299863               0.125991      -0.081420  0.048113    0.061535   \n",
      "299883               0.058460       0.054813  0.022847    0.027977   \n",
      "299888               0.098730       0.016643 -0.051765    0.041019   \n",
      "299913               0.072439       0.113523 -0.025534    0.071275   \n",
      "299914               0.032467       0.089790  0.015733    0.063509   \n",
      "299917               0.016634      -0.035398  0.012640   -0.028245   \n",
      "299995               0.072439       0.064529  0.012640    0.071275   \n",
      "\n",
      "            amax_temperature  amin_bun         ...          \\\n",
      "icustay_id                                     ...           \n",
      "219346              0.005728  0.183238         ...           \n",
      "219360              0.063709 -0.312219         ...           \n",
      "219373              0.021964 -0.240028         ...           \n",
      "219384              0.025592 -0.209194         ...           \n",
      "219385              0.028005 -0.032379         ...           \n",
      "219391              0.040008 -0.087975         ...           \n",
      "219411              0.028005  0.015050         ...           \n",
      "219412              0.119382 -0.274114         ...           \n",
      "219441              0.055853 -0.405288         ...           \n",
      "219481              0.021964  0.104422         ...           \n",
      "219551              0.040008 -0.181044         ...           \n",
      "219612              0.032017 -0.108854         ...           \n",
      "219620              0.005728  0.093070         ...           \n",
      "219628             -0.018986 -0.274114         ...           \n",
      "219630             -0.002462  0.043199         ...           \n",
      "219652              0.015897 -0.240028         ...           \n",
      "219666              0.032017 -0.049870         ...           \n",
      "219674              0.013869 -0.312219         ...           \n",
      "219712              0.001639 -0.087975         ...           \n",
      "219769              0.023981 -0.155149         ...           \n",
      "219794              0.067621  0.155882         ...           \n",
      "219830              0.019945  0.043199         ...           \n",
      "219869              0.019945 -0.131174         ...           \n",
      "219917             -0.006575  0.015050         ...           \n",
      "219918              0.119382 -0.108854         ...           \n",
      "219923              0.055853 -0.087975         ...           \n",
      "219943              0.011838 -0.209194         ...           \n",
      "219945              0.036018 -0.155149         ...           \n",
      "219961              0.011838  0.029431         ...           \n",
      "219987              0.085088  0.104422         ...           \n",
      "...                      ...       ...         ...           \n",
      "299557              0.043986 -0.015784         ...           \n",
      "299593              0.068402 -0.087975         ...           \n",
      "299614              0.015898 -0.355418         ...           \n",
      "299629              0.133613  0.104422         ...           \n",
      "299630              0.001639  0.328666         ...           \n",
      "299645              0.000000 -0.087975         ...           \n",
      "299654              0.123516  0.056406         ...           \n",
      "299655             -0.012767  0.504894         ...           \n",
      "299685              0.009804 -0.274114         ...           \n",
      "299695              0.023980  0.000000         ...           \n",
      "299715              0.075411  0.246564         ...           \n",
      "299728              0.082385 -0.087975         ...           \n",
      "299734              0.040008 -0.108854         ...           \n",
      "299736              0.041998 -0.240028         ...           \n",
      "299751              0.038014  0.191865         ...           \n",
      "299765              0.055853 -0.068362         ...           \n",
      "299767              0.025994 -0.209194         ...           \n",
      "299800              0.001639  0.136269         ...           \n",
      "299806              0.081225 -0.108854         ...           \n",
      "299826             -0.000410 -0.108854         ...           \n",
      "299828              0.007768  0.029431         ...           \n",
      "299832             -0.000410  0.246564         ...           \n",
      "299853              0.013869  0.069095         ...           \n",
      "299863              0.003685  0.093070         ...           \n",
      "299883             -0.008636 -0.131174         ...           \n",
      "299888              0.063709 -0.131174         ...           \n",
      "299913              0.017923  0.420338         ...           \n",
      "299914              0.009804  0.208459         ...           \n",
      "299917              0.071521 -0.464272         ...           \n",
      "299995              0.034019 -0.355418         ...           \n",
      "\n",
      "            any_vasoactive_False  any_vasoactive_True  leukocyte_False  \\\n",
      "icustay_id                                                               \n",
      "219346                         1                    0                1   \n",
      "219360                         1                    0                1   \n",
      "219373                         1                    0                1   \n",
      "219384                         0                    1                1   \n",
      "219385                         1                    0                1   \n",
      "219391                         1                    0                1   \n",
      "219411                         1                    0                1   \n",
      "219412                         1                    0                1   \n",
      "219441                         1                    0                1   \n",
      "219481                         1                    0                1   \n",
      "219551                         1                    0                1   \n",
      "219612                         0                    1                1   \n",
      "219620                         1                    0                1   \n",
      "219628                         0                    1                1   \n",
      "219630                         1                    0                1   \n",
      "219652                         1                    0                1   \n",
      "219666                         1                    0                1   \n",
      "219674                         1                    0                1   \n",
      "219712                         0                    1                0   \n",
      "219769                         1                    0                1   \n",
      "219794                         1                    0                1   \n",
      "219830                         1                    0                1   \n",
      "219869                         1                    0                1   \n",
      "219917                         1                    0                1   \n",
      "219918                         0                    1                1   \n",
      "219923                         1                    0                1   \n",
      "219943                         1                    0                1   \n",
      "219945                         0                    1                1   \n",
      "219961                         0                    1                1   \n",
      "219987                         0                    1                0   \n",
      "...                          ...                  ...              ...   \n",
      "299557                         0                    1                1   \n",
      "299593                         0                    1                1   \n",
      "299614                         0                    1                1   \n",
      "299629                         1                    0                0   \n",
      "299630                         1                    0                1   \n",
      "299645                         0                    1                1   \n",
      "299654                         0                    1                1   \n",
      "299655                         1                    0                1   \n",
      "299685                         1                    0                1   \n",
      "299695                         1                    0                1   \n",
      "299715                         0                    1                1   \n",
      "299728                         0                    1                1   \n",
      "299734                         0                    1                1   \n",
      "299736                         1                    0                1   \n",
      "299751                         0                    1                1   \n",
      "299765                         1                    0                1   \n",
      "299767                         1                    0                1   \n",
      "299800                         1                    0                1   \n",
      "299806                         1                    0                0   \n",
      "299826                         1                    0                1   \n",
      "299828                         1                    0                1   \n",
      "299832                         1                    0                1   \n",
      "299853                         1                    0                1   \n",
      "299863                         0                    1                1   \n",
      "299883                         1                    0                1   \n",
      "299888                         0                    1                1   \n",
      "299913                         1                    0                1   \n",
      "299914                         1                    0                1   \n",
      "299917                         1                    0                1   \n",
      "299995                         1                    0                1   \n",
      "\n",
      "            leukocyte_True  pao2fio2Ratio_(0, 200]  pao2fio2Ratio_(200, 333]  \\\n",
      "icustay_id                                                                     \n",
      "219346                   0                       0                         0   \n",
      "219360                   0                       0                         0   \n",
      "219373                   0                       0                         0   \n",
      "219384                   0                       0                         0   \n",
      "219385                   0                       0                         0   \n",
      "219391                   0                       0                         0   \n",
      "219411                   0                       0                         0   \n",
      "219412                   0                       0                         1   \n",
      "219441                   0                       0                         1   \n",
      "219481                   0                       0                         0   \n",
      "219551                   0                       0                         0   \n",
      "219612                   0                       0                         0   \n",
      "219620                   0                       0                         0   \n",
      "219628                   0                       0                         0   \n",
      "219630                   0                       0                         0   \n",
      "219652                   0                       0                         0   \n",
      "219666                   0                       0                         0   \n",
      "219674                   0                       0                         0   \n",
      "219712                   1                       0                         0   \n",
      "219769                   0                       0                         0   \n",
      "219794                   0                       0                         0   \n",
      "219830                   0                       0                         0   \n",
      "219869                   0                       0                         0   \n",
      "219917                   0                       0                         1   \n",
      "219918                   0                       1                         0   \n",
      "219923                   0                       0                         0   \n",
      "219943                   0                       0                         0   \n",
      "219945                   0                       0                         0   \n",
      "219961                   0                       0                         0   \n",
      "219987                   1                       0                         0   \n",
      "...                    ...                     ...                       ...   \n",
      "299557                   0                       0                         0   \n",
      "299593                   0                       0                         0   \n",
      "299614                   0                       0                         0   \n",
      "299629                   1                       0                         0   \n",
      "299630                   0                       0                         0   \n",
      "299645                   0                       0                         0   \n",
      "299654                   0                       1                         0   \n",
      "299655                   0                       0                         0   \n",
      "299685                   0                       0                         0   \n",
      "299695                   0                       0                         0   \n",
      "299715                   0                       1                         0   \n",
      "299728                   0                       0                         0   \n",
      "299734                   0                       0                         0   \n",
      "299736                   0                       0                         0   \n",
      "299751                   0                       0                         1   \n",
      "299765                   0                       0                         0   \n",
      "299767                   0                       0                         0   \n",
      "299800                   0                       0                         0   \n",
      "299806                   1                       1                         0   \n",
      "299826                   0                       0                         0   \n",
      "299828                   0                       0                         0   \n",
      "299832                   0                       0                         0   \n",
      "299853                   0                       0                         0   \n",
      "299863                   0                       1                         0   \n",
      "299883                   0                       0                         0   \n",
      "299888                   0                       0                         1   \n",
      "299913                   0                       0                         0   \n",
      "299914                   0                       0                         0   \n",
      "299917                   0                       0                         0   \n",
      "299995                   0                       0                         0   \n",
      "\n",
      "            pao2fio2Ratio_(333, 475]  pao2fio2Ratio_(475, 3000]  \\\n",
      "icustay_id                                                        \n",
      "219346                             0                          1   \n",
      "219360                             0                          1   \n",
      "219373                             1                          0   \n",
      "219384                             0                          1   \n",
      "219385                             0                          1   \n",
      "219391                             0                          1   \n",
      "219411                             0                          1   \n",
      "219412                             0                          0   \n",
      "219441                             0                          0   \n",
      "219481                             0                          1   \n",
      "219551                             0                          1   \n",
      "219612                             0                          1   \n",
      "219620                             0                          1   \n",
      "219628                             0                          1   \n",
      "219630                             0                          1   \n",
      "219652                             0                          1   \n",
      "219666                             0                          1   \n",
      "219674                             0                          1   \n",
      "219712                             0                          1   \n",
      "219769                             0                          1   \n",
      "219794                             0                          1   \n",
      "219830                             1                          0   \n",
      "219869                             0                          1   \n",
      "219917                             0                          0   \n",
      "219918                             0                          0   \n",
      "219923                             0                          1   \n",
      "219943                             0                          1   \n",
      "219945                             1                          0   \n",
      "219961                             0                          1   \n",
      "219987                             0                          1   \n",
      "...                              ...                        ...   \n",
      "299557                             0                          1   \n",
      "299593                             0                          1   \n",
      "299614                             0                          1   \n",
      "299629                             0                          1   \n",
      "299630                             0                          1   \n",
      "299645                             0                          1   \n",
      "299654                             0                          0   \n",
      "299655                             0                          1   \n",
      "299685                             0                          1   \n",
      "299695                             0                          1   \n",
      "299715                             0                          0   \n",
      "299728                             0                          1   \n",
      "299734                             0                          1   \n",
      "299736                             0                          1   \n",
      "299751                             0                          0   \n",
      "299765                             0                          1   \n",
      "299767                             0                          1   \n",
      "299800                             0                          1   \n",
      "299806                             0                          0   \n",
      "299826                             0                          1   \n",
      "299828                             0                          1   \n",
      "299832                             0                          1   \n",
      "299853                             0                          1   \n",
      "299863                             0                          0   \n",
      "299883                             0                          1   \n",
      "299888                             0                          0   \n",
      "299913                             0                          1   \n",
      "299914                             0                          1   \n",
      "299917                             0                          1   \n",
      "299995                             0                          1   \n",
      "\n",
      "            vent_recieved_False  vent_recieved_True  \n",
      "icustay_id                                           \n",
      "219346                        1                   0  \n",
      "219360                        1                   0  \n",
      "219373                        0                   1  \n",
      "219384                        0                   1  \n",
      "219385                        1                   0  \n",
      "219391                        1                   0  \n",
      "219411                        1                   0  \n",
      "219412                        0                   1  \n",
      "219441                        0                   1  \n",
      "219481                        0                   1  \n",
      "219551                        1                   0  \n",
      "219612                        0                   1  \n",
      "219620                        1                   0  \n",
      "219628                        0                   1  \n",
      "219630                        1                   0  \n",
      "219652                        1                   0  \n",
      "219666                        1                   0  \n",
      "219674                        1                   0  \n",
      "219712                        1                   0  \n",
      "219769                        1                   0  \n",
      "219794                        1                   0  \n",
      "219830                        0                   1  \n",
      "219869                        1                   0  \n",
      "219917                        1                   0  \n",
      "219918                        0                   1  \n",
      "219923                        1                   0  \n",
      "219943                        1                   0  \n",
      "219945                        1                   0  \n",
      "219961                        0                   1  \n",
      "219987                        0                   1  \n",
      "...                         ...                 ...  \n",
      "299557                        0                   1  \n",
      "299593                        1                   0  \n",
      "299614                        0                   1  \n",
      "299629                        1                   0  \n",
      "299630                        0                   1  \n",
      "299645                        0                   1  \n",
      "299654                        0                   1  \n",
      "299655                        1                   0  \n",
      "299685                        1                   0  \n",
      "299695                        1                   0  \n",
      "299715                        0                   1  \n",
      "299728                        0                   1  \n",
      "299734                        0                   1  \n",
      "299736                        0                   1  \n",
      "299751                        0                   1  \n",
      "299765                        0                   1  \n",
      "299767                        1                   0  \n",
      "299800                        1                   0  \n",
      "299806                        0                   1  \n",
      "299826                        0                   1  \n",
      "299828                        1                   0  \n",
      "299832                        1                   0  \n",
      "299853                        0                   1  \n",
      "299863                        0                   1  \n",
      "299883                        1                   0  \n",
      "299888                        1                   0  \n",
      "299913                        0                   1  \n",
      "299914                        1                   0  \n",
      "299917                        1                   0  \n",
      "299995                        0                   1  \n",
      "\n",
      "[4345 rows x 48 columns]\n",
      "all_xy_training is             amax_bun  amax_creatinine  amax_daily_sofa  amax_heartrate  \\\n",
      "icustay_id                                                               \n",
      "219346      0.299501         0.826712         0.660964        0.094159   \n",
      "219360     -0.240028        -0.253202         0.160964        0.068747   \n",
      "219373     -0.155149         0.000000        -1.000000        0.096860   \n",
      "219384     -0.131174        -0.347655         0.292481        0.074574   \n",
      "219385      0.403080         2.038366         0.160964        0.166536   \n",
      "219391      0.260632         0.076014        -0.500000        0.037873   \n",
      "219411      0.104422         0.148492         0.000000        0.053689   \n",
      "219412     -0.087975         0.076014         0.403677        0.157795   \n",
      "219441     -0.181044        -0.347655         0.000000        0.125134   \n",
      "219481      0.174374        -0.079914        -0.500000        0.102193   \n",
      "219551     -0.108854        -0.079914         0.160964        0.074574   \n",
      "219612     -0.032379        -0.079914         0.160964        0.083117   \n",
      "219620      0.146224        -0.253202         0.160964        0.014305   \n",
      "219628     -0.108854        -0.164150         0.292481        0.044295   \n",
      "219630      0.081304         0.467559         0.292481       -0.118566   \n",
      "219652     -0.068362        -0.164150        -1.000000        0.053689   \n",
      "219666      0.029431        -0.555695        -0.207519        0.127583   \n",
      "219674     -0.240028        -0.253202         0.160964       -0.007404   \n",
      "219712      0.015050        -0.253202        -0.207519        0.031318   \n",
      "219769     -0.155149        -0.164150        -0.500000        0.059799   \n",
      "219794      0.216448         0.000000         0.000000        0.107438   \n",
      "219830      0.104422         0.000000        -0.500000        0.088687   \n",
      "219869      0.043199        -0.253202        -0.207519        0.104827   \n",
      "219917      0.293338         0.000000         0.000000        0.120179   \n",
      "219918      0.136269         0.408760         0.584963        0.107438   \n",
      "219923      0.056406         0.000000         0.500000        0.172936   \n",
      "219943     -0.087975        -0.164150         0.000000        0.083117   \n",
      "219945     -0.032379         0.217747         0.403677        0.125134   \n",
      "219961      0.056406         0.148492         0.403677        0.104827   \n",
      "219987      0.311484         2.079914         0.660964        0.096860   \n",
      "...              ...              ...              ...             ...   \n",
      "299593      0.000000         0.217747         0.160964        0.091435   \n",
      "299614     -0.209194        -0.448205        -0.207519        0.132424   \n",
      "299629      0.260632         0.631709         0.160964        0.122666   \n",
      "299630      0.456100         2.325925         0.292481       -0.011172   \n",
      "299645      0.043199         0.076014         0.000000        0.151076   \n",
      "299654      0.411824         1.787638         1.043731        0.246092   \n",
      "299655      0.529792         0.871874         0.160964       -0.030723   \n",
      "299685     -0.209194        -0.079914        -0.207519        0.014305   \n",
      "299695      0.253675         0.000000        -0.500000        0.041101   \n",
      "299715      0.305548         0.076014         0.903677        0.223545   \n",
      "299728      0.200269         0.076014         0.903677        0.193375   \n",
      "299734     -0.068362        -0.164150        -0.207519        0.014305   \n",
      "299736     -0.155149        -0.079914         0.000000        0.170817   \n",
      "299751      0.311484         0.284055         0.403677        0.130013   \n",
      "299765      0.043199        -0.079914         0.000000        0.027988   \n",
      "299767     -0.209194        -0.164150         0.160964        0.071674   \n",
      "299800      0.155882         0.217747         0.160964        0.044295   \n",
      "299806     -0.049870         0.467559         0.160964        0.139548   \n",
      "299826      0.274114         0.000000         0.160964        0.085914   \n",
      "299828      0.104422         0.780201         0.403677        0.080295   \n",
      "299832      0.299501         1.297662         0.792481        0.017782   \n",
      "299853      0.360513         0.148492         0.160964        0.056759   \n",
      "299863      0.200269        -0.079914         0.729716        0.141888   \n",
      "299883      0.000000         0.284055        -0.207519        0.172936   \n",
      "299888     -0.049870        -0.079914         0.500000        0.080295   \n",
      "299913      0.484876         0.631709         0.160964        0.112596   \n",
      "299914      0.350242         2.641747         0.160964        0.047458   \n",
      "299917     -0.405288        -0.253202        -0.500000        0.056759   \n",
      "299995     -0.355418        -0.164150        -1.000000        0.050589   \n",
      "200012      0.069095         0.076014        -0.500000        0.077448   \n",
      "\n",
      "            amax_meanartpress  amax_platelet  amax_ptt  amax_sysbp  \\\n",
      "icustay_id                                                           \n",
      "219346               0.207891      -0.601942  0.045286    0.009628   \n",
      "219360               0.134597      -0.018116  0.012640    0.100420   \n",
      "219373               0.117153      -0.030310 -0.078711    0.065470   \n",
      "219384               0.108071      -0.013434 -0.036036    0.098681   \n",
      "219385               0.219026       0.167399 -0.119559    0.103866   \n",
      "219391               0.101873       0.020600 -0.084079    0.103866   \n",
      "219411               0.133653      -0.030310 -0.018697    0.095173   \n",
      "219412               0.210148       0.029235  0.012640    0.196913   \n",
      "219441               0.160889       0.114145  0.012640    0.105574   \n",
      "219481               0.056071       0.125022  0.012640    0.011986   \n",
      "219551               0.008422      -0.370230  0.086380   -0.025561   \n",
      "219612               0.082546      -0.024122  0.225030    0.095173   \n",
      "219620               0.062011      -0.027812  0.012640    0.071275   \n",
      "219628               0.128885      -0.126316  0.107699    0.134719   \n",
      "219630               0.179517       0.120259  0.014705    0.091621   \n",
      "219652               0.047571       0.166920  0.012640    0.067418   \n",
      "219666               0.062011      -0.057119 -0.027841    0.014325   \n",
      "219674               0.054870       0.036616  0.169083    0.014325   \n",
      "219712               0.069000      -0.057119  0.216875    0.034571   \n",
      "219769               0.054870       0.281735  0.012640    0.038885   \n",
      "219794               0.116157      -0.289405 -0.036036    0.053501   \n",
      "219830               0.092352       0.104617 -0.014210    0.084386   \n",
      "219869               0.101873      -0.008858 -0.045635    0.091621   \n",
      "219917               0.089116       0.211562 -0.085433    0.091621   \n",
      "219918               0.142984       0.098010  0.542324    0.123729   \n",
      "219923               0.134597      -0.145234 -0.030164    0.113967   \n",
      "219943               0.191639      -0.054275 -0.007582    0.018947   \n",
      "219945               0.079212      -0.012281 -0.064347    0.053501   \n",
      "219961               0.032467      -0.186576 -0.054247   -0.002457   \n",
      "219987               0.148458      -0.075070 -0.004313    0.063509   \n",
      "...                       ...            ...       ...         ...   \n",
      "299593               0.089116       0.068445  0.103696    0.007250   \n",
      "299614               0.069000       0.047211 -0.014210    0.041019   \n",
      "299629               0.172023       0.105266  0.058293    0.088026   \n",
      "299630               0.142984       0.022551  0.036670    0.140818   \n",
      "299645               0.095556       0.044613  0.024854    0.069353   \n",
      "299654               0.324817      -0.192038  0.373279    0.041019   \n",
      "299655               0.012554      -0.026575  0.041482    0.007250   \n",
      "299685               0.040106      -0.027812 -0.017570    0.021231   \n",
      "299695               0.118146       0.078289 -0.072111    0.084386   \n",
      "299715               0.134597      -0.061462 -0.017570    0.091621   \n",
      "299728               0.296670      -0.108998  0.473858    0.014325   \n",
      "299734               0.082546      -0.052867  0.244104    0.043137   \n",
      "299736               0.153843       0.065318 -0.069505    0.120511   \n",
      "299751               0.028580       0.046349 -0.099264    0.047327   \n",
      "299765               0.172023      -0.018116 -0.085433    0.105574   \n",
      "299767               0.101873      -0.008858  0.012640    0.071275   \n",
      "299800               0.098730       0.036616  0.012640    0.076966   \n",
      "299806               0.164355       0.081225  0.035700    0.096932   \n",
      "299826               0.040106      -0.094746  0.067307    0.036736   \n",
      "299828               0.047571      -0.103537  0.014705    0.011986   \n",
      "299832               0.054870      -0.192038  0.301125    0.027977   \n",
      "299853               0.164355       0.019618  0.091413    0.110639   \n",
      "299863               0.125991      -0.081420  0.048113    0.061535   \n",
      "299883               0.058460       0.054813  0.022847    0.027977   \n",
      "299888               0.098730       0.016643 -0.051765    0.041019   \n",
      "299913               0.072439       0.113523 -0.025534    0.071275   \n",
      "299914               0.032467       0.089790  0.015733    0.063509   \n",
      "299917               0.016634      -0.035398  0.012640   -0.028245   \n",
      "299995               0.072439       0.064529  0.012640    0.071275   \n",
      "200012               0.047571      -0.076639  0.012640   -0.022901   \n",
      "\n",
      "            amax_temperature  amin_bun         ...          \\\n",
      "icustay_id                                     ...           \n",
      "219346              0.005728  0.183238         ...           \n",
      "219360              0.063709 -0.312219         ...           \n",
      "219373              0.021964 -0.240028         ...           \n",
      "219384              0.025592 -0.209194         ...           \n",
      "219385              0.028005 -0.032379         ...           \n",
      "219391              0.040008 -0.087975         ...           \n",
      "219411              0.028005  0.015050         ...           \n",
      "219412              0.119382 -0.274114         ...           \n",
      "219441              0.055853 -0.405288         ...           \n",
      "219481              0.021964  0.104422         ...           \n",
      "219551              0.040008 -0.181044         ...           \n",
      "219612              0.032017 -0.108854         ...           \n",
      "219620              0.005728  0.093070         ...           \n",
      "219628             -0.018986 -0.274114         ...           \n",
      "219630             -0.002462  0.043199         ...           \n",
      "219652              0.015897 -0.240028         ...           \n",
      "219666              0.032017 -0.049870         ...           \n",
      "219674              0.013869 -0.312219         ...           \n",
      "219712              0.001639 -0.087975         ...           \n",
      "219769              0.023981 -0.155149         ...           \n",
      "219794              0.067621  0.155882         ...           \n",
      "219830              0.019945  0.043199         ...           \n",
      "219869              0.019945 -0.131174         ...           \n",
      "219917             -0.006575  0.015050         ...           \n",
      "219918              0.119382 -0.108854         ...           \n",
      "219923              0.055853 -0.087975         ...           \n",
      "219943              0.011838 -0.209194         ...           \n",
      "219945              0.036018 -0.155149         ...           \n",
      "219961              0.011838  0.029431         ...           \n",
      "219987              0.085088  0.104422         ...           \n",
      "...                      ...       ...         ...           \n",
      "299593              0.068402 -0.087975         ...           \n",
      "299614              0.015898 -0.355418         ...           \n",
      "299629              0.133613  0.104422         ...           \n",
      "299630              0.001639  0.328666         ...           \n",
      "299645              0.000000 -0.087975         ...           \n",
      "299654              0.123516  0.056406         ...           \n",
      "299655             -0.012767  0.504894         ...           \n",
      "299685              0.009804 -0.274114         ...           \n",
      "299695              0.023980  0.000000         ...           \n",
      "299715              0.075411  0.246564         ...           \n",
      "299728              0.082385 -0.087975         ...           \n",
      "299734              0.040008 -0.108854         ...           \n",
      "299736              0.041998 -0.240028         ...           \n",
      "299751              0.038014  0.191865         ...           \n",
      "299765              0.055853 -0.068362         ...           \n",
      "299767              0.025994 -0.209194         ...           \n",
      "299800              0.001639  0.136269         ...           \n",
      "299806              0.081225 -0.108854         ...           \n",
      "299826             -0.000410 -0.108854         ...           \n",
      "299828              0.007768  0.029431         ...           \n",
      "299832             -0.000410  0.246564         ...           \n",
      "299853              0.013869  0.069095         ...           \n",
      "299863              0.003685  0.093070         ...           \n",
      "299883             -0.008636 -0.131174         ...           \n",
      "299888              0.063709 -0.131174         ...           \n",
      "299913              0.017923  0.420338         ...           \n",
      "299914              0.009804  0.208459         ...           \n",
      "299917              0.071521 -0.464272         ...           \n",
      "299995              0.034019 -0.355418         ...           \n",
      "200012              0.021964 -0.049870         ...           \n",
      "\n",
      "            any_vasoactive_False  any_vasoactive_True  leukocyte_False  \\\n",
      "icustay_id                                                               \n",
      "219346                       1.0                  0.0              1.0   \n",
      "219360                       1.0                  0.0              1.0   \n",
      "219373                       1.0                  0.0              1.0   \n",
      "219384                       0.0                  1.0              1.0   \n",
      "219385                       1.0                  0.0              1.0   \n",
      "219391                       1.0                  0.0              1.0   \n",
      "219411                       1.0                  0.0              1.0   \n",
      "219412                       1.0                  0.0              1.0   \n",
      "219441                       1.0                  0.0              1.0   \n",
      "219481                       1.0                  0.0              1.0   \n",
      "219551                       1.0                  0.0              1.0   \n",
      "219612                       0.0                  1.0              1.0   \n",
      "219620                       1.0                  0.0              1.0   \n",
      "219628                       0.0                  1.0              1.0   \n",
      "219630                       1.0                  0.0              1.0   \n",
      "219652                       1.0                  0.0              1.0   \n",
      "219666                       1.0                  0.0              1.0   \n",
      "219674                       1.0                  0.0              1.0   \n",
      "219712                       0.0                  1.0              0.0   \n",
      "219769                       1.0                  0.0              1.0   \n",
      "219794                       1.0                  0.0              1.0   \n",
      "219830                       1.0                  0.0              1.0   \n",
      "219869                       1.0                  0.0              1.0   \n",
      "219917                       1.0                  0.0              1.0   \n",
      "219918                       0.0                  1.0              1.0   \n",
      "219923                       1.0                  0.0              1.0   \n",
      "219943                       1.0                  0.0              1.0   \n",
      "219945                       0.0                  1.0              1.0   \n",
      "219961                       0.0                  1.0              1.0   \n",
      "219987                       0.0                  1.0              0.0   \n",
      "...                          ...                  ...              ...   \n",
      "299593                       0.0                  1.0              1.0   \n",
      "299614                       0.0                  1.0              1.0   \n",
      "299629                       1.0                  0.0              0.0   \n",
      "299630                       1.0                  0.0              1.0   \n",
      "299645                       0.0                  1.0              1.0   \n",
      "299654                       0.0                  1.0              1.0   \n",
      "299655                       1.0                  0.0              1.0   \n",
      "299685                       1.0                  0.0              1.0   \n",
      "299695                       1.0                  0.0              1.0   \n",
      "299715                       0.0                  1.0              1.0   \n",
      "299728                       0.0                  1.0              1.0   \n",
      "299734                       0.0                  1.0              1.0   \n",
      "299736                       1.0                  0.0              1.0   \n",
      "299751                       0.0                  1.0              1.0   \n",
      "299765                       1.0                  0.0              1.0   \n",
      "299767                       1.0                  0.0              1.0   \n",
      "299800                       1.0                  0.0              1.0   \n",
      "299806                       1.0                  0.0              0.0   \n",
      "299826                       1.0                  0.0              1.0   \n",
      "299828                       1.0                  0.0              1.0   \n",
      "299832                       1.0                  0.0              1.0   \n",
      "299853                       1.0                  0.0              1.0   \n",
      "299863                       0.0                  1.0              1.0   \n",
      "299883                       1.0                  0.0              1.0   \n",
      "299888                       0.0                  1.0              1.0   \n",
      "299913                       1.0                  0.0              1.0   \n",
      "299914                       1.0                  0.0              1.0   \n",
      "299917                       1.0                  0.0              1.0   \n",
      "299995                       1.0                  0.0              1.0   \n",
      "200012                       1.0                  0.0              1.0   \n",
      "\n",
      "            leukocyte_True  pao2fio2Ratio_(0, 200]  pao2fio2Ratio_(200, 333]  \\\n",
      "icustay_id                                                                     \n",
      "219346                 0.0                     0.0                       0.0   \n",
      "219360                 0.0                     0.0                       0.0   \n",
      "219373                 0.0                     0.0                       0.0   \n",
      "219384                 0.0                     0.0                       0.0   \n",
      "219385                 0.0                     0.0                       0.0   \n",
      "219391                 0.0                     0.0                       0.0   \n",
      "219411                 0.0                     0.0                       0.0   \n",
      "219412                 0.0                     0.0                       1.0   \n",
      "219441                 0.0                     0.0                       1.0   \n",
      "219481                 0.0                     0.0                       0.0   \n",
      "219551                 0.0                     0.0                       0.0   \n",
      "219612                 0.0                     0.0                       0.0   \n",
      "219620                 0.0                     0.0                       0.0   \n",
      "219628                 0.0                     0.0                       0.0   \n",
      "219630                 0.0                     0.0                       0.0   \n",
      "219652                 0.0                     0.0                       0.0   \n",
      "219666                 0.0                     0.0                       0.0   \n",
      "219674                 0.0                     0.0                       0.0   \n",
      "219712                 1.0                     0.0                       0.0   \n",
      "219769                 0.0                     0.0                       0.0   \n",
      "219794                 0.0                     0.0                       0.0   \n",
      "219830                 0.0                     0.0                       0.0   \n",
      "219869                 0.0                     0.0                       0.0   \n",
      "219917                 0.0                     0.0                       1.0   \n",
      "219918                 0.0                     1.0                       0.0   \n",
      "219923                 0.0                     0.0                       0.0   \n",
      "219943                 0.0                     0.0                       0.0   \n",
      "219945                 0.0                     0.0                       0.0   \n",
      "219961                 0.0                     0.0                       0.0   \n",
      "219987                 1.0                     0.0                       0.0   \n",
      "...                    ...                     ...                       ...   \n",
      "299593                 0.0                     0.0                       0.0   \n",
      "299614                 0.0                     0.0                       0.0   \n",
      "299629                 1.0                     0.0                       0.0   \n",
      "299630                 0.0                     0.0                       0.0   \n",
      "299645                 0.0                     0.0                       0.0   \n",
      "299654                 0.0                     1.0                       0.0   \n",
      "299655                 0.0                     0.0                       0.0   \n",
      "299685                 0.0                     0.0                       0.0   \n",
      "299695                 0.0                     0.0                       0.0   \n",
      "299715                 0.0                     1.0                       0.0   \n",
      "299728                 0.0                     0.0                       0.0   \n",
      "299734                 0.0                     0.0                       0.0   \n",
      "299736                 0.0                     0.0                       0.0   \n",
      "299751                 0.0                     0.0                       1.0   \n",
      "299765                 0.0                     0.0                       0.0   \n",
      "299767                 0.0                     0.0                       0.0   \n",
      "299800                 0.0                     0.0                       0.0   \n",
      "299806                 1.0                     1.0                       0.0   \n",
      "299826                 0.0                     0.0                       0.0   \n",
      "299828                 0.0                     0.0                       0.0   \n",
      "299832                 0.0                     0.0                       0.0   \n",
      "299853                 0.0                     0.0                       0.0   \n",
      "299863                 0.0                     1.0                       0.0   \n",
      "299883                 0.0                     0.0                       0.0   \n",
      "299888                 0.0                     0.0                       1.0   \n",
      "299913                 0.0                     0.0                       0.0   \n",
      "299914                 0.0                     0.0                       0.0   \n",
      "299917                 0.0                     0.0                       0.0   \n",
      "299995                 0.0                     0.0                       0.0   \n",
      "200012                 0.0                     0.0                       0.0   \n",
      "\n",
      "            pao2fio2Ratio_(333, 475]  pao2fio2Ratio_(475, 3000]  \\\n",
      "icustay_id                                                        \n",
      "219346                           0.0                        1.0   \n",
      "219360                           0.0                        1.0   \n",
      "219373                           1.0                        0.0   \n",
      "219384                           0.0                        1.0   \n",
      "219385                           0.0                        1.0   \n",
      "219391                           0.0                        1.0   \n",
      "219411                           0.0                        1.0   \n",
      "219412                           0.0                        0.0   \n",
      "219441                           0.0                        0.0   \n",
      "219481                           0.0                        1.0   \n",
      "219551                           0.0                        1.0   \n",
      "219612                           0.0                        1.0   \n",
      "219620                           0.0                        1.0   \n",
      "219628                           0.0                        1.0   \n",
      "219630                           0.0                        1.0   \n",
      "219652                           0.0                        1.0   \n",
      "219666                           0.0                        1.0   \n",
      "219674                           0.0                        1.0   \n",
      "219712                           0.0                        1.0   \n",
      "219769                           0.0                        1.0   \n",
      "219794                           0.0                        1.0   \n",
      "219830                           1.0                        0.0   \n",
      "219869                           0.0                        1.0   \n",
      "219917                           0.0                        0.0   \n",
      "219918                           0.0                        0.0   \n",
      "219923                           0.0                        1.0   \n",
      "219943                           0.0                        1.0   \n",
      "219945                           1.0                        0.0   \n",
      "219961                           0.0                        1.0   \n",
      "219987                           0.0                        1.0   \n",
      "...                              ...                        ...   \n",
      "299593                           0.0                        1.0   \n",
      "299614                           0.0                        1.0   \n",
      "299629                           0.0                        1.0   \n",
      "299630                           0.0                        1.0   \n",
      "299645                           0.0                        1.0   \n",
      "299654                           0.0                        0.0   \n",
      "299655                           0.0                        1.0   \n",
      "299685                           0.0                        1.0   \n",
      "299695                           0.0                        1.0   \n",
      "299715                           0.0                        0.0   \n",
      "299728                           0.0                        1.0   \n",
      "299734                           0.0                        1.0   \n",
      "299736                           0.0                        1.0   \n",
      "299751                           0.0                        0.0   \n",
      "299765                           0.0                        1.0   \n",
      "299767                           0.0                        1.0   \n",
      "299800                           0.0                        1.0   \n",
      "299806                           0.0                        0.0   \n",
      "299826                           0.0                        1.0   \n",
      "299828                           0.0                        1.0   \n",
      "299832                           0.0                        1.0   \n",
      "299853                           0.0                        1.0   \n",
      "299863                           0.0                        0.0   \n",
      "299883                           0.0                        1.0   \n",
      "299888                           0.0                        0.0   \n",
      "299913                           0.0                        1.0   \n",
      "299914                           0.0                        1.0   \n",
      "299917                           0.0                        1.0   \n",
      "299995                           0.0                        1.0   \n",
      "200012                           0.0                        1.0   \n",
      "\n",
      "            vent_recieved_False  vent_recieved_True  \n",
      "icustay_id                                           \n",
      "219346                      1.0                 0.0  \n",
      "219360                      1.0                 0.0  \n",
      "219373                      0.0                 1.0  \n",
      "219384                      0.0                 1.0  \n",
      "219385                      1.0                 0.0  \n",
      "219391                      1.0                 0.0  \n",
      "219411                      1.0                 0.0  \n",
      "219412                      0.0                 1.0  \n",
      "219441                      0.0                 1.0  \n",
      "219481                      0.0                 1.0  \n",
      "219551                      1.0                 0.0  \n",
      "219612                      0.0                 1.0  \n",
      "219620                      1.0                 0.0  \n",
      "219628                      0.0                 1.0  \n",
      "219630                      1.0                 0.0  \n",
      "219652                      1.0                 0.0  \n",
      "219666                      1.0                 0.0  \n",
      "219674                      1.0                 0.0  \n",
      "219712                      1.0                 0.0  \n",
      "219769                      1.0                 0.0  \n",
      "219794                      1.0                 0.0  \n",
      "219830                      0.0                 1.0  \n",
      "219869                      1.0                 0.0  \n",
      "219917                      1.0                 0.0  \n",
      "219918                      0.0                 1.0  \n",
      "219923                      1.0                 0.0  \n",
      "219943                      1.0                 0.0  \n",
      "219945                      1.0                 0.0  \n",
      "219961                      0.0                 1.0  \n",
      "219987                      0.0                 1.0  \n",
      "...                         ...                 ...  \n",
      "299593                      1.0                 0.0  \n",
      "299614                      0.0                 1.0  \n",
      "299629                      1.0                 0.0  \n",
      "299630                      0.0                 1.0  \n",
      "299645                      0.0                 1.0  \n",
      "299654                      0.0                 1.0  \n",
      "299655                      1.0                 0.0  \n",
      "299685                      1.0                 0.0  \n",
      "299695                      1.0                 0.0  \n",
      "299715                      0.0                 1.0  \n",
      "299728                      0.0                 1.0  \n",
      "299734                      0.0                 1.0  \n",
      "299736                      0.0                 1.0  \n",
      "299751                      0.0                 1.0  \n",
      "299765                      0.0                 1.0  \n",
      "299767                      1.0                 0.0  \n",
      "299800                      1.0                 0.0  \n",
      "299806                      0.0                 1.0  \n",
      "299826                      0.0                 1.0  \n",
      "299828                      1.0                 0.0  \n",
      "299832                      1.0                 0.0  \n",
      "299853                      0.0                 1.0  \n",
      "299863                      0.0                 1.0  \n",
      "299883                      1.0                 0.0  \n",
      "299888                      1.0                 0.0  \n",
      "299913                      0.0                 1.0  \n",
      "299914                      1.0                 0.0  \n",
      "299917                      1.0                 0.0  \n",
      "299995                      0.0                 1.0  \n",
      "200012                      1.0                 0.0  \n",
      "\n",
      "[4346 rows x 48 columns]\n",
      "time: 54.6 ms\n"
     ]
    }
   ],
   "source": [
    "# #duplicate\n",
    "# # for i, j in zip(test_z_icustay_id, test_index):  #looping through the zipped indicies of the test indicies/test icustay_id\n",
    "\n",
    "# testing_sample_id = i #numerical index of first 1/2 of data ##??? this seems to be instead the \n",
    "# all_xy_0 = all_xy.loc[train_z_icustay_id] # select all TRAINING samples from  the current fold\n",
    "\n",
    "# print(\"I is {}\".format(i))\n",
    "# print(\"testing_sample_id is {}\".format(testing_sample_id))\n",
    "# print(\"all_xy_0 is {}\".format(all_xy_0))\n",
    "\n",
    "# ###unclear what's going on. it seems like we are individually taking the test icustay id index\n",
    "# all_xy_training = all_xy_0.append(all_xy.loc[i]) # append the testing \n",
    "\n",
    "# print(\"all_xy_training is {}\".format(all_xy_training))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['label'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-58e2066e2690>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# print 'start selecting......'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mId_train_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_train_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_sample_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_xy_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_interval\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  individulization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-89897c9f6b89>\u001b[0m in \u001b[0;36mselect_train_samples\u001b[0;34m(sample_id, all_xy, m, time_interval)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mx_candidate_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_xy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the object sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mx_candidate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_candidate_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mx_candidate_tem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_candidate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mtesting_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_candidate_tem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# covert into ....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3414\u001b[0m         return super(Series, self).drop(labels=labels, axis=axis, index=index,\n\u001b[1;32m   3415\u001b[0m                                         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3416\u001b[0;31m                                         inplace=inplace, errors=errors)\n\u001b[0m\u001b[1;32m   3417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3418\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mSubstitution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3111\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3141\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3143\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3144\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   4402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4403\u001b[0m                 raise KeyError(\n\u001b[0;32m-> 4404\u001b[0;31m                     '{} not found in axis'.format(labels[mask]))\n\u001b[0m\u001b[1;32m   4405\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['label'] not found in axis\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 22.5 ms\n"
     ]
    }
   ],
   "source": [
    "# ###important parameter. was at 400, i changed to X\n",
    "# m = 250  # m is the number of similar cases or similar controls\n",
    "\n",
    "# X_test_00 = x[j]\n",
    "# y_test = y[j]\n",
    "\n",
    "# X_test = X_test_00.reshape(1, -1)\n",
    "\n",
    "# # print 'start selecting......'\n",
    "\n",
    "# ##good now have all these parameters for testing this \n",
    "# Id_train_set = select_train_samples(testing_sample_id, all_xy_training, m, time_interval)  #  individulization  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ###important parameter. was at 400, i changed to X\n",
    "    m = 250  # m is the number of similar cases or similar controls\n",
    "\n",
    "    X_test_00 = x[j]\n",
    "    y_test = y[j]\n",
    "\n",
    "    X_test = X_test_00.reshape(1, -1)\n",
    "\n",
    "    # print 'start selecting......'\n",
    "\n",
    "    Id_train_set = select_train_samples(testing_sample_id, all_xy_training, m, time_interval)  #  individulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "1001\n",
      "1002\n",
      "1003\n",
      "1004\n",
      "1005\n",
      "1006\n",
      "1007\n",
      "1008\n",
      "1009\n",
      "1010\n",
      "1011\n",
      "1012\n",
      "1013\n",
      "1014\n",
      "1015\n",
      "1016\n",
      "1017\n",
      "1018\n",
      "1019\n",
      "1020\n",
      "1021\n",
      "1022\n",
      "1023\n",
      "1024\n",
      "1025\n",
      "1026\n",
      "1027\n",
      "1028\n",
      "1029\n",
      "1030\n",
      "1031\n",
      "1032\n",
      "1033\n",
      "1034\n",
      "1035\n",
      "1036\n",
      "1037\n",
      "1038\n",
      "1039\n",
      "1040\n",
      "1041\n",
      "1042\n",
      "1043\n",
      "1044\n",
      "1045\n",
      "1046\n",
      "1047\n",
      "1048\n",
      "1049\n",
      "1050\n",
      "1051\n",
      "1052\n",
      "1053\n",
      "1054\n",
      "1055\n",
      "1056\n",
      "1057\n",
      "1058\n",
      "1059\n",
      "1060\n",
      "1061\n",
      "1062\n",
      "1063\n",
      "1064\n",
      "1065\n",
      "1066\n",
      "1067\n",
      "1068\n",
      "1069\n",
      "1070\n",
      "1071\n",
      "1072\n",
      "1073\n",
      "1074\n",
      "1075\n",
      "1076\n",
      "1077\n",
      "1078\n",
      "1079\n",
      "1080\n",
      "1081\n",
      "1082\n",
      "1083\n",
      "1084\n",
      "1085\n",
      "time: 154 ms\n"
     ]
    }
   ],
   "source": [
    "for i, j in zip(train_index_tester, test_index_tester):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 26.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Generate indices to split data into training and test set.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "X : array-like, shape (n_samples, n_features)\n",
       "    Training data, where n_samples is the number of samples\n",
       "    and n_features is the number of features.\n",
       "\n",
       "    Note that providing ``y`` is sufficient to generate the splits and\n",
       "    hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
       "    ``X`` instead of actual training data.\n",
       "\n",
       "y : array-like, shape (n_samples,)\n",
       "    The target variable for supervised learning problems.\n",
       "    Stratification is done based on the y labels.\n",
       "\n",
       "groups : object\n",
       "    Always ignored, exists for compatibility.\n",
       "\n",
       "Yields\n",
       "------\n",
       "train : ndarray\n",
       "    The training set indices for that split.\n",
       "\n",
       "test : ndarray\n",
       "    The testing set indices for that split.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Randomized CV splitters may return different results for each call of\n",
       "split. You can make the results identical by setting ``random_state``\n",
       "to an integer.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/model_selection/_split.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?skf.split(x, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#his unannotated code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('%%%%%')? (<ipython-input-58-348e33b4eb37>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-348e33b4eb37>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    print '%%%%%'\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('%%%%%')?\n"
     ]
    }
   ],
   "source": [
    "# # if __name__ == '__main__': #basically execute only if run as a script. i will unravel this so i can run it inline here\n",
    "\n",
    "#ge:reading in a file when running as script\n",
    "\n",
    "# path = './logs/individualization_24_1th.txt'\n",
    "# f = open(path, 'a+')\n",
    "# orig_stdout = sys.stdout\n",
    "# sys.stdout = f\n",
    "\n",
    "\n",
    "\n",
    "for time_interval in [24]:  # ,48,72,96,120,144]:\n",
    "    x, y, z_icustay_id, all_xy = preprocessing(folder, time_interval)  # all_xy is for compute gower distance\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    print '%%%%%'\n",
    "    num_fold = 0\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        print '***************'\n",
    "        # print 'This is the '+ str(i)+' times result of '+str(n_fold)+' fold'\n",
    "        X_train_0, X_test_0 = x[train_index], x[test_index]\n",
    "        y_train_0, y_test_0 = y[train_index], y[test_index]\n",
    "\n",
    "        print '#####################'\n",
    "\n",
    "        num_fold = num_fold + 1\n",
    "        print 'this is the results of the %d fold in 5 folds:' %num_fold\n",
    "\n",
    "        print 'the number of testing samples in this fold:', test_index.size\n",
    "\n",
    "        train_z_icustay_id = z_icustay_id[train_index] # the icustay_id of samples in training set from 5 fold\n",
    "        test_z_icustay_id = z_icustay_id[test_index] # the icustay_id of samples in testing set from 5 fold\n",
    "\n",
    "        xg_one_fold_pred = [] # obtain the pred label of testing samples for one fold using xgboost\n",
    "        xg_one_fold_proba = [] # obtain the proba  of testing samples for one fold using xgboost\n",
    "\n",
    "        lr_one_fold_pred = [] # obtain the pred label of testing samples for one fold using lr\n",
    "        lr_one_fold_proba = [] # obtain the proba  of testing samples for one fold using lr\n",
    "\n",
    "        indicator_time = 0 # the indicator\n",
    "        for i, j in zip(test_z_icustay_id, test_index):\n",
    "            # i_index = np.where(test_z_icustay_id == i)\n",
    "            # tem_test_z_icustay_id = np.delete(test_z_icustay_id, i_index)\n",
    "            testing_sample_id = i\n",
    "\n",
    "            all_xy_0 = all_xy.loc[train_z_icustay_id] # select training samples from  5 fold\n",
    "            all_xy_training = all_xy_0.append(all_xy.loc[i]) # note that , containing the i\n",
    "\n",
    "            m = 400  # m is the number of similar cases or similar controls\n",
    "\n",
    "            X_test_00 = x[j]\n",
    "            y_test = y[j]\n",
    "\n",
    "            X_test = X_test_00.reshape(1, -1)\n",
    "\n",
    "            # print 'start selecting......'\n",
    "\n",
    "            Id_train_set = select_train_samples(testing_sample_id, all_xy_training, m, time_interval)  #  individulization\n",
    "\n",
    "            ix = np.isin(z_icustay_id, Id_train_set)\n",
    "            Id_train_set_index = list(np.where(ix))\n",
    "\n",
    "            # Id_train_set_index = np.argwhere(z_icustay_id == Id_train_set)\n",
    "\n",
    "            X_train = x[Id_train_set_index]\n",
    "            y_train = y[Id_train_set_index]\n",
    "\n",
    "            # print 'start training......'\n",
    "\n",
    "            # scoring = 'roc_auc'\n",
    "\n",
    "# xgboost\n",
    "\n",
    "            xgboost_mod = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "                          min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                          objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "            xgboost_mod.fit(X_train, y_train)\n",
    "            xg_y_pred = xgboost_mod.predict(X_test)\n",
    "            xg_y_pred_proba = xgboost_mod.predict_proba(X_test)[:,1]\n",
    "\n",
    "            xg_one_fold_pred.append(xg_y_pred)\n",
    "            xg_one_fold_proba.append(xg_y_pred_proba)\n",
    "\n",
    "# lr \n",
    "\n",
    "            logreg = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                                        intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "            logreg.fit(X_train, y_train)\n",
    "            lr_y_pred = logreg.predict(X_test)\n",
    "            lr_y_pred_proba = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "            lr_one_fold_pred.append(lr_y_pred)\n",
    "            lr_one_fold_proba.append(lr_y_pred_proba)\n",
    "\n",
    "            indicator_time = indicator_time + 1\n",
    "            # print 'the next testing sample and total samples:', indicator_time, test_index.size\n",
    "\n",
    "        xg_y_individual_pred = np.array(xg_one_fold_pred)\n",
    "        xg_y_individual_proba = np.array(xg_one_fold_proba)\n",
    "\n",
    "        lr_y_individual_pred = np.array(lr_one_fold_pred)\n",
    "        lr_y_individual_proba = np.array(lr_one_fold_proba)\n",
    "\n",
    "        one_fold_y_test = y[test_index]\n",
    "\n",
    "        print 'this is the result of individual predictor using xgboost:'\n",
    "        print 'the acc of one fold:', accuracy_score(one_fold_y_test, xg_y_individual_pred)\n",
    "        print 'the classification_report :', classification_report(one_fold_y_test, xg_y_individual_pred)\n",
    "        print 'the auc of one fold:', roc_auc_score(one_fold_y_test, xg_y_individual_proba)\n",
    "\n",
    "        print 'this is the result of individual predictor using lr:'\n",
    "        print 'the acc of one fold:', accuracy_score(one_fold_y_test, lr_y_individual_pred)\n",
    "        print 'the classification_report :', classification_report(one_fold_y_test, lr_y_individual_pred)\n",
    "        print 'the auc of one fold:', roc_auc_score(one_fold_y_test, lr_y_individual_pred)\n",
    "\n",
    "# using non-individual predictor for classification\n",
    "\n",
    "        xgboost_random = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "                                    min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                    objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "        xgboost_random.fit(X_train_0, y_train_0)\n",
    "        y_pred_random = xgboost_random.predict(X_test_0)\n",
    "        y_proba_random = xgboost_random.predict_proba(X_test_0)[:,1]\n",
    "\n",
    "        y_test_random = y[test_index]\n",
    "\n",
    "        print 'this is the result of non-individual predictor using xgboost:'\n",
    "        print 'the acc is:',accuracy_score(y_test_random, y_pred_random)\n",
    "        print 'the classification_report:', classification_report(y_test_random, y_pred_random)\n",
    "        print 'the auc is:', roc_auc_score(y_test_random, y_proba_random)\n",
    "\n",
    "        logreg_random = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                                    intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "        logreg_random.fit(X_train_0, y_train_0)\n",
    "        lr_y_pred_random = logreg_random.predict(X_test_0)\n",
    "        lr_y_pred_proba_random = logreg_random.predict_proba(X_test_0)[:, 1]\n",
    "\n",
    "        print 'this is the result of non-individual predictor using lr:'\n",
    "        print 'the acc is:',accuracy_score(y_test_random, lr_y_pred_random)\n",
    "        print 'the classification_report:', classification_report(y_test_random, lr_y_pred_random)\n",
    "        print 'the auc is:', roc_auc_score(y_test_random, lr_y_pred_proba_random)\n",
    "\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == '__main__': #basically execute only if run as a script. i will undo this\n",
    "#     path = './logs/individualization_24_1th.txt'\n",
    "#     f = open(path, 'a+')\n",
    "#     orig_stdout = sys.stdout\n",
    "#     sys.stdout = f\n",
    "#     for time_interval in [24]:  # ,48,72,96,120,144]:\n",
    "#         x, y, z_icustay_id, all_xy = preprocessing(folder, time_interval)  # all_xy is for compute gower distance\n",
    "\n",
    "#         skf = StratifiedKFold(n_splits=5)\n",
    "#         print '%%%%%'\n",
    "#         num_fold = 0\n",
    "#         for train_index, test_index in skf.split(x, y):\n",
    "#             print '***************'\n",
    "#             # print 'This is the '+ str(i)+' times result of '+str(n_fold)+' fold'\n",
    "#             X_train_0, X_test_0 = x[train_index], x[test_index]\n",
    "#             y_train_0, y_test_0 = y[train_index], y[test_index]\n",
    "\n",
    "#             print '#####################'\n",
    "\n",
    "#             num_fold = num_fold + 1\n",
    "#             print 'this is the results of the %d fold in 5 folds:' %num_fold\n",
    "\n",
    "#             print 'the number of testing samples in this fold:', test_index.size\n",
    "\n",
    "#             train_z_icustay_id = z_icustay_id[train_index] # the icustay_id of samples in training set from 5 fold\n",
    "#             test_z_icustay_id = z_icustay_id[test_index] # the icustay_id of samples in testing set from 5 fold\n",
    "\n",
    "#             xg_one_fold_pred = [] # obtain the pred label of testing samples for one fold using xgboost\n",
    "#             xg_one_fold_proba = [] # obtain the proba  of testing samples for one fold using xgboost\n",
    "\n",
    "#             lr_one_fold_pred = [] # obtain the pred label of testing samples for one fold using lr\n",
    "#             lr_one_fold_proba = [] # obtain the proba  of testing samples for one fold using lr\n",
    "\n",
    "#             indicator_time = 0 # the indicator\n",
    "#             for i, j in zip(test_z_icustay_id, test_index):\n",
    "#                 # i_index = np.where(test_z_icustay_id == i)\n",
    "#                 # tem_test_z_icustay_id = np.delete(test_z_icustay_id, i_index)\n",
    "#                 testing_sample_id = i\n",
    "\n",
    "#                 all_xy_0 = all_xy.loc[train_z_icustay_id] # select training samples from  5 fold\n",
    "#                 all_xy_training = all_xy_0.append(all_xy.loc[i]) # note that , containing the i\n",
    "\n",
    "#                 m = 400  # m is the number of similar cases or similar controls\n",
    "\n",
    "#                 X_test_00 = x[j]\n",
    "#                 y_test = y[j]\n",
    "\n",
    "#                 X_test = X_test_00.reshape(1, -1)\n",
    "\n",
    "#                 # print 'start selecting......'\n",
    "\n",
    "#                 Id_train_set = select_train_samples(testing_sample_id, all_xy_training, m, time_interval)  #  individulization\n",
    "\n",
    "#                 ix = np.isin(z_icustay_id, Id_train_set)\n",
    "#                 Id_train_set_index = list(np.where(ix))\n",
    "\n",
    "#                 # Id_train_set_index = np.argwhere(z_icustay_id == Id_train_set)\n",
    "\n",
    "#                 X_train = x[Id_train_set_index]\n",
    "#                 y_train = y[Id_train_set_index]\n",
    "\n",
    "#                 # print 'start training......'\n",
    "\n",
    "#                 # scoring = 'roc_auc'\n",
    "\n",
    "# # xgboost\n",
    "\n",
    "#                 xgboost_mod = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "#                               min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "#                               objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "#                 xgboost_mod.fit(X_train, y_train)\n",
    "#                 xg_y_pred = xgboost_mod.predict(X_test)\n",
    "#                 xg_y_pred_proba = xgboost_mod.predict_proba(X_test)[:,1]\n",
    "\n",
    "#                 xg_one_fold_pred.append(xg_y_pred)\n",
    "#                 xg_one_fold_proba.append(xg_y_pred_proba)\n",
    "\n",
    "# # lr \n",
    "\n",
    "#                 logreg = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "#                                             intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "#                 logreg.fit(X_train, y_train)\n",
    "#                 lr_y_pred = logreg.predict(X_test)\n",
    "#                 lr_y_pred_proba = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "#                 lr_one_fold_pred.append(lr_y_pred)\n",
    "#                 lr_one_fold_proba.append(lr_y_pred_proba)\n",
    "\n",
    "#                 indicator_time = indicator_time + 1\n",
    "#                 # print 'the next testing sample and total samples:', indicator_time, test_index.size\n",
    "\n",
    "#             xg_y_individual_pred = np.array(xg_one_fold_pred)\n",
    "#             xg_y_individual_proba = np.array(xg_one_fold_proba)\n",
    "\n",
    "#             lr_y_individual_pred = np.array(lr_one_fold_pred)\n",
    "#             lr_y_individual_proba = np.array(lr_one_fold_proba)\n",
    "\n",
    "#             one_fold_y_test = y[test_index]\n",
    "\n",
    "#             print 'this is the result of individual predictor using xgboost:'\n",
    "#             print 'the acc of one fold:', accuracy_score(one_fold_y_test, xg_y_individual_pred)\n",
    "#             print 'the classification_report :', classification_report(one_fold_y_test, xg_y_individual_pred)\n",
    "#             print 'the auc of one fold:', roc_auc_score(one_fold_y_test, xg_y_individual_proba)\n",
    "\n",
    "#             print 'this is the result of individual predictor using lr:'\n",
    "#             print 'the acc of one fold:', accuracy_score(one_fold_y_test, lr_y_individual_pred)\n",
    "#             print 'the classification_report :', classification_report(one_fold_y_test, lr_y_individual_pred)\n",
    "#             print 'the auc of one fold:', roc_auc_score(one_fold_y_test, lr_y_individual_pred)\n",
    "\n",
    "# # using non-individual predictor for classification\n",
    "\n",
    "#             xgboost_random = XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=5,\n",
    "#                                         min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "#                                         objective='binary:logistic', nthread=4, scale_pos_weight=1, seed=27)\n",
    "#             xgboost_random.fit(X_train_0, y_train_0)\n",
    "#             y_pred_random = xgboost_random.predict(X_test_0)\n",
    "#             y_proba_random = xgboost_random.predict_proba(X_test_0)[:,1]\n",
    "\n",
    "#             y_test_random = y[test_index]\n",
    "\n",
    "#             print 'this is the result of non-individual predictor using xgboost:'\n",
    "#             print 'the acc is:',accuracy_score(y_test_random, y_pred_random)\n",
    "#             print 'the classification_report:', classification_report(y_test_random, y_pred_random)\n",
    "#             print 'the auc is:', roc_auc_score(y_test_random, y_proba_random)\n",
    "\n",
    "#             logreg_random = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "#                                         intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "#             logreg_random.fit(X_train_0, y_train_0)\n",
    "#             lr_y_pred_random = logreg_random.predict(X_test_0)\n",
    "#             lr_y_pred_proba_random = logreg_random.predict_proba(X_test_0)[:, 1]\n",
    "\n",
    "#             print 'this is the result of non-individual predictor using lr:'\n",
    "#             print 'the acc is:',accuracy_score(y_test_random, lr_y_pred_random)\n",
    "#             print 'the classification_report:', classification_report(y_test_random, lr_y_pred_random)\n",
    "#             print 'the auc is:', roc_auc_score(y_test_random, lr_y_pred_proba_random)\n",
    "\n",
    "#             # break\n",
    "#     sys.stdout = orig_stdout\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to have a robust evaluation of model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to try cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
