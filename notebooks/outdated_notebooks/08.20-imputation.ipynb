{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# description 6/03/19 clinically guided aggregation modeling w/ 2 elix variables\n",
    "\n",
    "sklearn modeling using local methods of the median imputed training data using origional min/max clinically guided aggregation. note the preprocessing of data from 07.20-worst_case_model was performed in R (09.newagg2_preprocessing_med_impute.rmd). this eventually will be converted over to python, but for now works in r. \n",
    "\n",
    "preprocessing includes variable formatting (categorical to factor variables in r, train/test split, and median imputation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 60.9 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, auc, precision_recall_fscore_support, pairwise, f1_score, log_loss, make_scorer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, Imputer\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import validation\n",
    "from scipy.sparse import issparse\n",
    "from scipy.spatial import distance\n",
    "from sklearn import svm\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#importin xg boost and all needed otherstuff\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier #conda install -c conda-forge xgboost to install\n",
    "##adding these, lets see if it helps with xgboost crash\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "#reducing warnings that are super common in my model\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.simplefilter(action='ignore') #ignore all warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "# warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "\n",
    "memory = Memory(cachedir='/tmp', verbose=0)\n",
    "#@memory.cache above any def fxn.\n",
    "\n",
    "RANDOM_STATE = 15485867\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1024,\n",
    "        'height': 768,\n",
    "        'scroll': True,\n",
    "})\n",
    "\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables used to saving models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 840 Âµs\n"
     ]
    }
   ],
   "source": [
    "date=\"11062019\"#'03062019'\n",
    "dataset=\"clinagg\"\n",
    "timewindowdays=\"72\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 93.5 ms\n"
     ]
    }
   ],
   "source": [
    "#cohort import\n",
    "\n",
    "os.chdir('/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling') #use to change working directory\n",
    "wd= os.getcwd() #'/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling'\n",
    "\n",
    "\n",
    "final_pt_df2 = pd.read_csv(Path(wd + '/data/raw/csv/04042019_final_pt_df2_v.csv') , index_col=0) #only for patients with minimum vitals, 14478 icustay_id\n",
    "patients= list(final_pt_df2['subject_id'].unique())\n",
    "hadm_id= list(final_pt_df2['hadm_id'].unique())\n",
    "icustay_id= list(final_pt_df2['icustay_id'].unique())\n",
    "icustay_id= [int(x) for x in icustay_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 111 ms\n"
     ]
    }
   ],
   "source": [
    "#importing in all clinical_variable files\n",
    "timewindowdays=\"72\"\n",
    "# date= '04042019'\n",
    "date='11062019'\n",
    "os.chdir(r'/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling/data/processed/')\n",
    "preimp_train_df= pd.read_csv(Path(wd+'/data/processed/merged/{}_worst_df_train_preImp_{}.csv'.format(date,timewindowdays),  index_col=0))\n",
    "preimp_test_df= pd.read_csv(Path(wd+'/data/processed/merged/{}_worst_df_test_preImp_{}.csv'.format(date,timewindowdays),  index_col=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 103 ms\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(data):\n",
    "\n",
    "    \"\"\"\n",
    "    1) rename columns\n",
    "    2) standardize last 2 columns to be standardized\n",
    "    3) convert categorical columns to proper format\n",
    "    4) median impute\n",
    "    \"\"\"\n",
    "    from sklearn.impute import SimpleImputer\n",
    "        \n",
    "    rename_dic={\n",
    "    \"('max', 'sodium')\": \"maxSodium\" ,\n",
    "    \"('max', 'sodium')\" : \"maxSodium\",\n",
    "    \"('min', 'sodium')\" : \"minSodium\",\n",
    "    \"('max', 'calcium')\" : \"maxCalcium\",\n",
    "    \"('min', 'calcium')\" : \"minCalcium\",\n",
    "    \"('max', 'sodium')\": \"maxSodium\",\n",
    "    \"('min', 'sodium')\": \"minSodium\",\n",
    "    \"('max', 'wbc')\": \"maxWBC\",\n",
    "    \"('min', 'wbc')\": \"minWBC\",\n",
    "    \"bands\": \"ibands\",\n",
    "    \"pco2\": \"ipco2\"\n",
    "        }\n",
    "    preimp_df=preimp_df.rename(rename_dic, axis='columns')\n",
    "    preimp_df.loc[preimp_df['first_admit_age']>90,\"first_admit_age\"]=90\n",
    "    \n",
    "    \n",
    "    train_data=data.copy()\n",
    "    weight_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).median()\n",
    "    weight_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.25)#.between(train_data['col'].quantile(.25), df['col'].quantile(.75), inclusive=True)]\n",
    "    weight_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.75)\n",
    "    weight_iqr=weight_quant3-weight_quant1\n",
    "    #print(weight_median,weight_quant3,weight_quant1, weight_iqr)\n",
    "\n",
    "    age_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).median()\n",
    "    age_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.25)\n",
    "    age_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.75)\n",
    "    age_iqr=age_quant3-age_quant1\n",
    "\n",
    "    #converting to log scaled standardized data for age/weight\n",
    "    train_data['weight']=train_data['weight'].apply(lambda x: (np.log(x+1)-weight_median)/weight_iqr)\n",
    "    train_data['first_admit_age']=train_data['first_admit_age'].apply(lambda x: (np.log(x+1)-age_median)/age_iqr)\n",
    "    \n",
    "    ### onehot encoding categorical var\n",
    "    cols_to_transform=['ethnicity', 'ibands', 'ipco2',\n",
    "                       'any_vasoactive',\"leukocyte\",\"nitrite\",\n",
    "                       'pao2fio2ratio', 'vent_recieved',  \"dobutamine\",\n",
    "                       \"dopamine\",\"epinephrine\",\"norepinephrine\",\n",
    "                       \"phenylephrine\",\"rrt\",\"vasopressin\",'cancer_elix' ]\n",
    "    train_data = pd.get_dummies(train_data, columns = cols_to_transform, drop_first=True)\n",
    "    \n",
    "    \n",
    "    #binarizing and poping outcome for training data\n",
    "    train_data.loc[train_data['final_bin']==\"C_pos/A_full\",\"final_bin\"]=1\n",
    "    train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"final_bin\"]=0\n",
    "    train_data['final_bin']=pd.to_numeric(train_data['final_bin'])\n",
    "    \n",
    "    \n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    train_data=pd.DataFrame(imp.fit_transform(train_data), columns=list(train_data))\n",
    "\n",
    "    ## establishing training data and labels\n",
    "    x_train= train_data.copy()\n",
    "    z_icustay_id=x_train.pop('icustay_id')\n",
    "    y_train= x_train.pop(\"final_bin\").values\n",
    "    \n",
    "    return(x_train, y_train, z_icustay_id)\n",
    "\n",
    "x_train, y_train, z_icustay_id= preprocessing(pd.merge(preimp_train_df, final_pt_df2[['icustay_id','final_bin']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, y_train, z_icustay_id= preprocessing(pd.merge(preimp_train_df, final_pt_df2[['icustay_id','final_bin']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bilirubin</th>\n",
       "      <th>bun</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>daily_sofa</th>\n",
       "      <th>glucose</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>inr</th>\n",
       "      <th>lactate</th>\n",
       "      <th>o2_flow</th>\n",
       "      <th>...</th>\n",
       "      <th>vent_recieved_1.0</th>\n",
       "      <th>vent_recieved_2.0</th>\n",
       "      <th>dobutamine_1.0</th>\n",
       "      <th>dopamine_1.0</th>\n",
       "      <th>epinephrine_1.0</th>\n",
       "      <th>norepinephrine_1.0</th>\n",
       "      <th>phenylephrine_1.0</th>\n",
       "      <th>rrt_1.0</th>\n",
       "      <th>vasopressin_1.0</th>\n",
       "      <th>cancer_elix_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.167681</td>\n",
       "      <td>0.104422</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.185525</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.263315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.231986</td>\n",
       "      <td>0.122666</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>-0.043109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136269</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.270437</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.352864</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204831</td>\n",
       "      <td>0.068747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311484</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.780201</td>\n",
       "      <td>0.347709</td>\n",
       "      <td>0.045738</td>\n",
       "      <td>0.085914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.828793</td>\n",
       "      <td>-0.240028</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>-0.347655</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>-0.042022</td>\n",
       "      <td>0.068747</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>-0.183151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>-0.013067</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>-0.026489</td>\n",
       "      <td>0.120179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.922516</td>\n",
       "      <td>0.125998</td>\n",
       "      <td>0.033114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.110384</td>\n",
       "      <td>0.034613</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.296820</td>\n",
       "      <td>0.148804</td>\n",
       "      <td>1.412966</td>\n",
       "      <td>-0.597678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.054435</td>\n",
       "      <td>0.146224</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169502</td>\n",
       "      <td>0.110028</td>\n",
       "      <td>0.247811</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.049870</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.061299</td>\n",
       "      <td>0.125134</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.608593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.181044</td>\n",
       "      <td>0.016858</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045738</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.233925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.181044</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>-0.861353</td>\n",
       "      <td>-0.011875</td>\n",
       "      <td>0.010789</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.579273</td>\n",
       "      <td>0.246564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.105387</td>\n",
       "      <td>0.099538</td>\n",
       "      <td>2.481030</td>\n",
       "      <td>0.157802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.178021</td>\n",
       "      <td>0.224244</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>0.780201</td>\n",
       "      <td>0.821253</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>1.927931</td>\n",
       "      <td>0.833285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>0.033114</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.861353</td>\n",
       "      <td>0.107898</td>\n",
       "      <td>0.068747</td>\n",
       "      <td>-0.415374</td>\n",
       "      <td>0.120348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104422</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.059787</td>\n",
       "      <td>-0.022756</td>\n",
       "      <td>-0.270369</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.183560</td>\n",
       "      <td>-0.155149</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.111617</td>\n",
       "      <td>0.074574</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.536462</td>\n",
       "      <td>0.016858</td>\n",
       "      <td>-0.347655</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.023925</td>\n",
       "      <td>0.053689</td>\n",
       "      <td>-0.415374</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.155149</td>\n",
       "      <td>0.033114</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.070187</td>\n",
       "      <td>0.083117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.183151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043199</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032544</td>\n",
       "      <td>0.179208</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>-0.134537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104422</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.347655</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.213132</td>\n",
       "      <td>0.189392</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.120348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.031093</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>-0.015963</td>\n",
       "      <td>0.083117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.155149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.347655</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.118893</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>0.688918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.118583</td>\n",
       "      <td>0.231856</td>\n",
       "      <td>-0.021991</td>\n",
       "      <td>1.685438</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.152368</td>\n",
       "      <td>-0.034783</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.118583</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.037575</td>\n",
       "      <td>0.021221</td>\n",
       "      <td>-0.270369</td>\n",
       "      <td>-0.233925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.464272</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>-0.347655</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.192721</td>\n",
       "      <td>0.102193</td>\n",
       "      <td>-0.415374</td>\n",
       "      <td>0.194066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.253022</td>\n",
       "      <td>0.660966</td>\n",
       "      <td>0.033114</td>\n",
       "      <td>2.235843</td>\n",
       "      <td>0.569323</td>\n",
       "      <td>0.022164</td>\n",
       "      <td>0.037873</td>\n",
       "      <td>1.994721</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.253022</td>\n",
       "      <td>0.081304</td>\n",
       "      <td>-0.008669</td>\n",
       "      <td>0.217747</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.131699</td>\n",
       "      <td>0.141888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.233925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183238</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.286851</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>-0.233925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5286</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408760</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.070187</td>\n",
       "      <td>0.031318</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5287</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174374</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.467559</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.125960</td>\n",
       "      <td>0.074574</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>-0.183151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5288</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125998</td>\n",
       "      <td>-0.008669</td>\n",
       "      <td>1.929371</td>\n",
       "      <td>0.503859</td>\n",
       "      <td>0.127118</td>\n",
       "      <td>0.062810</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>-0.087908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5289</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041030</td>\n",
       "      <td>0.217747</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.035909</td>\n",
       "      <td>0.091435</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5290</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.209194</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>-0.448205</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.199714</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>0.157802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5291</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.260632</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>0.631709</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.382112</td>\n",
       "      <td>0.122666</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.531559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5292</th>\n",
       "      <td>-0.327633</td>\n",
       "      <td>0.456100</td>\n",
       "      <td>-0.008669</td>\n",
       "      <td>2.325925</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.258499</td>\n",
       "      <td>-0.011172</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>-0.087908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5293</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043199</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040872</td>\n",
       "      <td>0.151076</td>\n",
       "      <td>-0.270369</td>\n",
       "      <td>0.194066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>-0.253022</td>\n",
       "      <td>0.411824</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>1.787638</td>\n",
       "      <td>0.899021</td>\n",
       "      <td>0.204831</td>\n",
       "      <td>0.246092</td>\n",
       "      <td>1.492224</td>\n",
       "      <td>1.703836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.529792</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.871874</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.137308</td>\n",
       "      <td>-0.030723</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>-0.031093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047339</td>\n",
       "      <td>0.096860</td>\n",
       "      <td>0.247811</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.209194</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.011330</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>-0.401383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253675</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.075939</td>\n",
       "      <td>0.041101</td>\n",
       "      <td>-0.270369</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5299</th>\n",
       "      <td>-0.057547</td>\n",
       "      <td>0.305548</td>\n",
       "      <td>0.056459</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.778385</td>\n",
       "      <td>0.251609</td>\n",
       "      <td>0.223545</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.448951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300</th>\n",
       "      <td>1.489125</td>\n",
       "      <td>0.200269</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.778385</td>\n",
       "      <td>0.092434</td>\n",
       "      <td>0.193375</td>\n",
       "      <td>2.481030</td>\n",
       "      <td>1.540379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5301</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>-0.164150</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.091101</td>\n",
       "      <td>0.014305</td>\n",
       "      <td>0.247811</td>\n",
       "      <td>0.583488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5302</th>\n",
       "      <td>1.501789</td>\n",
       "      <td>-0.155149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078766</td>\n",
       "      <td>0.170817</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>1.349563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311484</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>0.347709</td>\n",
       "      <td>0.070187</td>\n",
       "      <td>0.130013</td>\n",
       "      <td>-0.415374</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5304</th>\n",
       "      <td>-0.253022</td>\n",
       "      <td>0.043199</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053641</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5305</th>\n",
       "      <td>1.934870</td>\n",
       "      <td>0.015050</td>\n",
       "      <td>-0.026519</td>\n",
       "      <td>0.076014</td>\n",
       "      <td>0.628543</td>\n",
       "      <td>0.197114</td>\n",
       "      <td>-0.003680</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>-0.043109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5306</th>\n",
       "      <td>0.610254</td>\n",
       "      <td>0.274114</td>\n",
       "      <td>0.037089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.080167</td>\n",
       "      <td>0.085914</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5307</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.360513</td>\n",
       "      <td>0.067697</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.068727</td>\n",
       "      <td>0.056759</td>\n",
       "      <td>0.247811</td>\n",
       "      <td>0.608593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5308</th>\n",
       "      <td>-0.327633</td>\n",
       "      <td>-0.108854</td>\n",
       "      <td>-0.004314</td>\n",
       "      <td>0.217747</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.009478</td>\n",
       "      <td>-0.060310</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5309</th>\n",
       "      <td>0.547309</td>\n",
       "      <td>0.200269</td>\n",
       "      <td>0.041030</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.628543</td>\n",
       "      <td>0.169502</td>\n",
       "      <td>0.141888</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.328603</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5310</th>\n",
       "      <td>-0.183560</td>\n",
       "      <td>0.403080</td>\n",
       "      <td>0.048810</td>\n",
       "      <td>1.883544</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.445095</td>\n",
       "      <td>-0.078556</td>\n",
       "      <td>0.364376</td>\n",
       "      <td>0.157802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5311</th>\n",
       "      <td>0.330736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.284055</td>\n",
       "      <td>-0.178747</td>\n",
       "      <td>0.018606</td>\n",
       "      <td>0.172936</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>-0.118583</td>\n",
       "      <td>0.484876</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.631709</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.009478</td>\n",
       "      <td>0.112596</td>\n",
       "      <td>1.161660</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5313</th>\n",
       "      <td>-0.183560</td>\n",
       "      <td>0.350242</td>\n",
       "      <td>-0.049864</td>\n",
       "      <td>2.641747</td>\n",
       "      <td>0.138647</td>\n",
       "      <td>0.104122</td>\n",
       "      <td>0.047458</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>0.008507</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.158551</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>-0.270369</td>\n",
       "      <td>-0.401383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>-0.327633</td>\n",
       "      <td>0.253675</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>2.140231</td>\n",
       "      <td>0.347709</td>\n",
       "      <td>-0.018033</td>\n",
       "      <td>0.094159</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5316 rows Ã 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bilirubin       bun  chloride  creatinine  daily_sofa   glucose  \\\n",
       "0      2.167681  0.104422  0.025059    0.000000    0.251930  0.185525   \n",
       "1      0.000000 -0.068362  0.012702   -0.253202   -0.430677  0.231986   \n",
       "2      0.000000  0.136269  0.020977    0.000000   -0.430677  0.270437   \n",
       "3      1.352864 -0.087975  0.004273   -0.079914    0.000000  0.204831   \n",
       "4      0.000000  0.311484  0.012702    0.780201    0.347709  0.045738   \n",
       "5      0.828793 -0.240028  0.020977   -0.347655    0.138647 -0.042022   \n",
       "6      0.000000  0.015050 -0.013067    0.148492    0.138647 -0.026489   \n",
       "7      0.922516  0.125998  0.033114    0.000000    0.251930  0.110384   \n",
       "8      0.000000  0.104422  0.000000   -0.164150   -0.178747  0.296820   \n",
       "9      0.054435  0.146224 -0.004314    0.148492    0.000000  0.169502   \n",
       "10     0.000000 -0.049870  0.020977   -0.253202    0.138647  0.061299   \n",
       "11     0.000000 -0.181044  0.016858   -0.164150    0.000000  0.045738   \n",
       "12     0.000000 -0.181044 -0.004314   -0.079914   -0.861353 -0.011875   \n",
       "13     0.579273  0.246564  0.000000    0.284055    0.138647  0.105387   \n",
       "14     1.178021  0.224244  0.008507    0.780201    0.821253  0.011330   \n",
       "15     0.000000 -0.087975  0.033114   -0.253202   -0.861353  0.107898   \n",
       "16     0.000000  0.104422  0.037089   -0.253202    0.138647  0.059787   \n",
       "17    -0.183560 -0.155149 -0.004314   -0.253202   -0.430677  0.111617   \n",
       "18     0.000000 -0.536462  0.016858   -0.347655    0.138647  0.023925   \n",
       "19     0.000000 -0.155149  0.033114   -0.164150   -0.178747  0.070187   \n",
       "20     0.000000  0.043199  0.037089   -0.079914    0.000000  0.032544   \n",
       "21     0.000000  0.104422  0.012702    0.347655   -0.430677  0.213132   \n",
       "22     0.000000  0.000000 -0.031093   -0.253202   -0.430677 -0.015963   \n",
       "23     0.000000 -0.155149  0.000000   -0.347655    0.251930  0.118893   \n",
       "24    -0.118583  0.231856 -0.021991    1.685438    0.251930  0.152368   \n",
       "25    -0.118583 -0.068362  0.000000    0.076014   -0.430677  0.037575   \n",
       "26     0.000000 -0.464272  0.025059   -0.347655   -0.430677  0.192721   \n",
       "27    -0.253022  0.660966  0.033114    2.235843    0.569323  0.022164   \n",
       "28    -0.253022  0.081304 -0.008669    0.217747    0.138647  0.131699   \n",
       "29     0.000000  0.183238 -0.004314    0.076014    0.000000  0.016807   \n",
       "...         ...       ...       ...         ...         ...       ...   \n",
       "5286   0.000000  0.056406  0.000000    0.408760   -0.178747  0.070187   \n",
       "5287   0.000000  0.174374  0.020977    0.467559    0.138647  0.125960   \n",
       "5288   0.000000  0.125998 -0.008669    1.929371    0.503859  0.127118   \n",
       "5289   0.000000  0.000000  0.041030    0.217747    0.138647  0.035909   \n",
       "5290   0.000000 -0.209194  0.012702   -0.448205   -0.178747  0.199714   \n",
       "5291   0.000000  0.260632 -0.004314    0.631709    0.138647  0.382112   \n",
       "5292  -0.327633  0.456100 -0.008669    2.325925    0.251930  0.258499   \n",
       "5293   0.000000  0.043199  0.020977    0.076014    0.000000  0.040872   \n",
       "5294  -0.253022  0.411824  0.020977    1.787638    0.899021  0.204831   \n",
       "5295   0.000000  0.529792  0.025059    0.871874    0.138647  0.137308   \n",
       "5296   0.000000  0.015050 -0.031093    0.000000    0.000000  0.047339   \n",
       "5297   0.000000 -0.209194  0.025059   -0.079914   -0.178747  0.011330   \n",
       "5298   0.000000  0.253675  0.012702    0.000000   -0.430677  0.075939   \n",
       "5299  -0.057547  0.305548  0.056459    0.076014    0.778385  0.251609   \n",
       "5300   1.489125  0.200269  0.029104    0.076014    0.778385  0.092434   \n",
       "5301   0.000000 -0.068362  0.008507   -0.164150   -0.178747  0.091101   \n",
       "5302   1.501789 -0.155149  0.000000   -0.079914    0.000000  0.078766   \n",
       "5303   0.000000  0.311484  0.012702    0.284055    0.347709  0.070187   \n",
       "5304  -0.253022  0.043199  0.008507   -0.079914    0.000000  0.053641   \n",
       "5305   1.934870  0.015050 -0.026519    0.076014    0.628543  0.197114   \n",
       "5306   0.610254  0.274114  0.037089    0.000000    0.138647  0.080167   \n",
       "5307   0.000000  0.360513  0.067697    0.148492    0.138647  0.068727   \n",
       "5308  -0.327633 -0.108854 -0.004314    0.217747   -0.430677  0.009478   \n",
       "5309   0.547309  0.200269  0.041030   -0.079914    0.628543  0.169502   \n",
       "5310  -0.183560  0.403080  0.048810    1.883544    0.251930  0.445095   \n",
       "5311   0.330736  0.000000  0.025059    0.284055   -0.178747  0.018606   \n",
       "5312  -0.118583  0.484876  0.025059    0.631709    0.138647  0.009478   \n",
       "5313  -0.183560  0.350242 -0.049864    2.641747    0.138647  0.104122   \n",
       "5314   0.000000 -0.068362  0.008507   -0.253202    0.000000  0.158551   \n",
       "5315  -0.327633  0.253675  0.012702    2.140231    0.347709 -0.018033   \n",
       "\n",
       "      heartrate       inr   lactate  o2_flow  ...  vent_recieved_1.0  \\\n",
       "0      0.132424  0.476541  0.263315      1.0  ...                1.0   \n",
       "1      0.122666 -0.132111 -0.043109      0.0  ...                0.0   \n",
       "2      0.132424  0.126488  0.081624      0.0  ...                1.0   \n",
       "3      0.068747  0.000000  0.081624      0.0  ...                1.0   \n",
       "4      0.085914  0.000000 -0.087908      0.0  ...                0.0   \n",
       "5      0.068747  0.126488 -0.183151      0.0  ...                1.0   \n",
       "6      0.120179  0.000000  0.081624      0.0  ...                1.0   \n",
       "7      0.034613  0.476541  0.081624      0.0  ...                0.0   \n",
       "8      0.148804  1.412966 -0.597678      0.0  ...                0.0   \n",
       "9      0.110028  0.247811  0.081624      0.0  ...                1.0   \n",
       "10     0.125134  0.476541  0.608593      0.0  ...                0.0   \n",
       "11     0.132424  0.000000 -0.233925      0.0  ...                1.0   \n",
       "12     0.010789 -0.132111  0.081624      0.0  ...                1.0   \n",
       "13     0.099538  2.481030  0.157802      0.0  ...                1.0   \n",
       "14     0.014305  1.927931  0.833285      0.0  ...                1.0   \n",
       "15     0.068747 -0.415374  0.120348      0.0  ...                0.0   \n",
       "16    -0.022756 -0.270369  0.081624      0.0  ...                1.0   \n",
       "17     0.074574  0.000000  0.081624      0.0  ...                1.0   \n",
       "18     0.053689 -0.415374  0.081624      0.0  ...                1.0   \n",
       "19     0.083117  0.000000 -0.183151      0.0  ...                1.0   \n",
       "20     0.179208 -0.132111 -0.134537      0.0  ...                0.0   \n",
       "21     0.189392 -0.132111  0.120348      0.0  ...                1.0   \n",
       "22     0.083117  0.000000  0.081624      0.0  ...                0.0   \n",
       "23     0.037873  0.688918  0.000000      0.0  ...                0.0   \n",
       "24    -0.034783  0.126488  0.081624      0.0  ...                1.0   \n",
       "25     0.021221 -0.270369 -0.233925      0.0  ...                1.0   \n",
       "26     0.102193 -0.415374  0.194066      0.0  ...                1.0   \n",
       "27     0.037873  1.994721  0.081624      0.0  ...                1.0   \n",
       "28     0.141888  0.000000 -0.233925      0.0  ...                1.0   \n",
       "29     0.286851  0.126488 -0.233925      0.0  ...                1.0   \n",
       "...         ...       ...       ...      ...  ...                ...   \n",
       "5286   0.031318 -0.132111  0.081624      0.0  ...                0.0   \n",
       "5287   0.074574  0.126488 -0.183151      0.0  ...                0.0   \n",
       "5288   0.062810 -0.132111 -0.087908      0.0  ...                0.0   \n",
       "5289   0.091435 -0.132111  0.081624      0.0  ...                1.0   \n",
       "5290   0.132424  0.126488  0.157802      0.0  ...                0.0   \n",
       "5291   0.122666 -0.132111  0.531559      0.0  ...                1.0   \n",
       "5292  -0.011172 -0.132111 -0.087908      0.0  ...                0.0   \n",
       "5293   0.151076 -0.270369  0.194066      0.0  ...                0.0   \n",
       "5294   0.246092  1.492224  1.703836      1.0  ...                0.0   \n",
       "5295  -0.030723  0.476541  0.081624      0.0  ...                1.0   \n",
       "5296   0.096860  0.247811  0.081624      0.0  ...                1.0   \n",
       "5297   0.014305 -0.132111 -0.401383      0.0  ...                1.0   \n",
       "5298   0.041101 -0.270369  0.081624      0.0  ...                1.0   \n",
       "5299   0.223545 -0.132111  0.448951      0.0  ...                0.0   \n",
       "5300   0.193375  2.481030  1.540379      0.0  ...                0.0   \n",
       "5301   0.014305  0.247811  0.583488      0.0  ...                0.0   \n",
       "5302   0.170817 -0.132111  1.349563      0.0  ...                0.0   \n",
       "5303   0.130013 -0.415374  0.081624      0.0  ...                0.0   \n",
       "5304   0.027988  0.000000 -0.087908      0.0  ...                0.0   \n",
       "5305  -0.003680  0.476541 -0.043109      0.0  ...                0.0   \n",
       "5306   0.085914  0.476541  0.081624      0.0  ...                0.0   \n",
       "5307   0.056759  0.247811  0.608593      0.0  ...                0.0   \n",
       "5308  -0.060310  0.126488  0.081624      0.0  ...                1.0   \n",
       "5309   0.141888 -0.132111  0.328603      0.0  ...                0.0   \n",
       "5310  -0.078556  0.364376  0.157802      0.0  ...                0.0   \n",
       "5311   0.172936  0.126488  0.081624      0.0  ...                1.0   \n",
       "5312   0.112596  1.161660  0.081624      0.0  ...                0.0   \n",
       "5313   0.047458 -0.132111  0.081624      0.0  ...                1.0   \n",
       "5314   0.027988 -0.270369 -0.401383      0.0  ...                1.0   \n",
       "5315   0.094159  0.126488  0.081624      0.0  ...                1.0   \n",
       "\n",
       "      vent_recieved_2.0  dobutamine_1.0  dopamine_1.0  epinephrine_1.0  \\\n",
       "0                   0.0             1.0           0.0              0.0   \n",
       "1                   1.0             0.0           0.0              0.0   \n",
       "2                   0.0             0.0           0.0              0.0   \n",
       "3                   0.0             0.0           0.0              0.0   \n",
       "4                   1.0             0.0           0.0              0.0   \n",
       "5                   0.0             0.0           0.0              0.0   \n",
       "6                   0.0             0.0           0.0              0.0   \n",
       "7                   1.0             0.0           0.0              0.0   \n",
       "8                   1.0             0.0           0.0              0.0   \n",
       "9                   0.0             0.0           0.0              0.0   \n",
       "10                  1.0             0.0           0.0              0.0   \n",
       "11                  0.0             0.0           0.0              0.0   \n",
       "12                  0.0             0.0           0.0              0.0   \n",
       "13                  0.0             0.0           0.0              0.0   \n",
       "14                  0.0             0.0           0.0              0.0   \n",
       "15                  1.0             0.0           0.0              0.0   \n",
       "16                  0.0             0.0           0.0              0.0   \n",
       "17                  0.0             0.0           0.0              0.0   \n",
       "18                  0.0             0.0           0.0              0.0   \n",
       "19                  0.0             0.0           0.0              0.0   \n",
       "20                  1.0             0.0           0.0              0.0   \n",
       "21                  0.0             0.0           0.0              0.0   \n",
       "22                  1.0             0.0           0.0              0.0   \n",
       "23                  1.0             0.0           0.0              0.0   \n",
       "24                  0.0             0.0           0.0              0.0   \n",
       "25                  0.0             0.0           0.0              0.0   \n",
       "26                  0.0             0.0           0.0              0.0   \n",
       "27                  0.0             0.0           0.0              0.0   \n",
       "28                  0.0             0.0           0.0              0.0   \n",
       "29                  0.0             0.0           0.0              0.0   \n",
       "...                 ...             ...           ...              ...   \n",
       "5286                1.0             0.0           0.0              0.0   \n",
       "5287                1.0             0.0           0.0              0.0   \n",
       "5288                1.0             0.0           0.0              0.0   \n",
       "5289                0.0             0.0           0.0              0.0   \n",
       "5290                1.0             0.0           0.0              0.0   \n",
       "5291                0.0             0.0           0.0              0.0   \n",
       "5292                1.0             0.0           0.0              0.0   \n",
       "5293                1.0             0.0           0.0              0.0   \n",
       "5294                1.0             1.0           1.0              0.0   \n",
       "5295                0.0             0.0           0.0              0.0   \n",
       "5296                0.0             0.0           0.0              0.0   \n",
       "5297                0.0             0.0           0.0              0.0   \n",
       "5298                0.0             0.0           0.0              0.0   \n",
       "5299                1.0             0.0           0.0              0.0   \n",
       "5300                1.0             0.0           0.0              1.0   \n",
       "5301                1.0             0.0           0.0              0.0   \n",
       "5302                1.0             0.0           0.0              0.0   \n",
       "5303                1.0             0.0           1.0              0.0   \n",
       "5304                1.0             0.0           0.0              0.0   \n",
       "5305                1.0             0.0           0.0              0.0   \n",
       "5306                1.0             0.0           0.0              0.0   \n",
       "5307                1.0             0.0           0.0              0.0   \n",
       "5308                0.0             0.0           0.0              0.0   \n",
       "5309                1.0             0.0           0.0              0.0   \n",
       "5310                0.0             0.0           0.0              0.0   \n",
       "5311                0.0             0.0           0.0              0.0   \n",
       "5312                1.0             0.0           0.0              0.0   \n",
       "5313                0.0             0.0           0.0              0.0   \n",
       "5314                0.0             0.0           0.0              0.0   \n",
       "5315                0.0             0.0           0.0              0.0   \n",
       "\n",
       "      norepinephrine_1.0  phenylephrine_1.0  rrt_1.0  vasopressin_1.0  \\\n",
       "0                    0.0                0.0      0.0              0.0   \n",
       "1                    0.0                1.0      0.0              0.0   \n",
       "2                    0.0                0.0      0.0              0.0   \n",
       "3                    0.0                0.0      0.0              0.0   \n",
       "4                    0.0                0.0      0.0              0.0   \n",
       "5                    0.0                0.0      0.0              0.0   \n",
       "6                    0.0                1.0      0.0              0.0   \n",
       "7                    0.0                0.0      0.0              0.0   \n",
       "8                    0.0                0.0      0.0              0.0   \n",
       "9                    0.0                1.0      0.0              0.0   \n",
       "10                   0.0                1.0      0.0              0.0   \n",
       "11                   0.0                0.0      0.0              0.0   \n",
       "12                   0.0                0.0      0.0              0.0   \n",
       "13                   0.0                0.0      0.0              0.0   \n",
       "14                   1.0                0.0      0.0              0.0   \n",
       "15                   0.0                0.0      0.0              0.0   \n",
       "16                   0.0                0.0      0.0              0.0   \n",
       "17                   0.0                0.0      0.0              0.0   \n",
       "18                   0.0                0.0      0.0              0.0   \n",
       "19                   0.0                0.0      0.0              0.0   \n",
       "20                   0.0                0.0      0.0              0.0   \n",
       "21                   0.0                0.0      0.0              0.0   \n",
       "22                   0.0                0.0      0.0              0.0   \n",
       "23                   0.0                1.0      0.0              0.0   \n",
       "24                   0.0                0.0      1.0              0.0   \n",
       "25                   0.0                0.0      0.0              0.0   \n",
       "26                   0.0                0.0      0.0              0.0   \n",
       "27                   0.0                0.0      0.0              0.0   \n",
       "28                   0.0                0.0      0.0              0.0   \n",
       "29                   0.0                1.0      0.0              0.0   \n",
       "...                  ...                ...      ...              ...   \n",
       "5286                 0.0                0.0      0.0              0.0   \n",
       "5287                 1.0                1.0      0.0              0.0   \n",
       "5288                 1.0                1.0      1.0              0.0   \n",
       "5289                 0.0                1.0      0.0              0.0   \n",
       "5290                 0.0                1.0      0.0              0.0   \n",
       "5291                 0.0                0.0      0.0              0.0   \n",
       "5292                 0.0                0.0      0.0              0.0   \n",
       "5293                 0.0                1.0      0.0              0.0   \n",
       "5294                 1.0                1.0      1.0              1.0   \n",
       "5295                 0.0                0.0      0.0              0.0   \n",
       "5296                 0.0                0.0      0.0              0.0   \n",
       "5297                 0.0                0.0      0.0              0.0   \n",
       "5298                 0.0                0.0      0.0              0.0   \n",
       "5299                 1.0                0.0      0.0              0.0   \n",
       "5300                 1.0                1.0      0.0              1.0   \n",
       "5301                 0.0                1.0      0.0              0.0   \n",
       "5302                 0.0                0.0      0.0              0.0   \n",
       "5303                 1.0                0.0      0.0              0.0   \n",
       "5304                 0.0                0.0      0.0              0.0   \n",
       "5305                 1.0                0.0      0.0              0.0   \n",
       "5306                 0.0                0.0      0.0              0.0   \n",
       "5307                 0.0                0.0      0.0              0.0   \n",
       "5308                 0.0                0.0      0.0              0.0   \n",
       "5309                 1.0                1.0      0.0              1.0   \n",
       "5310                 0.0                0.0      0.0              0.0   \n",
       "5311                 0.0                0.0      1.0              0.0   \n",
       "5312                 0.0                0.0      0.0              0.0   \n",
       "5313                 0.0                0.0      1.0              0.0   \n",
       "5314                 0.0                0.0      0.0              0.0   \n",
       "5315                 0.0                0.0      1.0              0.0   \n",
       "\n",
       "      cancer_elix_1.0  \n",
       "0                 0.0  \n",
       "1                 1.0  \n",
       "2                 0.0  \n",
       "3                 0.0  \n",
       "4                 0.0  \n",
       "5                 0.0  \n",
       "6                 0.0  \n",
       "7                 0.0  \n",
       "8                 0.0  \n",
       "9                 0.0  \n",
       "10                0.0  \n",
       "11                0.0  \n",
       "12                0.0  \n",
       "13                0.0  \n",
       "14                1.0  \n",
       "15                0.0  \n",
       "16                0.0  \n",
       "17                0.0  \n",
       "18                1.0  \n",
       "19                0.0  \n",
       "20                0.0  \n",
       "21                0.0  \n",
       "22                0.0  \n",
       "23                0.0  \n",
       "24                0.0  \n",
       "25                0.0  \n",
       "26                0.0  \n",
       "27                0.0  \n",
       "28                0.0  \n",
       "29                0.0  \n",
       "...               ...  \n",
       "5286              0.0  \n",
       "5287              0.0  \n",
       "5288              0.0  \n",
       "5289              0.0  \n",
       "5290              0.0  \n",
       "5291              0.0  \n",
       "5292              0.0  \n",
       "5293              0.0  \n",
       "5294              0.0  \n",
       "5295              0.0  \n",
       "5296              0.0  \n",
       "5297              0.0  \n",
       "5298              0.0  \n",
       "5299              0.0  \n",
       "5300              1.0  \n",
       "5301              0.0  \n",
       "5302              0.0  \n",
       "5303              0.0  \n",
       "5304              0.0  \n",
       "5305              0.0  \n",
       "5306              0.0  \n",
       "5307              0.0  \n",
       "5308              0.0  \n",
       "5309              1.0  \n",
       "5310              0.0  \n",
       "5311              0.0  \n",
       "5312              0.0  \n",
       "5313              0.0  \n",
       "5314              0.0  \n",
       "5315              1.0  \n",
       "\n",
       "[5316 rows x 57 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verbose=0)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 27.3 ms\n"
     ]
    }
   ],
   "source": [
    "test_df\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(x_test_df)\n",
    "pd.DataFrame(imp.fit_transform(x_test_df), columns=list(x_test_df)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bilirubin       bun  chloride  creatinin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0       200030.0\n",
       "1       200033.0\n",
       "2       2000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0        bilirubin       bun  chloride  creatinin...\n",
       "1  [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...\n",
       "2  0       200030.0\n",
       "1       200033.0\n",
       "2       2000..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 42.6 ms\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bilirubin</th>\n",
       "      <th>bun</th>\n",
       "      <th>chloride</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>daily_sofa</th>\n",
       "      <th>glucose</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>inr</th>\n",
       "      <th>lactate</th>\n",
       "      <th>o2_flow</th>\n",
       "      <th>...</th>\n",
       "      <th>vent_recieved_1.0</th>\n",
       "      <th>vent_recieved_2.0</th>\n",
       "      <th>dobutamine_1.0</th>\n",
       "      <th>dopamine_1.0</th>\n",
       "      <th>epinephrine_1.0</th>\n",
       "      <th>norepinephrine_1.0</th>\n",
       "      <th>phenylephrine_1.0</th>\n",
       "      <th>rrt_1.0</th>\n",
       "      <th>vasopressin_1.0</th>\n",
       "      <th>cancer_elix_1.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.167681</td>\n",
       "      <td>0.104422</td>\n",
       "      <td>0.025059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251930</td>\n",
       "      <td>0.185525</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.476541</td>\n",
       "      <td>0.263315</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.068362</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>-0.253202</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.231986</td>\n",
       "      <td>0.122666</td>\n",
       "      <td>-0.132111</td>\n",
       "      <td>-0.043109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136269</td>\n",
       "      <td>0.020977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.430677</td>\n",
       "      <td>0.270437</td>\n",
       "      <td>0.132424</td>\n",
       "      <td>0.126488</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.352864</td>\n",
       "      <td>-0.087975</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>-0.079914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204831</td>\n",
       "      <td>0.068747</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311484</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>0.780201</td>\n",
       "      <td>0.347709</td>\n",
       "      <td>0.045738</td>\n",
       "      <td>0.085914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.087908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bilirubin       bun  chloride  creatinine  daily_sofa   glucose  heartrate  \\\n",
       "0   2.167681  0.104422  0.025059    0.000000    0.251930  0.185525   0.132424   \n",
       "1   0.000000 -0.068362  0.012702   -0.253202   -0.430677  0.231986   0.122666   \n",
       "2   0.000000  0.136269  0.020977    0.000000   -0.430677  0.270437   0.132424   \n",
       "3   1.352864 -0.087975  0.004273   -0.079914    0.000000  0.204831   0.068747   \n",
       "4   0.000000  0.311484  0.012702    0.780201    0.347709  0.045738   0.085914   \n",
       "\n",
       "        inr   lactate  o2_flow  ...  vent_recieved_1.0  vent_recieved_2.0  \\\n",
       "0  0.476541  0.263315      1.0  ...                1.0                0.0   \n",
       "1 -0.132111 -0.043109      0.0  ...                0.0                1.0   \n",
       "2  0.126488  0.081624      0.0  ...                1.0                0.0   \n",
       "3  0.000000  0.081624      0.0  ...                1.0                0.0   \n",
       "4  0.000000 -0.087908      0.0  ...                0.0                1.0   \n",
       "\n",
       "   dobutamine_1.0  dopamine_1.0  epinephrine_1.0  norepinephrine_1.0  \\\n",
       "0             1.0           0.0              0.0                 0.0   \n",
       "1             0.0           0.0              0.0                 0.0   \n",
       "2             0.0           0.0              0.0                 0.0   \n",
       "3             0.0           0.0              0.0                 0.0   \n",
       "4             0.0           0.0              0.0                 0.0   \n",
       "\n",
       "   phenylephrine_1.0  rrt_1.0  vasopressin_1.0  cancer_elix_1.0  \n",
       "0                0.0      0.0              0.0              0.0  \n",
       "1                1.0      0.0              0.0              1.0  \n",
       "2                0.0      0.0              0.0              0.0  \n",
       "3                0.0      0.0              0.0              0.0  \n",
       "4                0.0      0.0              0.0              0.0  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 48 ms\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(imp.fit_transform(x_test_df), columns=list(x_test_df)).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/impute.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    189\u001b[0m             X = check_array(X, accept_sparse='csc', dtype=dtype,\n\u001b[0;32m--> 190\u001b[0;31m                             force_all_finite=force_all_finite, copy=self.copy)\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'black'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0c1a5da6d143>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'median'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreimp_train_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/impute.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \"\"\"\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/sklearn/impute.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 raise ValueError(\"Cannot use {0} strategy with non-numeric \"\n\u001b[1;32m    194\u001b[0m                                  \u001b[0;34m\"data. Received datatype :{1}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                                  \"\".format(self.strategy, X.dtype.kind))\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/rpy-env/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5065\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5066\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5067\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'dtype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 37.6 ms\n"
     ]
    }
   ],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(preimp_train_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### r code to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_transform=['ethnicity', 'ibands', 'ipco2',\n",
    "#                    'any_vasoactive',\"leukocyte\",\"nitrite\",\n",
    "#                    'pao2fio2ratio', 'vent_recieved',  \"dobutamine\",\n",
    "#                    \"dopamine\",\"epinephrine\",\"norepinephrine\",\n",
    "#                    \"phenylephrine\",\"rrt\",\"vasopressin\",'cancer_elix' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions related to gower distance, preprocessing, finding train samples, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_filter(df):  \n",
    "    global all_xy\n",
    "    categorical=['gender',\n",
    "     'ethnicity_black',\n",
    "     'ethnicity_hispanic',\n",
    "     'ethnicity_unknown/other',\n",
    "     'ethnicity_white/nonhispanic',\n",
    "     'ibands_absent',\n",
    "     'any_vasoactive_True',\n",
    "     'leukocyte_1',\n",
    "     'nitrite_1',\n",
    "     'pao2fio2ratio(200, 333]',\n",
    "     'pao2fio2ratio_(333, 475]',\n",
    "     'pao2fio2ratio_(475, 3000]',\n",
    "     'vent_recieved_1',\n",
    "     'vent_recieved_2',\n",
    "     'dobutamine_True',\n",
    "     'dopamine_True',\n",
    "     'epinephrine_True',\n",
    "     'norepinephrine_True',\n",
    "     'phenylephrine_True',\n",
    "     'rrt_True',\n",
    "     'vasopressin_True',\n",
    "     'ipco2_absent',\n",
    "     \"cancer_elix_True\"]\n",
    "    all_xy_label= list(all_xy)\n",
    "    in_both= list(set(categorical)& set(all_xy))\n",
    "\n",
    "    ##restricting all_xy to only continuous variables\n",
    "    all_xy_cont = df.drop(in_both, axis=1)\n",
    "    return(all_xy_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder = '/Users/xuzhenxing/Documents/mimic_AKI_data/real_time_prediction/features/all/dropped/xy'\n",
    "# folder = './xy'\n",
    "\n",
    "def preprocessing(folder, time_interval, isnormalized=True):\n",
    "    \"\"\"Data preprocessing, Preprocessing  missing data with mean imputation; Normalize continous feature with MinMaxScaler;\n",
    "    Normalize categorical feature with OneHotEncoder.\n",
    "\n",
    "    Args:\n",
    "        folder: dir path of source data;\n",
    "        time_interval: interval of time, can be 24,48,72,96,120,144.\n",
    "    Returns:\n",
    "        x: features\n",
    "        y: lables\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_xy = pd.read_csv(os.path.join(folder, 'all_{}hours_test_individualization_1thousand.csv'.format(time_interval)), index_col=0)\n",
    "    # print (all_xy.shape)\n",
    "    # print (all_xy.columns)\n",
    "\n",
    "    medi = ['diuretics', 'nsaid', 'radio', 'angiotensin']\n",
    "    pat = ['gender', 'age', 'ethnicity']\n",
    "    # Total 9 comorbidity\n",
    "    comm = ['congestive_heart_failure', 'peripheral_vascular', 'hypertension',\n",
    "            'diabetes', 'liver_disease', 'mi', 'cad', 'cirrhosis', 'jaundice']\n",
    "\n",
    "    # Total 8 chartevents\n",
    "    chart = ['DiasBP_min', 'DiasBP_max', 'DiasBP_first', 'DiasBP_last', 'DiasBP_slope', 'DiasBP_avg',\n",
    "             'Glucose_min', 'Glucose_max', 'Glucose_first', 'Glucose_last', 'Glucose_slope', 'Glucose_avg',\n",
    "             'HeartRate_min', 'HeartRate_max', 'HeartRate_first', 'HeartRate_last', 'HeartRate_slope', 'HeartRate_avg',\n",
    "             'MeanBP_min', 'MeanBP_max', 'MeanBP_first', 'MeanBP_last', 'MeanBP_slope', 'MeanBP_avg',\n",
    "             'RespRate_min', 'RespRate_max', 'RespRate_first', 'RespRate_last', 'RespRate_slope', 'RespRate_avg',\n",
    "             'SpO2_min', 'SpO2_max', 'SpO2_first', 'SpO2_last', 'SpO2_slope', 'SpO2_avg',\n",
    "             'SysBP_min', 'SysBP_max', 'SysBP_first', 'SysBP_last', 'SysBP_slope', 'SysBP_avg',\n",
    "             'Temp_min', 'Temp_max', 'Temp_first', 'Temp_last', 'Temp_slope', 'Temp_avg']\n",
    "\n",
    "    # Total 12 labvents\n",
    "    lab = ['BICARBONATE_first', 'BICARBONATE_last', 'BICARBONATE_min', 'BICARBONATE_max', 'BICARBONATE_avg',\n",
    "           'BICARBONATE_slope', 'BICARBONATE_count',\n",
    "           'BUN_first', 'BUN_last', 'BUN_min', 'BUN_max', 'BUN_avg', 'BUN_slope', 'BUN_count',\n",
    "           'CHLORIDE_first', 'CHLORIDE_last', 'CHLORIDE_min', 'CHLORIDE_max', 'CHLORIDE_avg', 'CHLORIDE_slope',\n",
    "           'CHLORIDE_count',\n",
    "           'CREATININE_first', 'CREATININE_last', 'CREATININE_min', 'CREATININE_max', 'CREATININE_avg',\n",
    "           'CREATININE_slope', 'CREATININE_count',\n",
    "           'HEMOGLOBIN_first', 'HEMOGLOBIN_last', 'HEMOGLOBIN_min', 'HEMOGLOBIN_max', 'HEMOGLOBIN_avg',\n",
    "           'HEMOGLOBIN_slope', 'HEMOGLOBIN_count',\n",
    "           'INR_first', 'INR_last', 'INR_min', 'INR_max', 'INR_avg', 'INR_count',\n",
    "           'PLATELET_first', 'PLATELET_last', 'PLATELET_min', 'PLATELET_max', 'PLATELET_avg', 'PLATELET_slope',\n",
    "           'PLATELET_count',\n",
    "           'POTASSIUM_first', 'POTASSIUM_last', 'POTASSIUM_min', 'POTASSIUM_max', 'POTASSIUM_avg', 'POTASSIUM_slope',\n",
    "           'POTASSIUM_count',\n",
    "           'PT_first', 'PT_last', 'PT_min', 'PT_max', 'PT_avg', 'PT_count',\n",
    "           'PTT_first', 'PTT_last', 'PTT_min', 'PTT_max', 'PTT_avg', 'PTT_count',\n",
    "           'WBC_first', 'WBC_last', 'WBC_min', 'WBC_max', 'WBC_avg', 'WBC_slope', 'WBC_count',\n",
    "           'CALCIUM_first', 'CALCIUM_last', 'CALCIUM_min', 'CALCIUM_max', 'CALCIUM_avg', 'CALCIUM_count'\n",
    "           ]\n",
    "\n",
    "    if time_interval != 24:  # The 24h data lack of the feature 'CALCIUM_slope'\n",
    "        lab.append('CALCIUM_slope')\n",
    "    subset = medi + pat + comm + ['avg_urine'] + ['egfr_min'] + ['label'] # note that ['avg_urine'] + ['egfr_min'] is important, ignoring if they are empty.\n",
    "\n",
    "    all_xy = all_xy.dropna(subset=subset)\n",
    "\n",
    "    # print ('after dropping nan in the catergorical variables, the shape is {}'.format(all_xy.shape))\n",
    "\n",
    "    all_conti_x = all_xy[chart + lab + ['avg_urine'] + ['egfr_min'] + ['age']]\n",
    "    # print (all_conti_x.shape)\n",
    "    # print (all_conti_x)\n",
    "    all_categ_x = all_xy[['gender'] + ['ethnicity'] + medi + comm]\n",
    "    # print (all_categ_x.shape)\n",
    "    # print (all_categ_x)\n",
    "\n",
    "    # Using mean imputer after drop the nan data in medication, patient demographic data, avg_ureine, egfr_min and label\n",
    "    imp = Imputer(strategy='mean', axis=0)\n",
    "    all_conti_x_fitted = imp.fit_transform(all_conti_x)\n",
    "\n",
    "    def normalize(all_conti_x_fitted, all_categ_x):\n",
    "        # using the MinMaxScaler to normalization the all_x\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        all_conti_x_fitted = min_max_scaler.fit_transform(all_conti_x_fitted)\n",
    "        # print (all_conti_x_fitted.shape, all_conti_x_fitted)\n",
    "        # all_conti_x = DataFrame(all_conti_x_fitted, columns=all_conti_x.columns)\n",
    "        # print (all_conti_x.shape)\n",
    "\n",
    "        onehot_enc = OneHotEncoder(sparse=False)  # dense format\n",
    "        all_categ_x_fitted = onehot_enc.fit_transform(all_categ_x)\n",
    "        # print (all_categ_x_fitted.shape, all_categ_x_fitted)\n",
    "        return all_conti_x_fitted, all_categ_x_fitted\n",
    "\n",
    "    if isnormalized:\n",
    "        all_conti_x_fitted, all_categ_x_fitted = normalize(all_conti_x_fitted, all_categ_x)\n",
    "\n",
    "    x = np.hstack((all_conti_x_fitted, all_categ_x_fitted))\n",
    "    # y = all_xy['label']\n",
    "    # x = np.array(x)\n",
    "    # y = np.array(y)\n",
    "    # print (x.shape, y.shape)\n",
    "    # return x, y\n",
    "    y = all_xy['label']\n",
    "    z_icustay_id = y.index\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    z_icustay_id = np.array(z_icustay_id)\n",
    "\n",
    "    print (x.shape, y.shape)\n",
    "    return x, y, z_icustay_id, all_xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_model(pipe, param_grid, name, X_train, X_test,\n",
    "               y_train, y_test, scoring, verbose=0):\n",
    "    gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=scoring, cv=5, n_jobs=-1, verbose=verbose)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = gs.predict(X_train)\n",
    "    y_test_pred = gs.predict(X_test)\n",
    "\n",
    "    acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "    acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_train, gs.predict_proba(X_train)[:, 1])\n",
    "    auc_train = auc(fpr, tpr)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, gs.predict_proba(X_test)[:, 1])\n",
    "    auc_test = auc(fpr, tpr)\n",
    "\n",
    "    confmat_train = confusion_matrix(y_true=y_train, y_pred=y_train_pred)\n",
    "    confmat_test = confusion_matrix(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "    print (' best parameter: ', gs.best_params_)\n",
    "    print (' training acc:%.2f auc:%.2f ' % (acc_train, auc_train))\n",
    "    print (' testing acc:%.2f auc:%.2f ' % (acc_test, auc_test))\n",
    "\n",
    "    print (' train confusion matrix:\\n', confmat_train)\n",
    "    print (' testing confusion matrix:\\n', confmat_test)\n",
    "    print (' classification report:\\n', classification_report(y_test, y_test_pred))\n",
    "\n",
    "    train_report = np.array(precision_recall_fscore_support(y_train, y_train_pred))\n",
    "    train_class1_report = train_report[:, 1]\n",
    "    train_metrics = list(train_class1_report[:-1])\n",
    "    train_metrics.extend([acc_train, auc_train])\n",
    "    print ('training metrics: precision, recall, f1-score, acc, auc')\n",
    "    print (train_metrics)\n",
    "\n",
    "    test_report = np.array(precision_recall_fscore_support(y_test, y_test_pred))\n",
    "    test_class1_report = test_report[:, 1]\n",
    "    test_metrics = list(test_class1_report[:-1])\n",
    "    test_metrics.extend([acc_test, auc_test])\n",
    "    print ('test metrics: precision, recall, f1-score, acc, auc')\n",
    "    print (test_metrics)\n",
    "\n",
    "    return train_metrics, test_metrics\n",
    "    \"\"\"\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (recall)\")\n",
    "\n",
    "    plt.plot(fpr, tpr, label=\"acc:%f auc:%f\" % (acc_test, auc_test))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_train, gs.predict_proba(X_train)[:,1])\n",
    "    average_precision = average_precision_score(y_test, gs.predict_proba(X_test)[:,1])\n",
    "    plt.xlabel(\"precision\")\n",
    "    plt.ylabel(\"recall\")\n",
    "    plt.step(precision, recall, where='post', label='AP={0:0.2f}'.format(average_precision))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_dbdt(X_train, X_test, y_train, y_test, scoring):\n",
    "    gbm = GradientBoostingClassifier(learning_rate=0.05, n_estimators=120, min_samples_leaf=60,\n",
    "                                     max_features=9, subsample=0.7, random_state=10)\n",
    "\n",
    "    param_grid = {'max_depth': list(range(3, 14, 2)), 'min_samples_split': list(range(100, 801, 200))}\n",
    "    train_metrics, test_metrics = perf_model(gbm, param_grid, 'GBDT', X_train, X_test, y_train, y_test, scoring, 0)\n",
    "    return train_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#issue im having is that \n",
    "\n",
    "def try_models_cross(X_train, X_test, y_train, y_test, scoring):#  select data cross 5 Fold\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, stratify=Y, random_state=RANDOM_STATE)\n",
    "    # \"\"\"\n",
    "    # print ('\\n\\nLinear Logistic Regression with L1 Penalty')\n",
    "    # lgr_l1_train_metrics, lgr_l1_test_metrics = try_lgr_l1(X_train, X_test, y_train, y_test, scoring)\n",
    "    #\n",
    "    # print ('\\n\\nLinear Logistic Regression with L2 Penalty')\n",
    "    # lgr_l2_train_metrics, lgr_l2_test_metrics = try_lgr_l2(X_train, X_test, y_train, y_test, scoring)\n",
    "    #\n",
    "    # print ('\\n\\nStochastic Gradient Descent')\n",
    "    # Elastic_train_metrics, Elastic_test_metrics = try_sgd(X_train, X_test, y_train, y_test, scoring)\n",
    "    #\n",
    "    # print ('\\n\\nRandom Forest')\n",
    "    # rf_train_metrics, rf_test_metrics = try_rf(X_train, X_test, y_train, y_test, scoring)\n",
    "    # #\n",
    "    print ('\\n\\nGradient Boosting Decision tree')\n",
    "    xgboost_train_metrics, xgboost_test_metrics = try_dbdt(X_train, X_test, y_train, y_test, scoring)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing and formatting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling') #use to change working directory\n",
    "wd= os.getcwd() #'/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling'\n",
    "\n",
    "final_pt_df2 = pd.read_csv(Path(wd + '/data/raw/csv/04042019_final_pt_df2_v.csv') , index_col=0) #only for patients with minimum vitals\n",
    "patients= list(final_pt_df2['subject_id'].unique())\n",
    "hadm_id= list(final_pt_df2['hadm_id'].unique())\n",
    "icustay_id= list(final_pt_df2['icustay_id'].unique())\n",
    "icustay_id= [int(x) for x in icustay_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= pd.read_csv(\"/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling/models/imputation/{}_median_imputed_train_{}.csv\".format(date,timewindowdays)) #two class training data\n",
    "test_data= pd.read_csv(\"/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling/models/imputation/{}_median_imputed_test_{}.csv\".format(date,timewindowdays)) #two class training data\n",
    "\n",
    "#/Users/geickelb1/Documents/GitHub/mimiciii-antibiotics-modeling/models/imputation/04042019_newagg2_median_imputed_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pt_df2['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## light data reformatting for model\n",
    "### most data are already converted to median type zscores, however weight and admit age still need to be converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 59.8 ms\n"
     ]
    }
   ],
   "source": [
    "def preprocessing(data):\n",
    "    train_data=data.copy()\n",
    "    weight_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).median()\n",
    "    weight_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.25)#.between(train_data['col'].quantile(.25), df['col'].quantile(.75), inclusive=True)]\n",
    "    weight_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.75)\n",
    "    weight_iqr=weight_quant3-weight_quant1\n",
    "    #print(weight_median,weight_quant3,weight_quant1, weight_iqr)\n",
    "\n",
    "    age_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).median()\n",
    "    age_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.25)\n",
    "    age_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.75)\n",
    "    age_iqr=age_quant3-age_quant1\n",
    "\n",
    "    #converting to log scaled standardized data for age/weight\n",
    "    train_data['weight']=train_data['weight'].apply(lambda x: (np.log(x+1)-weight_median)/weight_iqr)\n",
    "    train_data['first_admit_age']=train_data['first_admit_age'].apply(lambda x: (np.log(x+1)-age_median)/age_iqr)\n",
    "    \n",
    "    ### onehot encoding categorical var\n",
    "    cols_to_transform=['ethnicity', 'ibands', 'ipco2',\n",
    "                       'any_vasoactive',\"leukocyte\",\"nitrite\",\n",
    "                       'pao2fio2ratio', 'vent_recieved',  \"dobutamine\",\n",
    "                       \"dopamine\",\"epinephrine\",\"norepinephrine\",\n",
    "                       \"phenylephrine\",\"rrt\",\"vasopressin\",'cancer_elix' ]\n",
    "    train_data = pd.get_dummies(train_data, columns = cols_to_transform, drop_first=True)\n",
    "    \n",
    "    \n",
    "    #binarizing and poping outcome for training data\n",
    "    train_data.loc[train_data['final_bin']==\"C_pos/A_full\",\"final_bin\"]=1\n",
    "    train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"final_bin\"]=0\n",
    "    train_data['final_bin']=pd.to_numeric(train_data['final_bin'])\n",
    "    \n",
    "    ## establishing training data and labels\n",
    "    x_train= train_data.copy()\n",
    "    z_icustay_id=x_train.pop('icustay_id')\n",
    "    y_train= x_train.pop(\"final_bin\").values\n",
    "    \n",
    "    return(x_train, y_train, z_icustay_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_preprocessing(data):\n",
    "    train_data=data.copy()\n",
    "    weight_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).median()\n",
    "    weight_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.25)#.between(train_data['col'].quantile(.25), df['col'].quantile(.75), inclusive=True)]\n",
    "    weight_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"weight\"]+1).quantile(0.75)\n",
    "    weight_iqr=weight_quant3-weight_quant1\n",
    "    #print(weight_median,weight_quant3,weight_quant1, weight_iqr)\n",
    "\n",
    "    age_median=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).median()\n",
    "    age_quant1=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.25)\n",
    "    age_quant3=np.log(train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"first_admit_age\"]+1).quantile(0.75)\n",
    "    age_iqr=age_quant3-age_quant1\n",
    "\n",
    "    #converting to log scaled standardized data for age/weight\n",
    "    train_data['weight']=train_data['weight'].apply(lambda x: (np.log(x+1)-weight_median)/weight_iqr)\n",
    "    train_data['first_admit_age']=train_data['first_admit_age'].apply(lambda x: (np.log(x+1)-age_median)/age_iqr)\n",
    "    \n",
    "    ### onehot encoding categorical var\n",
    "    cols_to_transform=['ethnicity', 'ibands', 'ipco2', 'any_vasoactive',\"leukocyte\",\"nitrite\", 'pao2fio2ratio', 'vent_recieved',  \"dobutamine\",\"dopamine\",\"epinephrine\",\"norepinephrine\",\"phenylephrine\",\"rrt\",\"vasopressin\",'cancer_elix' ]\n",
    "    train_data = pd.get_dummies(train_data, columns = cols_to_transform, drop_first=True)\n",
    "    \n",
    "    \n",
    "    #binarizing and poping outcome for training data\n",
    "    train_data.loc[train_data['final_bin']==\"C_pos/A_full\",\"final_bin\"]=1\n",
    "    train_data.loc[train_data['final_bin']==\"C_neg/A_partial\",\"final_bin\"]=0\n",
    "    train_data['final_bin']=pd.to_numeric(train_data['final_bin'])\n",
    "    \n",
    "    ## establishing training data and labels\n",
    "\n",
    "    all_xy= train_data.copy()\n",
    "    all_xy=all_xy.set_index('icustay_id').rename(columns={'final_bin':\"label\"})\n",
    "    \n",
    "    return(all_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, z_icustay_id = preprocessing(train_data)\n",
    "x_test, y_test, z_icustay_id_test= preprocessing(test_data)\n",
    "#for local modeling\n",
    "all_xy=xy_preprocessing(train_data)\n",
    "all_xy_test=xy_preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(all_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optional qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.iloc[1:5, 25:45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.iloc[1:5, 35:65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.iloc[1:5, 10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## looking at correlation of all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = x_train.corr().abs()\n",
    "\n",
    "plt.figure(figsize=(25, 20))\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = (corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool)).stack().sort_values(ascending=False))\n",
    "cor_df=pd.DataFrame(sol)#.sort_values(kind=\"quicksort\") #[-10:0])\n",
    "cor_df=cor_df.reset_index()\n",
    "cor_df=cor_df.rename(columns={'level_0': 'corx', 'level_1': 'cory', 0:'corr'})\n",
    "cor_df2=cor_df[(cor_df['corx']!=cor_df['cory']) & (cor_df['corr']>0.7)].sort_values('corr', ascending=False)\n",
    "cor_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DROPING one of the 2 columns with correlation >0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tcorx\tcory\tcorr\n",
    "0\tipco2_absent\tpao2fio2Ratio_(475, 3000]\t0.872418\n",
    "1\tmaxWBC\tminWBC\t0.802373\n",
    "2\tbun\tcreatinine\t0.720861\n",
    "3\tmaxSodium\tminSodium\t0.704233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.drop(columns=list(cor_df2['corx']), inplace=True, errors='raise')\n",
    "x_test.drop(columns=list(cor_df2['corx']), inplace=True, errors='raise')\n",
    "all_xy.drop(columns=list(cor_df2['corx']), inplace=True, errors='raise')\n",
    "all_xy_test.drop(columns=list(cor_df2['corx']), inplace=True, errors='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formatting x and y for modleing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=np.array(x_train.iloc[:,[1,2,3,4,5,6,7,8,9,38,39,40,41]]).copy() #copy of x_train\n",
    "#x=np.array(x_train.iloc[:,[1,2,3,4,5,6,7,8,9]]).copy() #copy of x_train\n",
    "#x=np.array(x_train.iloc[:,38:])\n",
    "x=np.array(x_train.copy())\n",
    "\n",
    "#x=np.array(train_data.iloc[:,[1,2,3,4]]).copy() #copy of x_train\n",
    "#train_data.iloc[:,[1,2,3,4,5]] ###drastically reducing my dataframe size to test algorithm\n",
    "y=y_train.copy() #copy of y_train\n",
    "\n",
    "##all_xy: train data with finalbin:label and index=icustay_id\n",
    "#all_xy=train_data.copy().set_index(\"icustay_id\").rename(columns={'final_bin':\"label\"}) #\n",
    "\n",
    "time_interval=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train),len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelbuilding\n",
    "* step1) hypertune xgb on 5fold cv.\n",
    "* step2) test entire trainset and predict trainset.\n",
    "* step3) run hypertuned model on 5fold cv with lr and get overall metrics.\n",
    "* step4) local model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step1) XGB hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    from sklearn.metrics import log_loss\n",
    "    \n",
    "    y_hat = model.predict(test_features)\n",
    "    errors = abs(y_hat - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    auc=roc_auc_score(test_labels, y_hat)\n",
    "    loss= log_loss(test_labels, y_hat)\n",
    "    \n",
    "    print ('the AUC is: {:0.2f}'.format(auc))\n",
    "    print ('the logloss is: {:0.2f}'.format(loss))\n",
    "    print(confusion_matrix(test_labels, y_hat))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# base_model = RandomForestClassifier(n_estimators = 10, random_state = 42)\n",
    "# base_model.fit(x, y)\n",
    "# base_auc = evaluate(base_model, x, y)\n",
    "\n",
    "# best_random = rf_random.best_estimator_\n",
    "# random_auc = evaluate(best_random, x, y)\n",
    "\n",
    "# print('Improvement of {:0.2f}%.'.format( 100 * (random_auc - base_auc) / base_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypertuning_fxn(X, y, nfolds, model , param_grid, base_model, scoring=\"neg_log_loss\", gridsearch=True, n_iter=20): \n",
    "    if gridsearch==True:\n",
    "        grid_search = GridSearchCV(estimator= model,\n",
    "                                         param_grid=param_grid,\n",
    "                                         cv=nfolds,\n",
    "                                         scoring=scoring,\n",
    "                                         return_train_score=True,\n",
    "                                         n_jobs = -1)\n",
    "    else:\n",
    "        grid_search = RandomizedSearchCV(estimator= model,\n",
    "                                         param_distributions= param_grid,\n",
    "                                         n_iter=n_iter,\n",
    "                                         cv=nfolds,\n",
    "                                         scoring=scoring,\n",
    "                                         return_train_score=True,\n",
    "                                         n_jobs = -1)\n",
    "    grid_search.fit(X, y)    \n",
    "    \n",
    "    print(\"Grid scores on development set:\")\n",
    "    means = grid_search.cv_results_['mean_test_score']\n",
    "    stds = grid_search.cv_results_['std_test_score']\n",
    "    \n",
    "    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "        \n",
    "    #grid_search.best_params_\n",
    "    print(grid_search.best_score_)\n",
    "    print(\"\\n\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    print('\\n base model:')\n",
    "    base_model = base_model#(random_state = 42)\n",
    "    base_model.fit(x, y)\n",
    "    base_auc = evaluate(base_model, x, y)\n",
    "    \n",
    "    print('\\n hypertuned model:')\n",
    "    best_random = grid_search.best_estimator_\n",
    "    random_auc = evaluate(best_random, x, y)\n",
    "\n",
    "    print('logloss change of {:0.2f}%. after hypertuning on training set (may be overfit)'.format( 100 * (random_auc - base_auc) / base_auc))\n",
    "    \n",
    "    print(grid_search.best_estimator_)\n",
    "    \n",
    "    return(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###xgboost\n",
    "model= XGBClassifier(n_estimators=100, min_child_weight=2, #changed: GridSearchCV ->RandomizedSearchCV\n",
    "                                              gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                              objective='binary:logistic', n_jobs=-1, seed=27)\n",
    "scale_pos_weight = [0.1, 1, 5, 10]\n",
    "max_depth = [1, 2, 3, 4, 5]\n",
    "learning_rate=[0.01, 0.1, 0.5, 1]\n",
    "param_grid = {'scale_pos_weight': scale_pos_weight, 'max_depth' : max_depth, \"learning_rate\":learning_rate}\n",
    "\n",
    "base_model=XGBClassifier(random_state = 42)\n",
    "xgboost_hyper=hypertuning_fxn(x, y, nfolds=5, model=model , param_grid=param_grid, base_model= base_model, scoring=\"neg_log_loss\", n_iter=20, gridsearch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###rf\n",
    "# Number of trees in random forest\n",
    "#n_estimators = [100, 1000]#[int(x) for x in np.linspace(start = 10, stop = 1000, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [3,'auto', 10 ]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [5,10, 25]#[int(x) for x in np.linspace(5, 110, num = 5)]\n",
    "#max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [2, 5, 10]\n",
    "# Method of selecting samples for training each tree\n",
    "#bootstrap = [True, False]\n",
    "\n",
    "#class_weight is either a dictionary of each class to a uniform weight for that class (e.g., {1:.9, 2:.5, 3:.01}), or is a string telling sklearn how to automatically determine this dictionary.\n",
    "class_weight= [None,{0:1, 1:4}, {0:(1/np.bincount(y))[0], 1:(1/np.bincount(y))[1]}]\n",
    "\n",
    "\n",
    "param_grid = {#'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'class_weight': class_weight}\n",
    "\n",
    "\n",
    "model= RandomForestClassifier(criterion='entropy')\n",
    "base_model=RandomForestClassifier(random_state = 42, criterion='entropy')\n",
    "\n",
    "rf_hyper=hypertuning_fxn(x, y, nfolds=10, model=model , param_grid=param_grid, base_model= base_model, scoring=\"neg_log_loss\",n_iter = 30, gridsearch=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertune SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= svm.SVC(probability=True)\n",
    "kernel = ['linear', 'rbf', 'poly']\n",
    "gamma = [0.1, 1, 'auto'] #Kernel coefficient for ârbfâ, âpolyâ and âsigmoidâ. default=âautoâ uses 1 / n_features\n",
    "C = [0.1, 1, 10, 100] #Penalty parameter C of the error term.\n",
    "degree = [0, 1, 2]\n",
    "class_weight=['balanced', None]\n",
    "\n",
    "param_grid = {'kernel': kernel,\n",
    "              'gamma': gamma,\n",
    "              'C': C,\n",
    "              'degree': degree,\n",
    "              'class_weight':class_weight}\n",
    "\n",
    "base_model=svm.SVC(probability=True)\n",
    "\n",
    "svc_hyper=hypertuning_fxn(x, y, nfolds=4, model=model , param_grid=param_grid, base_model= base_model, scoring=\"neg_log_loss\", n_iter=10, gridsearch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model= KNeighborsClassifier()\n",
    "\n",
    "n_neighbors = [3,4,5, 8, 10, 25]\n",
    "weights=['uniform']\n",
    "p=[1,2] #1= mmanhattan, 2= euclidian\n",
    "\n",
    "\n",
    "param_grid = {'n_neighbors': n_neighbors,\n",
    "              'weights': weights,\n",
    "              'p': p}\n",
    "\n",
    "base_model=KNeighborsClassifier()\n",
    "\n",
    "knn_hyper=hypertuning_fxn(x, y, nfolds=10, model=model , param_grid=param_grid, base_model= base_model, scoring=\"neg_log_loss\", n_iter=40, gridsearch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_hyper.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(knn_hyper, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(base_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNeighborsClassifier(RANDOM_STATE=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuned Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "#        colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "#        max_depth=4, min_child_weight=2, missing=None, n_estimators=100,\n",
    "#        n_jobs=-1, nthread=None, objective='binary:logistic',\n",
    "#        random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "#        seed=27, silent=True, subsample=0.8)\n",
    "# #xgboost.fit(x, y)\n",
    "\n",
    "# logreg = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "#                         intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "\n",
    "# #logreg.fit(x, y)\n",
    "\n",
    "# rf= RandomForestClassifier(bootstrap=False, class_weight={0: 1, 1: 4},\n",
    "#             criterion='entropy', max_depth=10, max_features='auto',\n",
    "#             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "#             min_impurity_split=None, min_samples_leaf=2,\n",
    "#             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "#             n_estimators=600, n_jobs=None, oob_score=False,\n",
    "#             random_state=None, verbose=0, warm_start=False)\n",
    "# #rf.fit(x,y)\n",
    "\n",
    "# # from sklearn.naive_bayes import GaussianNB\n",
    "# # gnb =GaussianNB()\n",
    "# # nb_y_pred = gnb.fit(x, y)\n",
    "\n",
    "# svc= svm.SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "#           decision_function_shape='ovr', degree=0, gamma=1, kernel='linear',\n",
    "#           max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "#           tol=0.001, verbose=False)\n",
    "# #svc.fit(x, y)\n",
    "\n",
    "xgboost= xgboost_hyper.best_estimator_\n",
    "\n",
    "logreg = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                        intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "\n",
    "rf= rf_hyper.best_estimator_\n",
    "\n",
    "svc=svc_hypter.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(model_name, hardcode=True):\n",
    "    if hardcode==True:\n",
    "        if model_name== 'xgboost':\n",
    "            model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "               colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "               max_depth=4, min_child_weight=2, missing=None, n_estimators=100,\n",
    "               n_jobs=-1, nthread=None, objective='binary:logistic',\n",
    "               random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "               seed=27, silent=True, subsample=0.8)\n",
    "            \n",
    "\n",
    "        elif model_name== 'logreg':\n",
    "            model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                                    intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "\n",
    "        elif model_name== 'rf':\n",
    "            model = RandomForestClassifier(bootstrap=False, class_weight={0: 1, 1: 4},\n",
    "                criterion='entropy', max_depth=10, max_features='auto',\n",
    "                max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                min_impurity_split=None, min_samples_leaf=2,\n",
    "                min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                n_estimators=600, n_jobs=None, oob_score=False,\n",
    "                random_state=None, verbose=0, warm_start=False)\n",
    "\n",
    "        elif model_name== 'svc':\n",
    "            model = svm.SVC(C=100, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "                  decision_function_shape='ovr', degree=0, gamma=1, kernel='linear',\n",
    "                  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
    "                  tol=0.001, verbose=False)\n",
    "            \n",
    "        elif model_name== 'knn':\n",
    "            model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "                   metric_params=None, n_jobs=None, n_neighbors=25, p=1,\n",
    "                   weights='uniform')\n",
    "    \n",
    "    else:\n",
    "            if model_name== 'xgboost':\n",
    "                model = xgboost_hyper.best_estimator_\n",
    "\n",
    "            elif model_name== 'logreg':\n",
    "                model = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, fit_intercept=True,\n",
    "                                        intercept_scaling=1, class_weight='balanced', random_state=None)\n",
    "\n",
    "            elif model_name== 'rf':\n",
    "                model = rf_hyper.best_estimator_\n",
    "\n",
    "            elif model_name== 'svc':\n",
    "                model = svc_hyper.best_estimator_\n",
    "                \n",
    "            elif model_name== 'knn':\n",
    "                model = knn_hyper.best_estimator_\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_score(model,train_index, x=x,y=y):\n",
    "    y_pred_proba = model.predict_proba(x[train_index])[:, 1] \n",
    "    roc_score=roc_auc_score(y[train_index], y_pred_proba)\n",
    "    return(roc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## youden index and plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimal_youden_index(fpr, tpr, thresholds, tp90=True):\n",
    "    \"\"\"\n",
    "    inputs fpr, tpr, thresholds from metrics.roc(),\n",
    "    outputs the clasification threshold, roc dataframe, and the index of roc dataframe for optimal youden index\n",
    "    \"\"\"\n",
    "    #making dataframe out of the thresholds\n",
    "    roc_df= pd.DataFrame({\"thresholds\": thresholds,\"fpr\":fpr, \"tpr\": tpr})\n",
    "    roc_df.iloc[0,0] =1\n",
    "    roc_df['yuden']= roc_df['tpr']-roc_df['fpr']\n",
    "    \n",
    "    if tp90==True:\n",
    "        idx= roc_df[roc_df['tpr']>=0.9]['yuden'].idxmax() #changed this so now finds optimial yuden threshold but tp>=90%\n",
    "    else:\n",
    "        idx=roc_df['yuden'].idxmax() #MAX INDEX\n",
    "    \n",
    "    youden_threshold=roc_df.iloc[idx,0] #threshold for max youden\n",
    "    return(youden_threshold, roc_df, idx)\n",
    "    \n",
    "def plot_roc(fpr, tpr, roc_auc, roc_df, idx, save=False,model_name=None, folder_name=None, file_name=None):\n",
    "    plt.title('ROC with optimal Youden Index')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    \n",
    "    #finding the point on the line given threshold 0.5 (finding closest row in roc_df)\n",
    "    og_idx=roc_df.iloc[(roc_df['thresholds']-0.5).abs().argsort()[:1]].index[0]\n",
    "    plt.plot(roc_df.iloc[og_idx,1], roc_df.iloc[og_idx,2],marker='x', markersize=5, color=\"g\")\n",
    "    plt.annotate(s=\"P(>=0.5)\",xy=(roc_df.iloc[og_idx,1]+0.02, roc_df.iloc[og_idx,2]-0.04),color='g') #textcoords\n",
    "    \n",
    "    \n",
    "    plt.plot(roc_df.iloc[idx,1], roc_df.iloc[idx,2],marker='o', markersize=5, color=\"r\") ##\n",
    "    plt.annotate(s=\"M_Youden\",xy=(roc_df.iloc[idx,1]+0.02, roc_df.iloc[idx,2]-0.04),color='r' ) #textcoords\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    \n",
    "    if save==True:\n",
    "        if folder_name != None:\n",
    "            address = 'figures/{}/'.format(folder_name)\n",
    "        else:\n",
    "            address = 'figures/'\n",
    "        if not os.path.exists(address):\n",
    "            os.makedirs(address)\n",
    "        plt.savefig(address+\"/{}_{}.png\".format(model_name,file_name),bbox_inches='tight')\n",
    "    else: pass\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def plot_table_as_fig(table_in, col_labels, row_labels, save=False,model_name=None,folder_name=None, file_name=None,figsize=(6,1)):\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    table = plt.table(cellText = table_in, \n",
    "                  colLabels = col_labels,\n",
    "                  rowLabels = row_labels,\n",
    "                  loc='best')\n",
    "    plt.axis(\"tight\")\n",
    "    plt.axis('off')\n",
    "    if save==True:\n",
    "        if folder_name != None:\n",
    "            address = 'figures/{}/'.format(folder_name)\n",
    "        else:\n",
    "            address = 'figures/'\n",
    "        if not os.path.exists(address):\n",
    "            os.makedirs(address)\n",
    "        plt.savefig(address+\"/{}_{}.png\".format(model_name,file_name),bbox_inches='tight')\n",
    "    else: pass\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_eval(model, x=x, y=y, proba_input=False,pos_label=1, print_default=True, save=False,model_name=None, folder_name=None):\n",
    "    import sklearn.metrics as metrics\n",
    "    from sklearn.metrics import precision_score, roc_auc_score, f1_score, recall_score\n",
    "\n",
    "    \"\"\"\n",
    "    catchall classification evaluation function. will print/save the following:\n",
    "    \n",
    "    print/save the following:\n",
    "        ROC curve marked with threshold for optimal youden (maximizing tpr+fpr with constraint that tpr>0.9)\n",
    "\n",
    "        using 0.5 threshold:\n",
    "            confusion matrix\n",
    "            classification report\n",
    "            npv\n",
    "            accuracy\n",
    "\n",
    "\n",
    "        using optimal youden (maximizing tpr+fpr with constraint that tpr>0.9):\n",
    "            confusion matrix\n",
    "            classification report\n",
    "            npv\n",
    "            accuracy\n",
    "    \n",
    "    output: \n",
    "        outputs modelname, auc, precision, recall, f1, and npv to a dictionary. \n",
    "    \n",
    "    notes:\n",
    "    youden's J statistic:\n",
    "    J= sensitivity + specificity -1\n",
    "    (truepos/ truepos+falseneg) + (true neg/ trueneg + falsepos) -1\n",
    "    \n",
    "    \"\"\"\n",
    "    if save==True: #making folder if one doesn't exist\n",
    "        if folder_name != None:\n",
    "            address = '../figures/{}/'.format(folder_name)\n",
    "        else:\n",
    "            address = 'train/'\n",
    "        if not os.path.exists(address):\n",
    "            os.makedirs(address)\n",
    "    \n",
    "    if proba_input==True:  #incorporating classifier_eval2() functionality into this (ie allowing user to input a y_proba instead of a model)\n",
    "        y_proba= model\n",
    "        y_pred=[1 if y >= 0.5 else 0 for y in y_proba]\n",
    "    \n",
    "    else:\n",
    "        model_name=type(model).__name__\n",
    "\n",
    "        y_pred = model.predict(x)\n",
    "        y_proba = model.predict_proba(x)[:,1]\n",
    "        \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, y_proba, pos_label=pos_label)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    #gathering the optimal youden_index and df of tpr/fpr for auc and index of that optimal youden. idx is needed in the roc\n",
    "    youden_threshold, roc_df, idx= optimal_youden_index(fpr, tpr, thresholds,tp90=True)\n",
    "\n",
    "    #plotting roc\n",
    "    plot_roc(fpr, tpr, roc_auc, roc_df, idx, save=save, model_name=model_name,folder_name=folder_name, file_name='roc')\n",
    "    plt.show(), plt.close()\n",
    "    \n",
    "    #printing npv, recall, precision, accuracy\n",
    "    npv=confusion_matrix(y, y_pred)[0,0]/sum(np.array(y_pred)==0)*100\n",
    "    prec= precision_score(y_true=y, y_pred= y_pred, pos_label=pos_label)\n",
    "    recall= recall_score(y_true=y, y_pred= y_pred, pos_label=pos_label)\n",
    "    f1= f1_score(y_true=y, y_pred= y_pred, pos_label=pos_label)\n",
    "    \n",
    "    if print_default==True: ###can opt to not print the 0.5 classification threshold classification report/conf matrix\n",
    "        #plotting confusion matrixs\n",
    "        print(\"\\n******* Using 0.5 Classification Threshold *******\\n\")\n",
    "        print(confusion_matrix(y, y_pred))\n",
    "        print ('the Accuracy is: {:01.2f}'.format(accuracy_score(y, y_pred)))\n",
    "        print (\"npv: {:01.2f}\".format(npv))\n",
    "        print ('the classification_report:\\n', classification_report(y,y_pred))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #### YOUDEN ADJUSTMENT #####\n",
    "\n",
    "    print(\"\\n******* Using Optimal Youden Classification Threshold *******\\n\")\n",
    "    print(\"\\nthe Youden optimal index is : {:01.2f}\".format(youden_threshold))\n",
    "\n",
    "    y_pred_youden = [1 if y >= youden_threshold else 0 for y in y_proba]\n",
    "\n",
    "    npv_y=confusion_matrix(y, y_pred_youden)[0,0]/sum(np.array(y_pred_youden)==0)*100\n",
    "    prec_y= precision_score(y_true=y, y_pred= y_pred_youden, pos_label=pos_label)*100\n",
    "    recall_y= recall_score(y_true=y, y_pred= y_pred_youden, pos_label=pos_label)*100\n",
    "    f1_y= f1_score(y_true=y, y_pred= y_pred_youden, pos_label=pos_label)*100\n",
    "    auc_y=roc_auc_score(y_true=y, y_score= y_proba)*100\n",
    "    \n",
    "    ##plotting and saving confusion matrix\n",
    "    confusion_youden=confusion_matrix(y, y_pred_youden)\n",
    "    \n",
    "    plot_table_as_fig(confusion_youden,\n",
    "                  col_labels=['predicted_neg','predicted_pos'],\n",
    "                  row_labels=['true_neg',\"true_pos\"],\n",
    "                  save=save,\n",
    "                  figsize=(6,1),\n",
    "                  model_name=model_name,\n",
    "                  folder_name=folder_name,\n",
    "                  file_name='y_confusion')\n",
    "    plt.show(), plt.close()\n",
    "   \n",
    "    #print(confusion_matrix(y, y_pred_youden))\n",
    "    print(\"the Accuracy is: {:01.2f}\".format(accuracy_score(y, y_pred_youden)))\n",
    "    print(\"npv: {:01.2f}\".format(npv_y))\n",
    "    \n",
    "    ###formatting classification report to be compatable with matplotlib table\n",
    "    report_youden=classification_report(y,y_pred_youden,output_dict=True)  \n",
    "    report_youden = pd.DataFrame.from_dict(report_youden).transpose()[['precision','recall','f1-score','support']]\n",
    "    report_youden = np.round(report_youden,2)\n",
    "    \n",
    "    ##plotting and saving classification report\n",
    "    plot_table_as_fig(table_in=np.array(report_youden),#classification_report(y, xgboost.predict(x))),\n",
    "                      col_labels=['precision','recall','f1-score','support'],\n",
    "                      row_labels=['neg',\"pos\",\"micro_avg\",\"macro_avg\",'weighted_avg'],\n",
    "                      figsize=(15,5),\n",
    "                      save=save,\n",
    "                      model_name=model_name,\n",
    "                      folder_name=folder_name,\n",
    "                      file_name='y_report')\n",
    "    plt.show(), plt.close()\n",
    "    \n",
    "    youden_dic= {'model':model_name, 'auc':auc_y, 'precision':prec_y, 'recall':recall_y, 'f1':f1_y, 'npv':npv_y}\n",
    "    return(youden_dic)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing global model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test entire trainset and predict trainset.\n",
    "<del> * step1) hypertune xgb on 5fold cv.\n",
    "    \n",
    "<del> * step2) run hypertuned model on 5fold cv with lr and get overall metrics.\n",
    "* step3) test entire train set and predict testset.\n",
    "* step4) local model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thresholds: \n",
    "* Decreasing thresholds on the decision function used to compute\n",
    "    fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
    "    and is arbitrarily set to `max(y_score) + 1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up test table\n",
    "\n",
    "#print(pd.DataFrame({'xg':xg_cv_auc, 'lr':lr_cv_auc, 'rf':rf_cv_auc, 'svc':svc_cv_auc}))\n",
    "#pd.DataFrame({'model':[],'auc':[], 'precision':[], 'recall':[], 'f1':[]})\n",
    "test_summary_df= pd.DataFrame({'model':[],'auc':[], 'precision':[], 'recall':[], 'f1':[], 'npv':[]})\n",
    "test_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = reset_model('xgboost', hardcode=False)\n",
    "xgboost.fit(x, y)\n",
    "\n",
    "logreg = reset_model('logreg', hardcode=False)\n",
    "logreg.fit(x, y)\n",
    "\n",
    "rf= reset_model('rf', hardcode=False)\n",
    "rf.fit(x,y)\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# gnb =GaussianNB()\n",
    "# nb_y_pred = gnb.fit(x, y)\n",
    "\n",
    "from sklearn import svm\n",
    "svc= reset_model('svc', hardcode=False)\n",
    "svc.fit(x, y)\n",
    "\n",
    "knn= reset_model('knn', hardcode=False)\n",
    "knn.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global model test set evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old\n",
    "#svc_eval= classifier_eval(svc, x=np.array(x_test), y=y_test, save=True, model_name='svc', folder_name='clinical_agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_eval= classifier_eval(svc, x=np.array(x_test), y=y_test, save=False, model_name='svc', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_eval= classifier_eval(svc, x=np.array(x_test), y=y_test, save=True, model_name='svc', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_eval= classifier_eval(xgboost, x=np.array(x_test), y=y_test, save=False, model_name='xgboost', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost2=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "       max_depth=2, min_child_weight=2, missing=None, n_estimators=100,\n",
    "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "       seed=27, silent=True, subsample=0.8)\n",
    "\n",
    "xgboost2.fit(x, y)\n",
    "\n",
    "xgboost2_eval= classifier_eval(xgboost2, x=np.array(x_test), y=y_test, save=False, model_name='xgboost2', folder_name='clinical_agg_24')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_eval= classifier_eval(xgboost, x=np.array(x_test), y=y_test, save=False, model_name='xgboost', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_eval= classifier_eval(xgboost, x=np.array(x_test), y=y_test, save=False, model_name='xgboost', folder_name='clinical_agg_elix')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_hyper.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##without elix\n",
    "#xgboost_eval= classifier_eval(xgboost, x=np.array(x_test), y=y_test, save=True, model_name='xgboost', folder_name='clinical_agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_eval= classifier_eval(rf, x=np.array(x_test), y=y_test, save=False, model_name='rf', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without elix\n",
    "#rf_eval= classifier_eval(rf, x=np.array(x_test), y=y_test, save=True, model_name='rf', folder_name='clinical_agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg_eval= classifier_eval(logreg, x=np.array(x_test), y=y_test)\n",
    "logreg_eval= classifier_eval(logreg, x=np.array(x_test), y=y_test, save=False, model_name='logreg', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn_eval= classifier_eval(knn, x=np.array(x_test), y=y_test, save=False, model_name='knn', folder_name='clinical_agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval= classifier_eval(knn, x=np.array(x_test), y=y_test, save=False, model_name='knn', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "#create a dictionary of our models\n",
    "estimators=[(\"xgboost\", xgboost), ('rf', rf), ('log_reg', logreg), ('svc',svc)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators, voting='soft', n_jobs=-1)\n",
    "# If âhardâ, uses predicted class labels for majority rule voting.\n",
    "# Else if âsoftâ, predicts the class label based on the argmax of the sums of the predicted probabilities,\n",
    "# which is recommended for an ensemble of well-calibrated classifiers.\n",
    "\n",
    "#weights: array-like, shape (n_classifiers,), optional (default=`None`)\n",
    "#Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting).\n",
    "#Uses uniform weights if None.\n",
    "ensemble.fit(x, y)#, sample_weight=np.array([0.67289604, 1.94595562]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble2 = VotingClassifier(estimators, voting='hard', n_jobs=-1)\n",
    "ensemble2.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble2.predict(np.array(x_test))\n",
    "#y_proba = ensemble2.predict_proba(np.array(x_test))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, roc_auc_score, f1_score, recall_score\n",
    "pos_label=1\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=pos_label)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "#gathering the optimal youden_index and df of tpr/fpr for auc and index of that optimal youden. idx is needed in the roc\n",
    "youden_threshold, roc_df, idx= optimal_youden_index(fpr, tpr, thresholds,tp90=True)\n",
    "\n",
    "#plotting roc\n",
    "plot_roc(fpr, tpr, roc_auc, roc_df, idx, save=False, model_name=ensemble2,folder_name=None, file_name='roc')\n",
    "plt.show(), plt.close()\n",
    "\n",
    "#printing npv, recall, precision, accuracy\n",
    "npv=confusion_matrix(y_test, y_pred)[0,0]/sum(np.array(y_pred)==0)*100\n",
    "prec= precision_score(y_true=y_test, y_pred= y_pred, pos_label=pos_label)*100\n",
    "recall= recall_score(y_true=y_test, y_pred= y_pred, pos_label=pos_label)*100\n",
    "f1= f1_score(y_true=y_test, y_pred= y_pred, pos_label=pos_label)*100\n",
    "acc=accuracy_score(y_test, y_pred)*100\n",
    "\n",
    "print(\"npv:\", npv,'\\n')\n",
    "print(\"prec:\", prec,'\\n')\n",
    "print(\"recall:\", recall,'\\n')\n",
    "print(\"f1:\", f1,'\\n')\n",
    "print(\"acc:\", acc,'\\n')\n",
    "\n",
    "hard_vote_summary={'model':\"hard_voting_classifier\",'auc':acc, 'precision':prec, 'recall':recall, 'f1':f1, 'npv':npv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_summary_df= pd.DataFrame({'model':[],'auc':[], 'precision':[], 'recall':[], 'f1':[], 'npv':[]})\n",
    "# test_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_eval= classifier_eval(ensemble, x=np.array(x_test), y=y_test, save=True, model_name='model_ensemble', folder_name='clinical_agg_elix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "compute_class_weight('balanced', np.unique(y), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier_eval(gnb, x=x_test, y=y_test)\n",
    "test_summary_df= pd.DataFrame([rf_eval,\n",
    "                             logreg_eval,\n",
    "                             xgboost_eval,\n",
    "                             svc_eval,\n",
    "                            ensemble_eval,\n",
    "                              hard_vote_summary])\n",
    "test_summary_df.set_index('model').round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveplot(figure_name,folder_name=None):\n",
    "    \"\"\"\n",
    "    simple function for saving plots\n",
    "    \"\"\"\n",
    "    if folder_name != None:\n",
    "        address = 'figures/{}/'.format(folder_name)\n",
    "    else:\n",
    "        address = 'figures/'\n",
    "    if not os.path.exists(address):\n",
    "        os.makedirs(address)\n",
    "    plt.savefig(address+\"/{}.png\".format(figure_name),bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_imp(model,folder_name,model_name, n_var=4, save=True):\n",
    "    model_name=type(model).__name__\n",
    "    plot_title= \"Top {} {} {} Variable Importance\".format(n_var, folder_name,model_name)\n",
    "    feat_importances = pd.Series(model.feature_importances_, index=x_train.columns)\n",
    "    topn=feat_importances.nlargest(n_var).sort_values()\n",
    "    ax=topn.plot(kind='barh', x='doop', title=plot_title)#.xlabel(\"xlab\")\n",
    "    ax.set_xlabel(\"Variable Importance\")\n",
    "    if save==True:\n",
    "        saveplot(figure_name=plot_title, folder_name=folder_name)\n",
    "    return(topn)\n",
    "\n",
    "#     ###\n",
    "#     imp= model.feature_importances_\n",
    "#     var_index=[ x for x in range(0,len(rf.feature_importances_))]\n",
    "#     variables=list(x_train)\n",
    "#     return(pd.DataFrame({\"imp\":imp, 'index':var_index, 'variable': variables}).sort_values('imp', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#var_imp(rf,plot_title='RF_' n_var=4)\n",
    "#var_imp(rf,\"clinical_agg\",\"RF\", n_var=6, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp(rf,\"clinical_agg_elix\",\"RF\", n_var=20, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp(xgboost2,\"clinical_agg\",\"xgboost\", n_var=20, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp(xgboost,\"clinical_agg_elix_72\",\"xgboost\", n_var=10, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_summary_df['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train),len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import model_selection\n",
    "\n",
    "# save the model to disk\n",
    "model_list=[rf,logreg, xgboost,svc ]\n",
    "for element in model_list:#test_summary_df['model']:\n",
    "    filename = 'models/{}_{}_{}.sav'.format(date,dataset,type(element).__name__)\n",
    "    #os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    pickle.dump(element, open(filename, 'wb'))\n",
    "    \n",
    "    \n",
    "# xgboost = reset_model('xgboost')\n",
    "# xgboost.fit(x, y)\n",
    "\n",
    "# logreg = reset_model('logreg')\n",
    "# logreg.fit(x, y)\n",
    "\n",
    "# rf= reset_model('rf')\n",
    "# rf.fit(x,y)\n",
    "\n",
    "# # from sklearn.naive_bayes import GaussianNB\n",
    "# # gnb =GaussianNB()\n",
    "# # nb_y_pred = gnb.fit(x, y)\n",
    "\n",
    "# from sklearn import svm\n",
    "# svc= reset_model('svc')\n",
    "# svc.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$dataset= \"clinagg_elix\"\n",
    "filename\n",
    "os.path.exists(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn import model_selection\n",
    "\n",
    "# save the model to disk\n",
    "model_list=[rf,logreg, xgboost,svc ]\n",
    "for element in model_list:#test_summary_df['model']:  \n",
    "    filename = 'models/{}_{}_{}.sav'.format(date,dataset,type(element).__name__)\n",
    "    pickle.dump(element, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the model from disk\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))\n",
    "# result = loaded_model.score(X_test, Y_test)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
